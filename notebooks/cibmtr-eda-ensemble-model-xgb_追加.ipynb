{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックは、[Albanito](https://www.kaggle.com/albansteff)さんのノートブック[CIBMTR | EDA & Ensemble Model - Recalculate HLA](https://www.kaggle.com/code/albansteff/cibmtr-eda-ensemble-model-recalculate-hla)を元にしています。素晴らしい分析や洞察を共有していただきありがとうございます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Introduction</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition in this notebook is the recalculation of HLA sums which are often missing. Since the sum is full of missing values, it was interesting to modify the values by recalculating them based on the data dictionary explanations and see the results.\n",
    "\n",
    "このノートブックにおける追加事項は、しばしば欠けているHLA合計の再計算です。この合計は欠損値が多いため、データ辞書の説明に基づいて値を再計算し、その結果を確認することが興味深いものでした"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T05:30:32.193055Z",
     "iopub.status.busy": "2025-01-26T05:30:32.192653Z",
     "iopub.status.idle": "2025-01-26T05:30:32.235794Z",
     "shell.execute_reply": "2025-01-26T05:30:32.234415Z",
     "shell.execute_reply.started": "2025-01-26T05:30:32.193015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# HLA : Human Leukocyte Antigen matching levels.\n",
    "# Homozygous chromosomes have the same allele at a given locus (fixed position on a chromosome where a particular gene is located).\n",
    "# Those that have different alleles at a given locus are called heterozygous.\n",
    "# Values Explanation\n",
    "# 0 - No Match: Neither of the donor's two HLA antigens/alleles matches the recipient's HLA.\n",
    "# This indicates a complete mismatch at the locus, which increases the risk of complications like graft rejection or graft-versus-host disease (GVHD).\n",
    "# 1 - Partial Match: One of the donor's HLA antigens/alleles matches one of the recipient's.\n",
    "# This represents a half-match (heterozygous compatibility) at the locus. It's better than a full mismatch but still carries a moderate risk of immune complications.\n",
    "# 2 - Full Match: Both of the donor's HLA match both of the recipient's HLA.\n",
    "# This is the optimal scenario, indicating full compatibility at the locus and minimizing the risk of immune complications.\n",
    "\n",
    "# High VS Low resolution\n",
    "\n",
    "# High-Resolution Typing: Identifies specific alleles (e.g., HLA-A*02:01).\n",
    "# Provides the most precise match and is essential for unrelated donor transplants.\n",
    "\n",
    "# Low-Resolution Typing: Identifies broader antigen groups (e.g., HLA-A2).\n",
    "# May suffice for related donor transplants where genetic similarity is inherently higher.\n",
    "\n",
    "# HLA: ヒト白血球抗原の一致レベルについて\n",
    "# ホモ接合染色体は、特定の遺伝子が位置する固定されたクロモソーム上の地点に同じ対立遺伝子を持っています。\n",
    "# 異なる対立遺伝子を持つものはヘテロ接合と呼ばれます。\n",
    "# 値の説明\n",
    "# 0 - 不一致: ドナーの2つのHLA抗原/対立遺伝子が受容者のHLAと一致しない。\n",
    "# これは、ラーカス（座位）での完全な不一致を示し、移植片拒絶や移植片対宿主病（GVHD）のような合併症のリスクを高めます。\n",
    "# 1 - 部分一致: ドナーのHLA抗原/対立遺伝子の1つが受容者のものと一致する。\n",
    "# これはラーカスでの半分の一致（ヘテロ接合の適合性）を表し、完全な不一致よりは良いですが、免疫の合併症のリスクは中程度あります。\n",
    "# 2 - 完全一致: ドナーの両方のHLAが受容者の両方のHLAと一致する。\n",
    "# これは最適なシナリオであり、ラーカスでの完全な適合性を示し、免疫合併症のリスクを最小限に抑えます。\n",
    "# 高解像度 VS 低解像度\n",
    "# 高解像度型: 特定の対立遺伝子を識別（例：HLA-A*02:01）。\n",
    "# 最も正確な一致を提供し、非関連ドナーの移植には不可欠です。\n",
    "# 低解像度型: より広い抗原グループを識別（例：HLA-A2）。\n",
    "# 遺伝的類似性が本質的に高い関連ドナーの移植には十分である場合があります。\n",
    "\n",
    "HLA_COLUMNS = [\n",
    "    # MHC class I molecules are one of two primary classes of major histocompatibility complex (MHC) molecules and are found on the cell surface of all nucleated cells.\n",
    "    # In humans, the HLAs corresponding to MHC class I are HLA-A, HLA-B, and HLA-C.\n",
    "\n",
    "#     MHCクラスI分子は、主要組織適合性複合体（MHC）分子の2つの主要なクラスのうちの1つで、すべての有核細胞の細胞表面に存在します。\n",
    "# ヒトでは、MHCクラスIに対応するHLAはHLA-A、HLA-B、HLA-Cです。\n",
    "    'hla_match_a_low', 'hla_match_a_high',\n",
    "    'hla_match_b_low', 'hla_match_b_high',\n",
    "    'hla_match_c_low', 'hla_match_c_high',\n",
    "\n",
    "    # MHC Class II molecules are a class of major histocompatibility complex (MHC) molecules normally found only on professional antigen-presenting cells \n",
    "    # such as dendritic cells, macrophages, some endothelial cells, thymic epithelial cells, and B cells.\n",
    "    # Antigens presented by MHC class II molecules are exogenous, originating from extracellular proteins rather than cytosolic and endogenous sources like \n",
    "    # those presented by MHC class I.\n",
    "    # HLAs corresponding to MHC class II are HLA-DP, HLA-DM, HLA-DOA, HLA-DOB, HLA-DQ, and HLA-DR.\n",
    "    # In this competition, we only have HLA-DR and HLA-DQ\n",
    "\n",
    "#     MHCクラスII分子は、通常、樹状細胞、マクロファージ、一部の内皮細胞、胸腺上皮細胞、B細胞などの専門の抗原提示細胞にのみ見られる主要組織適合性複合体（MHC）分子のクラスです。\n",
    "# MHCクラスII分子によって提示される抗原は外来由来であり、細胞質や内因性のソース（MHCクラスIによって提示されるもの）ではなく、細胞外タンパク質から由来しています。\n",
    "# MHCクラスIIに対応するHLAは、HLA-DP、HLA-DM、HLA-DOA、HLA-DOB、HLA-DQ、HLA-DRです。\n",
    "# この競技会では、HLA-DRとHLA-DQのみがあります。\n",
    "    \n",
    "    # Visit https://en.wikipedia.org/wiki/HLA-DQB1\n",
    "\n",
    "    'hla_match_dqb1_low', 'hla_match_dqb1_high',\n",
    "    'hla_match_drb1_low', 'hla_match_drb1_high',\n",
    "\n",
    "    # Combination of matches : sum of matches between multiple categories\n",
    "\n",
    "    # Matching at HLA-A(low), -B(low), -DRB1(high)\n",
    "    'hla_nmdp_6',\n",
    "    # Matching at HLA-A,-B,-DRB1 (low or high)\n",
    "    'hla_low_res_6', 'hla_high_res_6',\n",
    "    # Matching at HLA-A, -B, -C, -DRB1 (low or high)\n",
    "    'hla_low_res_8', 'hla_high_res_8',\n",
    "    # Matching at HLA-A, -B, -C, -DRB1, -DQB1 (low or high)\n",
    "    'hla_low_res_10', 'hla_high_res_10'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(247, 230, 202); border: 4px solid rgb(162, 87, 79); border-radius: 40px; padding: 20px; font-family: 'Roboto'; color: rgb(162, 87, 79); text-align: left; font-size: 120%;\">\n",
    "    <ul style=\"list-style-type: square; padding-left: 20px;\">\n",
    "        <li>Missing values are replaced with:\n",
    "            <ul style=\"list-style-type: circle; margin-top: 10px; margin-bottom: 10px;\">\n",
    "                <li>-1 for numeric columns</li>\n",
    "                <li>Unknown for categorical columns</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px;\">\n",
    "            LightGBM and CatBoost are trained on 3 different targets, estimated from the survival models:\n",
    "            <ul style=\"list-style-type: circle; margin-top: 10px; margin-bottom: 10px;\">\n",
    "                <li>Cox</li>\n",
    "                <li>Kaplan-Meier</li>\n",
    "                <li>Nelson-Aalen</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px;\">Two additional CatBoost model are trained, with Cox loss function.</li>\n",
    "        <li style=\"margin-top: 10px;\">As per <a href=\"https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/553061\" style=\"color: #A2574F; text-decoration: underline;\">this</a> discussion post, the target is consisted of the Out-of-Fold predictions of the survival models on the validation folds to prevent target leakage.</li>\n",
    "        <li style=\"margin-top: 10px;\">\n",
    "            The ensemble prediction for each sample is computed as:\n",
    "            <ul style=\"list-style-type: circle; margin-top: 10px; margin-bottom: 10px;\">\n",
    "                <p style=\"margin-top: 10px; font-size: 110%; color: #A2574F; font-family: 'Roboto'; text-align: left;\">\n",
    "                    $ \\text{preds}_{\\text{ensemble}} = \\sum_{i=1}^{n} w_i \\cdot \\text{rankdata}(\\text{preds}_i) $\n",
    "                </p>\n",
    "                where $n$ is the number of models, $w_i$ is the weight assigned to the $i$-th model, and $\\text{rankdata}(\\text{preds}_i)$ is the rank of predictions from the $i$-th model.\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px;\">Last but not least, since the competition metric evaluates only the order of predictions and not their magnitude, the model weights are not required to sum to 1, nor should the predictions fall within a predefined range.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(247, 230, 202); border: 4px solid rgb(162, 87, 79); border-radius: 40px; padding: 20px; font-family: 'Roboto'; color: rgb(162, 87, 79); text-align: left; font-size: 120%;\">\n",
    "    <ul style=\"list-style-type: square; padding-left: 20px;\">\n",
    "        <li>欠損値は次の値で置換されます：\n",
    "            <ul style=\"list-style-type: circle; margin-top: 10px; margin-bottom: 10px;\">\n",
    "                <li>数値列には-1</li>\n",
    "                <li>カテゴリ列には\"Unknown\"</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px;\">\n",
    "            LightGBMおよびCatBoostは、生存モデルから推定された3つの異なるターゲットで訓練されます：\n",
    "            <ul style=\"list-style-type: circle; margin-top: 10px; margin-bottom: 10px;\">\n",
    "                <li>Cox</li>\n",
    "                <li>Kaplan-Meier</li>\n",
    "                <li>Nelson-Aalen</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px;\">さらに2つのCatBoostモデルがCox損失関数で訓練されます。</li>\n",
    "        <li style=\"margin-top: 10px;\"><a href=\"https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/553061\" style=\"color: #A2574F; text-decoration: underline;\">この</a>ディスカッション投稿に従い、ターゲットは、バリデーションフォールドにおける生存モデルのOut-of-Fold予測で構成され、ターゲットリーケージを防止します。</li>\n",
    "        <li style=\"margin-top: 10px;\">\n",
    "            各サンプルのアンサンブル予測は次のように計算されます：\n",
    "            <ul style=\"list-style-type: circle; margin-top: 10px; margin-bottom: 10px;\">\n",
    "                <p style=\"margin-top: 10px; font-size: 110%; color: #A2574F; font-family: 'Roboto'; text-align: left;\">\n",
    "                    $ \\text{preds}_{\\text{ensemble}} = \\sum_{i=1}^{n} w_i \\cdot \\text{rankdata}(\\text{preds}_i) $\n",
    "                </p>\n",
    "                ここで $n$ はモデルの数、 $w_i$ は$i$番目のモデルに割り当てられた重み、$\\text{rankdata}(\\text{preds}_i)$ は$i$番目のモデルからの予測のランクです。\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px;\">最後に、コンペティションの評価指標は予測の大きさではなく順位のみを評価するため、モデルの重みが1に合計される必要はなく、予測が事前に定義された範囲に収まる必要もありません。</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006361,
     "end_time": "2024-12-13T13:19:30.95227",
     "exception": false,
     "start_time": "2024-12-13T13:19:30.945909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Install Libraries</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-26T05:30:32.239539Z",
     "iopub.status.busy": "2025-01-26T05:30:32.239110Z",
     "iopub.status.idle": "2025-01-26T05:34:10.345603Z",
     "shell.execute_reply": "2025-01-26T05:34:10.344311Z",
     "shell.execute_reply.started": "2025-01-26T05:30:32.239501Z"
    },
    "papermill": {
     "duration": 215.181879,
     "end_time": "2024-12-13T13:23:06.140734",
     "exception": false,
     "start_time": "2024-12-13T13:19:30.958855",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Imports</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:25.818486Z",
     "iopub.status.busy": "2025-01-26T08:54:25.817969Z",
     "iopub.status.idle": "2025-01-26T08:54:25.824494Z",
     "shell.execute_reply": "2025-01-26T08:54:25.823163Z",
     "shell.execute_reply.started": "2025-01-26T08:54:25.818420Z"
    },
    "papermill": {
     "duration": 0.017318,
     "end_time": "2024-12-13T13:23:06.166339",
     "exception": false,
     "start_time": "2024-12-13T13:23:06.149021",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:25.827660Z",
     "iopub.status.busy": "2025-01-26T08:54:25.827173Z",
     "iopub.status.idle": "2025-01-26T08:54:25.849206Z",
     "shell.execute_reply": "2025-01-26T08:54:25.847982Z",
     "shell.execute_reply.started": "2025-01-26T08:54:25.827606Z"
    },
    "papermill": {
     "duration": 1.790886,
     "end_time": "2024-12-13T13:23:07.965236",
     "exception": false,
     "start_time": "2024-12-13T13:23:06.17435",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import plotly.colors as pc\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:25.851342Z",
     "iopub.status.busy": "2025-01-26T08:54:25.850839Z",
     "iopub.status.idle": "2025-01-26T08:54:25.865987Z",
     "shell.execute_reply": "2025-01-26T08:54:25.864363Z",
     "shell.execute_reply.started": "2025-01-26T08:54:25.851293Z"
    },
    "papermill": {
     "duration": 0.172073,
     "end_time": "2024-12-13T13:23:08.145731",
     "exception": false,
     "start_time": "2024-12-13T13:23:07.973658",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:25.868676Z",
     "iopub.status.busy": "2025-01-26T08:54:25.868297Z",
     "iopub.status.idle": "2025-01-26T08:54:25.883512Z",
     "shell.execute_reply": "2025-01-26T08:54:25.881972Z",
     "shell.execute_reply.started": "2025-01-26T08:54:25.868639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import NelsonAalenFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:25.885550Z",
     "iopub.status.busy": "2025-01-26T08:54:25.885100Z",
     "iopub.status.idle": "2025-01-26T08:54:25.898983Z",
     "shell.execute_reply": "2025-01-26T08:54:25.897536Z",
     "shell.execute_reply.started": "2025-01-26T08:54:25.885497Z"
    },
    "papermill": {
     "duration": 3.110445,
     "end_time": "2024-12-13T13:23:11.289976",
     "exception": false,
     "start_time": "2024-12-13T13:23:08.179531",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from metric import score\n",
    "from scipy.stats import rankdata \n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# 全てのカラムを表示するオプションを設定\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00881,
     "end_time": "2024-12-13T13:23:11.307141",
     "exception": false,
     "start_time": "2024-12-13T13:23:11.298331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Configuration</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:26.036954Z",
     "iopub.status.busy": "2025-01-26T08:54:26.036485Z",
     "iopub.status.idle": "2025-01-26T08:54:26.059330Z",
     "shell.execute_reply": "2025-01-26T08:54:26.057998Z",
     "shell.execute_reply.started": "2025-01-26T08:54:26.036917Z"
    },
    "papermill": {
     "duration": 0.019849,
     "end_time": "2024-12-13T13:23:11.335144",
     "exception": false,
     "start_time": "2024-12-13T13:23:11.315295",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "\n",
    "    train_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\n",
    "    test_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\n",
    "    subm_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv')\n",
    "\n",
    "    color = '#A2574F'\n",
    "\n",
    "    batch_size = 32768\n",
    "    early_stop = 300\n",
    "    penalizer = 0.01\n",
    "    n_splits = 5\n",
    "\n",
    "    weights = [1.0, 1.0, 1.0, 8.0, 4.0, 6.0, 8.0, 4.0, 6.0, 6.0, 6.0]\n",
    "\n",
    "# loss_function :\n",
    "# モデルの損失関数を指定します。この例ではRMSE（Root Mean Squared Error）を使っており、回帰問題において予測値の誤差を評価するために用いられます。\n",
    "# learning_rate :\n",
    "# 学習率は、モデルが訓練データから学び取る際のステップサイズを決定します。この値が小さければ小さいほど、学習がより慎重に行われるため、収束が遅くなる可能性がありますが、過学習を抑えることができる場合があります。この例では0.03に設定されています。\n",
    "# random_state :\n",
    "# ランダムシードの値を設定し、再現性を確保します。この値を固定することで、毎回同じデータ分割や初期状態で実験を行うことができます。この値が42に設定されています。\n",
    "# task_type :\n",
    "# モデルの実行に使用する計算リソースのタイプを指定します。この例ではGPUが設定されているため、GPUを使用して計算が行われます。コメントアウトされたCPUは、CPUを使用するオプションです。\n",
    "# num_trees :\n",
    "# 学習に使用するツリーの数を指定します。この例では6000に設定されており、多くのツリーを用いることでモデルの表現力を高めることができますが、過学習のリスクも増えるため、適切な早期停止の設定が重要です。\n",
    "# subsample :\n",
    "# サンプリング比率を指定します。この値は、各ツリーを訓練する際に使用されるデータの割合を示します。この例では0.85となっており、85%のデータを使って各ツリーを学習することを示します。これにより過学習を抑える効果があります。\n",
    "# reg_lambda :\n",
    "# L2正則化項の係数を指定します。正則化は過学習を防ぐための手法の一つで、モデルの重みが過度に大きくならないように制約を与えます。この値が大きいほど、モデルの複雑さが制約されます。この例では8.0に設定されています。\n",
    "# depth :\n",
    "# 各決定木の深さを指定します。この値が大きいほど、モデルはより複雑なパターンを学習できますが、過学習のリスクも高まります。この例では8に設定されています。\n",
    "# bootstrap_type :\n",
    "# ブートストラップサンプリングの手法を指定します。この例ではBernoulliが選択されています。Bernoulliブートストラップは、各サンプリングごとにデータポイントを独立に選ぶ方法です。他のオプションとしては、No（ブートストラップを使用しない）やMVS（モンテカルロボンディング）などがあります。\n",
    "\n",
    "    ctb_params_1 = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'learning_rate': 0.02,\n",
    "        'random_state': 42,\n",
    "        'task_type': 'CPU',\n",
    "        # 'task_type': 'GPU',\n",
    "        'num_trees': 10000,\n",
    "        'subsample': 0.8,\n",
    "        'reg_lambda': 7.0,\n",
    "        'depth': 7,\n",
    "        'max_bin': 255    \n",
    "        # 'bootstrap_type': 'Bernoulli'\n",
    "    }\n",
    "    ctb_params_23 = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'learning_rate': 0.02,\n",
    "        'random_state': 42,\n",
    "        'task_type': 'CPU',\n",
    "        # 'task_type': 'GPU',\n",
    "        'num_trees': 10000,\n",
    "        'subsample': 0.8,\n",
    "        'reg_lambda': 7.0,\n",
    "        'depth': 7,\n",
    "        'max_bin': 255 \n",
    "        # 'bootstrap_type': 'Bernoulli'\n",
    "    }\n",
    "#     LightGBMパラメータの説明\n",
    "# objective :\n",
    "# モデルの目的を定義します。この例ではregressionと指定されており、回帰問題において連続値の予測を行うことを示しています。用途によってbinary（二値分類）やmulticlass（多クラス分類）なども指定できます。\n",
    "# min_child_samples :\n",
    "# 子ノードに必要な最小のデータサンプル数を指定します。この値が大きいほど、ノードが分割されるために必要なサンプル数も増え、過学習を抑制する働きがあります。この例では32に設定されています。\n",
    "# num_iterations :\n",
    "# 学習に使用するブースティングのイテレーション回数（決定木の数）を指定します。この例では6000に設定されており、モデルの収束を図るために多くの木を使用しますが、過学習のリスクも考慮する必要があります。\n",
    "# learning_rate :\n",
    "# 学習率を指定し、学習の速度を調整します。この値が小さいほど、モデルはより慎重に学習を進めることができ、安定性が増しますが、収束が遅くなる可能性があります。この例では0.03に設定されています。\n",
    "# extra_trees :\n",
    "# Trueに設定すると、より多様な木を構築するために、ノードの分割をランダムに行うことができます。これにより、モデルのバリエーションが増え、過学習を抑制する効果があります。\n",
    "# reg_lambda :\n",
    "# L2正則化項の係数を指定します。大きい値を設定することで、モデルの複雑度を制限し、過学習を防ぐ助けになります。この例では8.0に設定されています。\n",
    "# reg_alpha :\n",
    "# L1正則化項の係数を指定します。L1正則化はモデルの重みをゼロにすることがあり、特徴選択の効果を持つことがあります。この例では0.1に設定されています。\n",
    "# num_leaves :\n",
    "# 決定木の最大葉ノード数を指定します。この値が大きいほど、モデルの表現力が増しますが、過学習のリスクも高まります。この例では64に設定されています。\n",
    "# metric :\n",
    "# モデルの評価指標を指定します。この例ではrmseが設定されており、モデルの性能を根平均二乗誤差で評価します。\n",
    "# max_depth :\n",
    "# 決定木の最大深さを指定します。この値を設定することで、過学習を防ぎ、モデルの構造を制約することができます。この例では8に設定されています。\n",
    "# device :\n",
    "# モデルの実行に使用するデバイスを指定します。この例ではgpuが設定されており、GPUを使用して計算を行うことを示しています。コメントアウトされているcpuはCPUを使用するオプションです。\n",
    "# max_bin :\n",
    "# 特徴量のビンの最大数を指定します。この値を設定することで、カテゴリカルデータや連続値をどれだけ細かく分割するかを決定します。この例では128に設定されています。\n",
    "# verbose :\n",
    "# ログの出力レベルを指定します。-1に設定すると、情報メッセージを出力しないことを示しています。\n",
    "# seed :\n",
    "# ランダムシードの値を指定します。この値を固定しておくことで、再現性を確保することができます。この例では42に設定されています。\n",
    "\n",
    "    lgb_params_1 = {\n",
    "        'objective': 'regression',\n",
    "        'min_child_samples': 80,\n",
    "        'num_iterations': 6000,\n",
    "        'learning_rate': 0.02,\n",
    "        'extra_trees': False,\n",
    "        'reg_lambda': 7.0,\n",
    "        'reg_alpha': 1.0,\n",
    "        'num_leaves': 50,\n",
    "        'metric': 'rmse',\n",
    "        'max_depth': -1,\n",
    "        # 'device': 'gpu',\n",
    "        'device': 'cpu',\n",
    "        'max_bin': 255,\n",
    "        'subsample': 0.8,            # データのサンプリング率\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    lgb_params_23 = {\n",
    "        'objective': 'regression',\n",
    "        'min_child_samples': 32,\n",
    "        'num_iterations': 12000,\n",
    "        'learning_rate': 0.03,\n",
    "        'extra_trees': True,\n",
    "        'reg_lambda': 6.0,\n",
    "        'reg_alpha': 0.1,\n",
    "        'num_leaves': 64,\n",
    "        'metric': 'rmse',\n",
    "        'max_depth': 8,\n",
    "        # 'device': 'gpu',\n",
    "        'device': 'cpu',\n",
    "        'max_bin': 128,\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    xgb_params = {\n",
    "    'objective': 'reg:squarederror',  # 回帰タスク用の損失関数\n",
    "    'eval_metric': 'rmse',           # 評価指標\n",
    "    'learning_rate': 0.02,           # 学習率（低めに設定）\n",
    "    'max_depth': 7,                  # ツリーの深さ\n",
    "    'subsample': 0.8,                # サンプリング率\n",
    "    'colsample_bytree': 0.8,         # 特徴量のサンプリング率\n",
    "    'lambda': 7.0,                   # L2正則化\n",
    "    'alpha': 0.0,                    # L1正則化（必要に応じて調整）\n",
    "    'n_estimators': 10000,           # ツリーの本数（早期停止で自動制御）\n",
    "    'tree_method': 'hist',           # ツリー構築アルゴリズム（CPU用）\n",
    "    'random_state': 42,              # 再現性のための乱数シード\n",
    "    # 'early_stopping_rounds': 300     # 早期停止\n",
    "}\n",
    "\n",
    "\n",
    "    # Parameters for the first CatBoost model with Cox loss function\n",
    "    cox1_params = {\n",
    "        'grow_policy': 'Depthwise',\n",
    "        'min_child_samples': 8,\n",
    "        'loss_function': 'Cox',\n",
    "        'learning_rate': 0.02,\n",
    "        'random_state': 42,\n",
    "        # 'task_type': 'GPU',\n",
    "        'task_type': 'CPU',\n",
    "        'num_trees': 10000,\n",
    "        'subsample': 0.8,\n",
    "        'reg_lambda': 7.0,\n",
    "        'depth': 7,\n",
    "        'max_bin': 255\n",
    "        # 'bootstrap_type': 'Bernoulli'\n",
    "    }\n",
    "\n",
    "    # Parameters for the second CatBoost model with Cox loss function\n",
    "    cox2_params = {\n",
    "        'grow_policy': 'Lossguide',\n",
    "        'loss_function': 'Cox',\n",
    "        'learning_rate': 0.02,\n",
    "        'random_state': 42,\n",
    "        'task_type': 'CPU',\n",
    "        # 'task_type': 'GPU',\n",
    "        'num_trees': 10000,\n",
    "        'subsample': 0.8,\n",
    "        'reg_lambda': 7.0,\n",
    "        'num_leaves': 32,\n",
    "        'depth': 7,\n",
    "        'max_bin': 255\n",
    "        # 'bootstrap_type': 'Bernoulli'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007982,
     "end_time": "2024-12-13T13:23:11.351542",
     "exception": false,
     "start_time": "2024-12-13T13:23:11.34356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Feature Engineering</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:26.062896Z",
     "iopub.status.busy": "2025-01-26T08:54:26.062352Z",
     "iopub.status.idle": "2025-01-26T08:54:26.090100Z",
     "shell.execute_reply": "2025-01-26T08:54:26.088769Z",
     "shell.execute_reply.started": "2025-01-26T08:54:26.062819Z"
    },
    "papermill": {
     "duration": 0.022682,
     "end_time": "2024-12-13T13:23:11.382481",
     "exception": false,
     "start_time": "2024-12-13T13:23:11.359799",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FE:\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def load_data(self, path):\n",
    "#  batch_size\n",
    "# データをどの程度の大きさで分割して読み込むかを指定。\n",
    "# 大規模データを扱う際に、メモリ消費量を制御するために使用\n",
    "        return pl.read_csv(path, batch_size=self._batch_size)\n",
    "\n",
    "    def recalculate_hla_sums(self, df):\n",
    "        # Polarsライブラリを使用してデータフレーム（df）に新しい列を追加します。各新しい列は、特定のHLAマッチング列の合計値を計算し、それらの欠損値（Null）はゼロで補完\n",
    "        \n",
    "        df = df.with_columns(\n",
    "            # hla_match_a_low, hla_match_b_low, hla_match_drb1_highの各列の値（欠損値を0で補完）を合計。\n",
    "            (pl.col(\"hla_match_a_low\").fill_null(0) + pl.col(\"hla_match_b_low\").fill_null(0) + \n",
    "             pl.col(\"hla_match_drb1_high\").fill_null(0)).alias(\"hla_nmdp_6\"),\n",
    "            \n",
    "            (pl.col(\"hla_match_a_low\").fill_null(0) + pl.col(\"hla_match_b_low\").fill_null(0) + \n",
    "             pl.col(\"hla_match_drb1_low\").fill_null(0)).alias(\"hla_low_res_6\"),\n",
    "            \n",
    "            (pl.col(\"hla_match_a_high\").fill_null(0) + pl.col(\"hla_match_b_high\").fill_null(0) + \n",
    "             pl.col(\"hla_match_drb1_high\").fill_null(0)).alias(\"hla_high_res_6\"),\n",
    "            \n",
    "            (pl.col(\"hla_match_a_low\").fill_null(0) + pl.col(\"hla_match_b_low\").fill_null(0) + \n",
    "             pl.col(\"hla_match_c_low\").fill_null(0) + pl.col(\"hla_match_drb1_low\").fill_null(0)\n",
    "            ).alias(\"hla_low_res_8\"),\n",
    "            \n",
    "            (pl.col(\"hla_match_a_high\").fill_null(0) + pl.col(\"hla_match_b_high\").fill_null(0) + \n",
    "             pl.col(\"hla_match_c_high\").fill_null(0) + pl.col(\"hla_match_drb1_high\").fill_null(0)\n",
    "            ).alias(\"hla_high_res_8\"),\n",
    "            \n",
    "            (pl.col(\"hla_match_a_low\").fill_null(0) + pl.col(\"hla_match_b_low\").fill_null(0) + \n",
    "             pl.col(\"hla_match_c_low\").fill_null(0) + pl.col(\"hla_match_drb1_low\").fill_null(0) +\n",
    "             pl.col(\"hla_match_dqb1_low\").fill_null(0)).alias(\"hla_low_res_10\"),\n",
    "            \n",
    "            (pl.col(\"hla_match_a_high\").fill_null(0) + pl.col(\"hla_match_b_high\").fill_null(0) + \n",
    "             pl.col(\"hla_match_c_high\").fill_null(0) + pl.col(\"hla_match_drb1_high\").fill_null(0) +\n",
    "             pl.col(\"hla_match_dqb1_high\").fill_null(0)).alias(\"hla_high_res_10\"),\n",
    "\n",
    "             (pl.col(\"diabetes\").cast(pl.Utf8) + \"_\" + pl.col(\"obesity\").cast(pl.Utf8)).alias(\"diabetes_obesity\").cast(pl.Utf8),\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cast_datatypes(self, df):\n",
    "\n",
    "        num_cols = [\n",
    "            'hla_high_res_8',\n",
    "            'hla_low_res_8',\n",
    "            'hla_high_res_6',\n",
    "            'hla_low_res_6',\n",
    "            'hla_high_res_10',\n",
    "            'hla_low_res_10',\n",
    "            'hla_match_dqb1_high',\n",
    "            'hla_match_dqb1_low',\n",
    "            'hla_match_drb1_high',\n",
    "            'hla_match_drb1_low',\n",
    "            'hla_nmdp_6',\n",
    "            'year_hct',\n",
    "            'hla_match_a_high',\n",
    "            'hla_match_a_low',\n",
    "            'hla_match_b_high',\n",
    "            'hla_match_b_low',\n",
    "            'hla_match_c_high',\n",
    "            'hla_match_c_low',\n",
    "            'donor_age',\n",
    "            'age_at_hct',\n",
    "            'comorbidity_score',\n",
    "            'karnofsky_score',\n",
    "            'efs',\n",
    "            'efs_time'\n",
    "        ]\n",
    "\n",
    "        for col in df.columns:\n",
    "\n",
    "            if col in num_cols:\n",
    "                df = df.with_columns(pl.col(col).fill_null(-1).cast(pl.Float32))  \n",
    "\n",
    "            else:\n",
    "                df = df.with_columns(pl.col(col).fill_null('Unknown').cast(pl.String))  \n",
    "\n",
    "        return df.with_columns(pl.col('ID').cast(pl.Int32))\n",
    "\n",
    "    def info(self, df):\n",
    "        \n",
    "        print(f'\\nShape of dataframe: {df.shape}') \n",
    "        \n",
    "        mem = df.memory_usage().sum() / 1024**2\n",
    "        print('Memory usage: {:.2f} MB\\n'.format(mem))\n",
    "\n",
    "        display(df.head())\n",
    "\n",
    "    def apply_fe(self, path):\n",
    "\n",
    "        df = self.load_data(path)\n",
    "        df = self.recalculate_hla_sums(df)\n",
    "        df = self.cast_datatypes(df)\n",
    "        df = df.to_pandas()\n",
    "\n",
    "        self.info(df)\n",
    "        \n",
    "        cat_cols = [col for col in df.columns if df[col].dtype == pl.String]\n",
    "\n",
    "        return df, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:26.092135Z",
     "iopub.status.busy": "2025-01-26T08:54:26.091731Z",
     "iopub.status.idle": "2025-01-26T08:54:26.114014Z",
     "shell.execute_reply": "2025-01-26T08:54:26.112492Z",
     "shell.execute_reply.started": "2025-01-26T08:54:26.092098Z"
    },
    "papermill": {
     "duration": 0.016709,
     "end_time": "2024-12-13T13:23:11.407504",
     "exception": false,
     "start_time": "2024-12-13T13:23:11.390795",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fe = FE(CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:26.117387Z",
     "iopub.status.busy": "2025-01-26T08:54:26.117004Z",
     "iopub.status.idle": "2025-01-26T08:54:26.689107Z",
     "shell.execute_reply": "2025-01-26T08:54:26.687825Z",
     "shell.execute_reply.started": "2025-01-26T08:54:26.117352Z"
    },
    "papermill": {
     "duration": 0.726911,
     "end_time": "2024-12-13T13:23:12.142797",
     "exception": false,
     "start_time": "2024-12-13T13:23:11.415886",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, cat_cols = fe.apply_fe(CFG.train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:26.690761Z",
     "iopub.status.busy": "2025-01-26T08:54:26.690427Z",
     "iopub.status.idle": "2025-01-26T08:54:26.707386Z",
     "shell.execute_reply": "2025-01-26T08:54:26.706158Z",
     "shell.execute_reply.started": "2025-01-26T08:54:26.690729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data[\"diabetes_obesity\"].fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:26.709375Z",
     "iopub.status.busy": "2025-01-26T08:54:26.708991Z",
     "iopub.status.idle": "2025-01-26T08:54:26.776545Z",
     "shell.execute_reply": "2025-01-26T08:54:26.775254Z",
     "shell.execute_reply.started": "2025-01-26T08:54:26.709341Z"
    },
    "papermill": {
     "duration": 0.075873,
     "end_time": "2024-12-13T13:23:12.227636",
     "exception": false,
     "start_time": "2024-12-13T13:23:12.151763",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data, _ = fe.apply_fe(CFG.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:26.778431Z",
     "iopub.status.busy": "2025-01-26T08:54:26.777979Z",
     "iopub.status.idle": "2025-01-26T08:54:26.814164Z",
     "shell.execute_reply": "2025-01-26T08:54:26.812377Z",
     "shell.execute_reply.started": "2025-01-26T08:54:26.778392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CSVファイルのパスを指定\n",
    "csv_file_path = \"/kaggle/input/world-age-death-rate/death_rate.csv\"\n",
    "\n",
    "# CSVファイルを読み込む\n",
    "death_rate_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# \"男平均\" と \"女平均\" の行ごとの平均を計算して新しいカラム \"全平均\" に追加\n",
    "death_rate_df['全平均'] = death_rate_df[['男平均', '女平均']].mean(axis=1)\n",
    "\n",
    "# '男平均', '女平均', '全平均'の累積和カラムを作成\n",
    "death_rate_df['男平均_累積和'] = death_rate_df['男平均'].cumsum()\n",
    "death_rate_df['女平均_累積和'] = death_rate_df['女平均'].cumsum()\n",
    "death_rate_df['全平均_累積和'] = death_rate_df['全平均'].cumsum()\n",
    "\n",
    "# データフレームを表示\n",
    "display(death_rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:26.819305Z",
     "iopub.status.busy": "2025-01-26T08:54:26.818829Z",
     "iopub.status.idle": "2025-01-26T08:54:28.478734Z",
     "shell.execute_reply": "2025-01-26T08:54:28.477411Z",
     "shell.execute_reply.started": "2025-01-26T08:54:26.819261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# 1) 年齢区分を返す関数\n",
    "# =====================\n",
    "def get_age_group(age: float) -> str:\n",
    "    \"\"\"\n",
    "    death_rate_dfの「年齢」カラムに合わせて、数値ageを区間ラベルに変換する。\n",
    "    ただし age == -1 は不明を示すため None を返す。\n",
    "    \"\"\"\n",
    "    if age == -1:\n",
    "        return None\n",
    "    \n",
    "    # 0歳\n",
    "    if age == 0:\n",
    "        return \"0\"\n",
    "    # 1～4歳\n",
    "    elif 1 <= age < 5:\n",
    "        return \" 1～4\"\n",
    "    elif 5 <= age < 10:\n",
    "        return \" 5～9\"\n",
    "    elif 10 <= age < 15:\n",
    "        return \"10～14\"\n",
    "    elif 15 <= age < 20:\n",
    "        return \"15～19\"\n",
    "    elif 20 <= age < 25:\n",
    "        return \"20～24\"\n",
    "    elif 25 <= age < 30:\n",
    "        return \"25～29\"\n",
    "    elif 30 <= age < 35:\n",
    "        return \"30～34\"\n",
    "    elif 35 <= age < 40:\n",
    "        return \"35～39\"\n",
    "    elif 40 <= age < 45:\n",
    "        return \"40～44\"\n",
    "    elif 45 <= age < 50:\n",
    "        return \"45～49\"\n",
    "    elif 50 <= age < 55:\n",
    "        return \"50～54\"\n",
    "    elif 55 <= age < 60:\n",
    "        return \"55～59\"\n",
    "    elif 60 <= age < 65:\n",
    "        return \"60～64\"\n",
    "    elif 65 <= age < 70:\n",
    "        return \"65～69\"\n",
    "    elif 70 <= age < 75:\n",
    "        return \"70～74\"\n",
    "    elif 75 <= age < 80:\n",
    "        return \"75～79\"\n",
    "    elif 80 <= age < 85:\n",
    "        return \"80～84\"\n",
    "    elif 85 <= age < 90:\n",
    "        return \"85～89\"\n",
    "    elif 90 <= age < 95:\n",
    "        return \"90～94\"\n",
    "    elif 95 <= age < 100:\n",
    "        return \"95～99\"\n",
    "    else:\n",
    "        # 100歳以上\n",
    "        return \"100歳以上\"\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 2) sex_match から性別を取り出す関数\n",
    "# =====================\n",
    "def get_donor_sex(sex_match: str) -> str:\n",
    "    \"\"\"\n",
    "    sex_match: \"M-F\" のような文字列\n",
    "       → ドナーの性別(M/F)を返す\n",
    "    sex_matchがNaNなど不正ならNoneを返す(= 性別不明)\n",
    "    \"\"\"\n",
    "    if pd.isnull(sex_match):\n",
    "        return None\n",
    "    parts = sex_match.split(\"-\")\n",
    "    if len(parts) == 2:\n",
    "        return parts[0].strip()  # ドナー側\n",
    "    return None\n",
    "\n",
    "def get_recipient_sex(sex_match: str) -> str:\n",
    "    \"\"\"\n",
    "    sex_match: \"M-F\" のような文字列\n",
    "       → レシピエントの性別(M/F)を返す\n",
    "    sex_matchがNaNなど不正ならNoneを返す(= 性別不明)\n",
    "    \"\"\"\n",
    "    if pd.isnull(sex_match):\n",
    "        return None\n",
    "    parts = sex_match.split(\"-\")\n",
    "    if len(parts) == 2:\n",
    "        return parts[1].strip()  # レシピエント側\n",
    "    return None\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 3) death_rate_df を辞書化: { age_label: {\"M\": male, \"F\": female, \"All\": overall}, ... }\n",
    "# =====================\n",
    "death_rate_dict = {}\n",
    "for _, row in death_rate_df.iterrows():\n",
    "    age_label = row[\"年  齢\"]        # \"0\", \"1～4\", \"5～9\", ... \"100歳以上\"\n",
    "    male_rate = row[\"男平均\"]     # float\n",
    "    female_rate = row[\"女平均\"]   # float\n",
    "    all_rate   = row[\"全平均\"]    # float\n",
    "    cum_male_rate = row[\"男平均_累積和\"]     # float\n",
    "    cum_female_rate = row[\"女平均_累積和\"]   # float\n",
    "    cum_all_rate   = row[\"全平均_累積和\"]    # float\n",
    "    death_rate_dict[age_label] = {\n",
    "        \"M\": (male_rate, cum_male_rate),\n",
    "        \"F\": (female_rate, cum_female_rate),\n",
    "        \"All\": (all_rate, cum_all_rate)\n",
    "    }\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 4) (年齢, 性別) から死亡率を取得する関数\n",
    "# =====================\n",
    "def get_death_rate(age: float, sex: str) -> float:\n",
    "    \"\"\"\n",
    "    入力:\n",
    "      age: 年齢 (ドナー or レシピエントの年齢)\n",
    "      sex: 'M' または 'F'、またはNone (不明)\n",
    "    戻り値:\n",
    "      death_rate: floatまたは np.nan\n",
    "    \"\"\"\n",
    "    # まず「ageが-1」(不明)なら np.nanを返す\n",
    "    if age == -1:\n",
    "        return np.nan\n",
    "\n",
    "    # 年齢区分を取得\n",
    "    age_label = get_age_group(age)\n",
    "    if (age_label is None) or (age_label not in death_rate_dict):\n",
    "        # 万が一該当しない場合は np.nan でも良いが、\n",
    "        # ここでは \"100歳以上\" の全平均を返す等、運用次第で対応を変えてください。\n",
    "        return np.nan\n",
    "\n",
    "    # 性別が不明(None)なら全平均を返す\n",
    "    if sex is None:\n",
    "        return death_rate_dict[age_label][\"All\"][0]\n",
    "\n",
    "    # 性別が \"M\" または \"F\" なら対応する死亡率を返す\n",
    "    if sex == \"M\":\n",
    "        return death_rate_dict[age_label][\"M\"][0]\n",
    "    elif sex == \"F\":\n",
    "        return death_rate_dict[age_label][\"F\"][0]\n",
    "    else:\n",
    "        # 想定外の文字が入っている場合も全平均にフォールバック (例)\n",
    "        return death_rate_dict[age_label][\"All\"][0]\n",
    "\n",
    "# =====================\n",
    "# 4) (年齢, 性別) から累積死亡率を取得する関数\n",
    "# =====================\n",
    "def get_cum_death_rate(age: float, sex: str) -> float:\n",
    "    \"\"\"\n",
    "    入力:\n",
    "      age: 年齢 (ドナー or レシピエントの年齢)\n",
    "      sex: 'M' または 'F'、またはNone (不明)\n",
    "    戻り値:\n",
    "      death_rate: floatまたは np.nan\n",
    "    \"\"\"\n",
    "    # まず「ageが-1」(不明)なら np.nanを返す\n",
    "    if age == -1:\n",
    "        return np.nan\n",
    "\n",
    "    # 年齢区分を取得\n",
    "    age_label = get_age_group(age)\n",
    "    if (age_label is None) or (age_label not in death_rate_dict):\n",
    "        # 万が一該当しない場合は np.nan でも良いが、\n",
    "        # ここでは \"100歳以上\" の全平均を返す等、運用次第で対応を変えてください。\n",
    "        return np.nan\n",
    "\n",
    "    # 性別が不明(None)なら全平均を返す\n",
    "    if sex is None:\n",
    "        return death_rate_dict[age_label][\"All\"][1]\n",
    "\n",
    "    # 性別が \"M\" または \"F\" なら対応する死亡率を返す\n",
    "    if sex == \"M\":\n",
    "        return death_rate_dict[age_label][\"M\"][1]\n",
    "    elif sex == \"F\":\n",
    "        return death_rate_dict[age_label][\"F\"][1]\n",
    "    else:\n",
    "        # 想定外の文字が入っている場合も全平均にフォールバック (例)\n",
    "        return death_rate_dict[age_label][\"All\"][1]\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 5) train_data, test_data に新カラムを追加\n",
    "# =====================\n",
    "# ドナーの死亡率\n",
    "train_data[\"donor_age_death_rate\"] = train_data.apply(\n",
    "    lambda row: get_death_rate(row[\"donor_age\"], get_donor_sex(row[\"sex_match\"])), axis=1\n",
    ")\n",
    "\n",
    "train_data[\"cum_donor_age_death_rate\"] = train_data.apply(\n",
    "    lambda row: get_cum_death_rate(row[\"donor_age\"], get_donor_sex(row[\"sex_match\"])), axis=1\n",
    ")\n",
    "# レシピエントの死亡率\n",
    "train_data[\"recipient_age_death_rate\"] = train_data.apply(\n",
    "    lambda row: get_death_rate(row[\"age_at_hct\"], get_recipient_sex(row[\"sex_match\"])), axis=1\n",
    ")\n",
    "\n",
    "train_data[\"cum_recipient_age_death_rate\"] = train_data.apply(\n",
    "    lambda row: get_cum_death_rate(row[\"age_at_hct\"], get_recipient_sex(row[\"sex_match\"])), axis=1\n",
    ")\n",
    "\n",
    "# test_data も同様\n",
    "test_data[\"donor_age_death_rate\"] = test_data.apply(\n",
    "    lambda row: get_death_rate(row[\"donor_age\"], get_donor_sex(row[\"sex_match\"])), axis=1\n",
    ")\n",
    "\n",
    "test_data[\"cum_donor_age_death_rate\"] = test_data.apply(\n",
    "    lambda row: get_cum_death_rate(row[\"donor_age\"], get_donor_sex(row[\"sex_match\"])), axis=1)\n",
    "\n",
    "\n",
    "test_data[\"recipient_age_death_rate\"] = test_data.apply(\n",
    "    lambda row: get_death_rate(row[\"age_at_hct\"], get_recipient_sex(row[\"sex_match\"])), axis=1\n",
    ")\n",
    "\n",
    "test_data[\"cum_recipient_age_death_rate\"] = test_data.apply(\n",
    "    lambda row: get_cum_death_rate(row[\"age_at_hct\"], get_recipient_sex(row[\"sex_match\"])), axis=1\n",
    ")\n",
    "\n",
    "# # 新しい特徴量を作成して欠損値を判定\n",
    "# train_data['recipient_age_death_rate_is_na'] = train_data['recipient_age_death_rate'].isna().astype(int)\n",
    "# train_data['donor_age_death_rate_is_na'] = train_data['donor_age_death_rate'].isna().astype(int)\n",
    "\n",
    "# test_data['recipient_age_death_rate_is_na'] = test_data['recipient_age_death_rate'].isna().astype(int)\n",
    "# test_data['donor_age_death_rate_is_na'] = test_data['donor_age_death_rate'].isna().astype(int)\n",
    "\n",
    "# recipient_age_death_rate の NaN を同じカラムの NaN 以外の平均値で埋める\n",
    "train_data['recipient_age_death_rate'] = train_data['recipient_age_death_rate'].fillna(train_data['recipient_age_death_rate'].mean())\n",
    "\n",
    "# donor_age_death_rate の NaN を同じカラムの NaN 以外の平均値で埋める\n",
    "train_data['donor_age_death_rate'] = train_data['donor_age_death_rate'].fillna(train_data['donor_age_death_rate'].mean())\n",
    "\n",
    "# recipient_age_death_rate の NaN を同じカラムの NaN 以外の平均値で埋める\n",
    "test_data['recipient_age_death_rate'] = test_data['recipient_age_death_rate'].fillna(test_data['recipient_age_death_rate'].mean())\n",
    "\n",
    "# donor_age_death_rate の NaN を同じカラムの NaN 以外の平均値で埋める\n",
    "test_data['donor_age_death_rate'] = test_data['donor_age_death_rate'].fillna(test_data['donor_age_death_rate'].mean())\n",
    "\n",
    "# recipient_age_death_rate の NaN を同じカラムの NaN 以外の平均値で埋める\n",
    "train_data['cum_recipient_age_death_rate'] = train_data['cum_recipient_age_death_rate'].fillna(train_data['recipient_age_death_rate'].mean())\n",
    "\n",
    "# donor_age_death_rate の NaN を同じカラムの NaN 以外の平均値で埋める\n",
    "train_data['cum_donor_age_death_rate'] = train_data['cum_donor_age_death_rate'].fillna(train_data['donor_age_death_rate'].mean())\n",
    "\n",
    "# recipient_age_death_rate の NaN を同じカラムの NaN 以外の平均値で埋める\n",
    "test_data['cum_recipient_age_death_rate'] = test_data['cum_recipient_age_death_rate'].fillna(test_data['recipient_age_death_rate'].mean())\n",
    "\n",
    "# donor_age_death_rate の NaN を同じカラムの NaN 以外の平均値で埋める\n",
    "test_data['cum_donor_age_death_rate'] = test_data['cum_donor_age_death_rate'].fillna(test_data['donor_age_death_rate'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.480995Z",
     "iopub.status.busy": "2025-01-26T08:54:28.480484Z",
     "iopub.status.idle": "2025-01-26T08:54:28.598378Z",
     "shell.execute_reply": "2025-01-26T08:54:28.597190Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.480936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.602133Z",
     "iopub.status.busy": "2025-01-26T08:54:28.601745Z",
     "iopub.status.idle": "2025-01-26T08:54:28.614509Z",
     "shell.execute_reply": "2025-01-26T08:54:28.613279Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.602098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(test_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.616283Z",
     "iopub.status.busy": "2025-01-26T08:54:28.615708Z",
     "iopub.status.idle": "2025-01-26T08:54:28.633563Z",
     "shell.execute_reply": "2025-01-26T08:54:28.632242Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.616246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.635520Z",
     "iopub.status.busy": "2025-01-26T08:54:28.635183Z",
     "iopub.status.idle": "2025-01-26T08:54:28.702122Z",
     "shell.execute_reply": "2025-01-26T08:54:28.700768Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.635476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nan_counts = train_data.isna().sum()\n",
    "nan_columns = nan_counts[nan_counts > 0]\n",
    "print(\"NaNが存在するカラムとその数:\")\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.704061Z",
     "iopub.status.busy": "2025-01-26T08:54:28.703619Z",
     "iopub.status.idle": "2025-01-26T08:54:28.718078Z",
     "shell.execute_reply": "2025-01-26T08:54:28.716428Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.704020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nan_counts = test_data.isna().sum()\n",
    "nan_columns = nan_counts[nan_counts > 0]\n",
    "print(\"NaNが存在するカラムとその数:\")\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.720086Z",
     "iopub.status.busy": "2025-01-26T08:54:28.719608Z",
     "iopub.status.idle": "2025-01-26T08:54:28.738756Z",
     "shell.execute_reply": "2025-01-26T08:54:28.737431Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.720030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nan_records = test_data[test_data['donor_age_death_rate'].isna() | test_data['recipient_age_death_rate'].isna()]\n",
    "# nan_records = train_data[train_data['donor_age_death_rate'].isna() | train_data['recipient_age_death_rate'].isna()]\n",
    "# \"donor_age_death_rate\" が NaN のレコードを抽出\n",
    "# nan_records = train_data[train_data['donor_age_death_rate'].isna()]\n",
    "# nan_records = test_data[train_data['recipient_age_death_rate'].isna()]\n",
    "# \n",
    "# 結果を表示\n",
    "display(nan_records[[\"age_at_hct\",\"donor_age\",\"sex_match\", \"recipient_age_death_rate\", \"donor_age_death_rate\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.740477Z",
     "iopub.status.busy": "2025-01-26T08:54:28.740141Z",
     "iopub.status.idle": "2025-01-26T08:54:28.767998Z",
     "shell.execute_reply": "2025-01-26T08:54:28.766661Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.740444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nan_records = train_data[train_data['donor_age_death_rate'].isna() | train_data['recipient_age_death_rate'].isna()]\n",
    "# nan_records = train_data[train_data['donor_age_death_rate'].isna() | train_data['recipient_age_death_rate'].isna()]\n",
    "# \"donor_age_death_rate\" が NaN のレコードを抽出\n",
    "# nan_records = train_data[train_data['donor_age_death_rate'].isna()]\n",
    "# nan_records = train_data[train_data['recipient_age_death_rate'].isna()]\n",
    "\n",
    "# 結果を表示\n",
    "display(nan_records[[\"age_at_hct\",\"donor_age\",\"sex_match\", \"recipient_age_death_rate\", \"donor_age_death_rate\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.770396Z",
     "iopub.status.busy": "2025-01-26T08:54:28.769861Z",
     "iopub.status.idle": "2025-01-26T08:54:28.782261Z",
     "shell.execute_reply": "2025-01-26T08:54:28.780838Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.770345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(nan_records[\"donor_age_death_rate\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.784402Z",
     "iopub.status.busy": "2025-01-26T08:54:28.783924Z",
     "iopub.status.idle": "2025-01-26T08:54:28.801505Z",
     "shell.execute_reply": "2025-01-26T08:54:28.800073Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.784365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# display(nan_records[[\"age_at_hct\",\"donor_age\",\"sex_match\", \"recipient_age_death_rate\", \"donor_age_death_rate\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.803436Z",
     "iopub.status.busy": "2025-01-26T08:54:28.802977Z",
     "iopub.status.idle": "2025-01-26T08:54:28.818217Z",
     "shell.execute_reply": "2025-01-26T08:54:28.816693Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.803387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# データフレームを表示\n",
    "# display(death_rate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009157,
     "end_time": "2024-12-13T13:23:12.246272",
     "exception": false,
     "start_time": "2024-12-13T13:23:12.237115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Model Development</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.820337Z",
     "iopub.status.busy": "2025-01-26T08:54:28.819783Z",
     "iopub.status.idle": "2025-01-26T08:54:28.839729Z",
     "shell.execute_reply": "2025-01-26T08:54:28.838336Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.820286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EDA:\n",
    "    \n",
    "    def __init__(self, color, data):\n",
    "        self._color = color  \n",
    "        self.data = data\n",
    "\n",
    "    def _template(self, fig, title):\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            title_x=0.5, \n",
    "            plot_bgcolor='rgba(247, 230, 202, 1)',  \n",
    "            paper_bgcolor='rgba(247, 230, 202, 1)', \n",
    "            font=dict(color=self._color),\n",
    "            margin=dict(l=72, r=72, t=72, b=72), \n",
    "            height=720\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "\n",
    "    # Plotly Expressを使用してヒストグラムを作成し、そのレイアウトとトレースをカスタマイズ\n",
    "    def distribution_plot(self, col, title):\n",
    "\n",
    "#         データの準備:\n",
    "# px.histogramを使ってヒストグラムを作成しています。\n",
    "# self.data: データフレーム。\n",
    "# x=col: ヒストグラムのX軸に使用する列。\n",
    "# nbins=100: ビンの数を100に設定。\n",
    "# color_discrete_sequence=[self._color]: ヒストグラムの色を指定\n",
    "        \n",
    "        fig = px.histogram(\n",
    "            self.data,\n",
    "            x=col,\n",
    "            nbins=100,\n",
    "            color_discrete_sequence=[self._color]\n",
    "        )\n",
    "\n",
    "\n",
    "#         レイアウトの更新:\n",
    "# fig.update_layoutを使ってヒストグラムのレイアウトをカスタマイズ。\n",
    "# xaxis_title='Values': X軸のタイトル。\n",
    "# yaxis_title='Count': Y軸のタイトル。\n",
    "# bargap=0.1: バー間の隙間を設定。\n",
    "# xaxis.dict(gridcolor='grey')とyaxis.dict(gridcolor='grey', zerolinecolor='grey'): グリッド線とゼロ線の色をグレーに設定。\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title='Values',\n",
    "            yaxis_title='Count',\n",
    "            bargap=0.1,\n",
    "            xaxis=dict(gridcolor='grey'),\n",
    "            yaxis=dict(gridcolor='grey', zerolinecolor='grey')\n",
    "        )\n",
    "\n",
    "\n",
    "        # ホバー時の情報をカスタマイズ。\n",
    "# hovertemplate='Value: %{x:.2f}<br>Count: %{y:,}': ホバー時に表示されるテンプレート。\n",
    "        # 「ホバー時」とは、マウスカーソルをグラフ上の特定のデータポイントに重ねたときのことを指します。この操作中に、通常はそのポイントに関連する追加情報（例として、X軸の値やY軸のカウントなど）がポップアップとして表示されます。この情報はデータを視覚的に確認する際に便利です。\n",
    "        fig.update_traces(hovertemplate='Value: %{x:.2f}<br>Count: %{y:,}')\n",
    "\n",
    "#         テンプレートの適用と表示:\n",
    "# self._template(fig, f'{title}')でテンプレートを適用（詳細はself._templateメソッドによる）。\n",
    "# fig.show()でプロットを表示。\n",
    "        fig = self._template(fig, f'{title}')\n",
    "        fig.show()\n",
    "        \n",
    "    def _plot_cv(self, scores, title, metric='Stratified C-Index'):\n",
    "        \n",
    "        fold_scores = [round(score, 3) for score in scores]\n",
    "        mean_score = round(np.mean(scores), 3)\n",
    "\n",
    "        # で新しいグラフを作成。\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # 折りたたみスコア（クロスバリデーションの各フォールドのスコア）をプロット。\n",
    "#         x軸にはフォールド番号（1からスコア数まで）、y軸には該当するスコアを指定。\n",
    "# モードは'markers'で、プロットはダイヤ型のマーカーを使用。\n",
    "# hovertemplateでホバー時に「Fold x: score」を表示。\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x = list(range(1, len(fold_scores) + 1)),\n",
    "            y = fold_scores,\n",
    "            mode = 'markers', \n",
    "            name = 'Fold Scores',\n",
    "            marker = dict(size = 27, color=self._color, symbol='diamond'),\n",
    "            text = [f'{score:.3f}' for score in fold_scores],\n",
    "            hovertemplate = 'Fold %{x}: %{text}<extra></extra>',\n",
    "            hoverlabel = dict(font=dict(size=18)) # {'font': {'size': 18}}\n",
    "        ))\n",
    "\n",
    "\n",
    "        #         平均線の追加:\n",
    "# もう一つのトレースを追加し、クロスバリデーションスコアの平均値を示す水平な破線（ダッシュ）をプロット。\n",
    "# 平均線にはホバー情報が表示されません。\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x = [1, len(fold_scores)],\n",
    "            y = [mean_score, mean_score],\n",
    "            mode = 'lines',\n",
    "            name = f'Mean: {mean_score:.3f}',\n",
    "            line = dict(dash = 'dash', color = '#B22222'),\n",
    "            hoverinfo = 'none'\n",
    "        ))\n",
    "\n",
    "\n",
    "# レイアウトの更新:\n",
    "# fig.update_layout(...)で全体のレイアウトを調整。\n",
    "# グラフのタイトルに平均スコアを含め、X軸およびY軸に適切なタイトルを追加。\n",
    "# 背景色や文字色を設定し、グリッド線やゼロ線をグレーにすることで見やすくします。\n",
    "# X軸にはフォールドを示す線形タックを設定。\n",
    "        fig.update_layout(\n",
    "            title = f'{title} | Cross-validation Mean {metric} Score: {mean_score}',\n",
    "            xaxis_title = 'Fold',\n",
    "            yaxis_title = f'{metric} Score',\n",
    "            plot_bgcolor = 'rgba(247, 230, 202, 1)',  \n",
    "            paper_bgcolor = 'rgba(247, 230, 202, 1)',\n",
    "            font = dict(color=self._color), \n",
    "            xaxis = dict(\n",
    "                gridcolor = 'grey',\n",
    "                tickmode = 'linear',\n",
    "                tick0 = 1,\n",
    "                dtick = 1,\n",
    "                range = [0.5, len(fold_scores) + 0.5],\n",
    "                zerolinecolor = 'grey'\n",
    "            ),\n",
    "            \n",
    "            yaxis = dict(\n",
    "                gridcolor = 'grey',\n",
    "                zerolinecolor = 'grey'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "累積ハザード関数の出力は確率そのものではなく、イベントが発生するリスクの累積量です。\n",
    "\n",
    "### 特徴\n",
    "\n",
    "1. **リスクの累積量**:\n",
    "   - 累積ハザード関数は時間の経過に伴うイベント発生の累積リスクを示します。この値が大きいほど、それまでの時間にイベントが発生するリスクが高いことを示しています。\n",
    "\n",
    "2. **確率との関係**:\n",
    "   - 累積ハザード関数から生存関数（特定の時間までイベントが発生しない確率）を計算できます。生存関数は、累積ハザード関数を用いて次の式で表されます：\n",
    "\n",
    "   $$ S(t) = e^{-H(t)} $$\n",
    "\n",
    "   ここで、\\(S(t)\\) は生存関数で、\\(H(t)\\) は累積ハザード関数の値です。このため生存関数は時間とともに減少し、累積したリスクからイベント発生しない確率を導出できます。\n",
    "\n",
    "したがって、累積ハザード関数そのものは確率ではありませんが、この関数を使って生存確率などの確率的な指標を導出することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T08:54:28.842376Z",
     "iopub.status.busy": "2025-01-26T08:54:28.841963Z",
     "iopub.status.idle": "2025-01-26T08:54:28.862623Z",
     "shell.execute_reply": "2025-01-26T08:54:28.861340Z",
     "shell.execute_reply.started": "2025-01-26T08:54:28.842342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Targets:\n",
    "\n",
    "    def __init__(self, data, cat_cols, penalizer, n_splits):\n",
    "        \n",
    "        self.data = data\n",
    "        self.cat_cols = cat_cols\n",
    "        \n",
    "        self._length = len(self.data)\n",
    "        self._penalizer = penalizer\n",
    "        self._n_splits = n_splits\n",
    "\n",
    "    def _prepare_cv(self):\n",
    "        \n",
    "        oof_preds = np.zeros(self._length)\n",
    "            \n",
    "        cv = KFold(n_splits=self._n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        return cv, oof_preds\n",
    "\n",
    "    def validate_model(self, preds, title):\n",
    "            \n",
    "        y_true = self.data[['ID', 'efs', 'efs_time', 'race_group']].copy()\n",
    "        y_pred = self.data[['ID']].copy()\n",
    "        \n",
    "        y_pred['prediction'] = preds\n",
    "            \n",
    "        c_index_score = score(y_true.copy(), y_pred.copy(), 'ID')\n",
    "        # print(f'Overall Stratified C-Index Score for {title}: {c_index_score:.4f}')\n",
    "\n",
    "        return c_index_score\n",
    "\n",
    "    def create_target1(self):  \n",
    "\n",
    "        '''\n",
    "        Inside the CV loop, constant columns are dropped if they exist in a fold. Otherwise, the code produces error:\n",
    "\n",
    "        delta contains nan value(s). Convergence halted. Please see the following tips in the lifelines documentation: \n",
    "\n",
    "        CVループ内では、もしフォールドに定数列が存在すればそれを削除します。そうしないと、以下のようなエラーが発生します：\n",
    "\n",
    "「deltaにNaN値が含まれています。収束が停止しました。詳細はlifelinesのドキュメントの次のヒントを参照してください。」\n",
    "        https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n",
    "        '''\n",
    "\n",
    "        cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "        # Apply one hot encoding to categorical columns\n",
    "        data = pd.get_dummies(self.data, columns=self.cat_cols, drop_first=True).drop('ID', axis=1) \n",
    "\n",
    "        for train_index, valid_index in cv.split(data):\n",
    "\n",
    "            train_data = data.iloc[train_index]\n",
    "            valid_data = data.iloc[valid_index]\n",
    "\n",
    "            # Drop constant columns if they exist\n",
    "            train_data = train_data.loc[:, train_data.nunique() > 1]\n",
    "            valid_data = valid_data[train_data.columns]\n",
    "\n",
    "#             CoxPHFitterは、以下の入力を使用して分析を行い、特定の出力を生成します。\n",
    "# 入力:\n",
    "# 観察時間 (duration_col):\n",
    "# 各サンプルのフォローアップ期間や生存時間を示すデータです。\n",
    "# イベントが発生したかどうか (event_col):\n",
    "# 各サンプルについて、イベント（例えば死亡や故障）が実際に発生したか、打ち切られたかを示す二値データです（1はイベント発生、0は打ち切り）。\n",
    "# 共変量:\n",
    "# ハザードに影響を与えるとされる変数のセットです。例えば、年齢、性別、治療法など。\n",
    "# 出力:\n",
    "# ハザード比（係数） :\n",
    "# 各共変量が生存に与える影響を推定した係数（ログハザード比）を提供します。この係数は、その変数がハザードにどの程度の影響を与えるかを定量化します。\n",
    "# リスク比の解釈:\n",
    "# 推定された係数をもとに、共変量がリスクに与える影響（例えば、ある変数が1単位増えるとリスクが何倍になるか）を解釈します。\n",
    "# 要約統計量と信頼区間:\n",
    "# モデルの適合度や共変量に関する信頼区間、p値などの詳細な統計情報を提供します。\n",
    "# 生存関数とハザード関数の予測:\n",
    "# 新しいデータに対する予測生存関数や基礎ハザード率を計算することができます。\n",
    "# これにより、CoxPHFitterは、異なる共変量が生存時間にどのような影響を及ぼすかを分析し、リスクを定量的に評価することが可能です。\n",
    "\n",
    "            cph = CoxPHFitter(penalizer=self._penalizer)\n",
    "            cph.fit(train_data, duration_col='efs_time', event_col='efs')\n",
    "            \n",
    "            oof_preds[valid_index] = cph.predict_partial_hazard(valid_data)              \n",
    "\n",
    "        self.data['target1'] = oof_preds \n",
    "        self.validate_model(oof_preds, 'Cox') \n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def create_target2(self):        \n",
    "\n",
    "        cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "        for train_index, valid_index in cv.split(self.data):\n",
    "\n",
    "            train_data = self.data.iloc[train_index]\n",
    "            valid_data = self.data.iloc[valid_index]\n",
    "# KaplanMeierFitterを使用すると、特定の時間における生存率を変数として取得することができます。\n",
    "            kmf = KaplanMeierFitter()\n",
    "            kmf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n",
    "\n",
    "            # 検証データ（valid_data）の特定の時間における生存率を予測し、それを配列oof_predsのvalid_index位置に格納\n",
    "            oof_preds[valid_index] = kmf.survival_function_at_times(valid_data['efs_time']).values\n",
    "\n",
    "        self.data['target2'] = oof_preds  \n",
    "        self.validate_model(oof_preds, 'Kaplan-Meier')\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def create_target3(self):        \n",
    "\n",
    "        cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "        for train_index, valid_index in cv.split(self.data):\n",
    "\n",
    "            train_data = self.data.iloc[train_index]\n",
    "            valid_data = self.data.iloc[valid_index]\n",
    "\n",
    "            # Nelson-Aalen累積ハザード推定量を計算するためのクラスのインスタンスを作成\n",
    "            naf = NelsonAalenFitter()\n",
    "            \n",
    "            naf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n",
    "            \n",
    "            oof_preds[valid_index] = -naf.cumulative_hazard_at_times(valid_data['efs_time']).values\n",
    "\n",
    "        self.data['target3'] = oof_preds  \n",
    "        self.validate_model(oof_preds, 'Nelson-Aalen')\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def create_target4(self):\n",
    "\n",
    "        self.data['target4'] = self.data.efs_time.copy()\n",
    "        self.data.loc[self.data.efs == 0, 'target4'] *= -1\n",
    "\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.119086Z",
     "iopub.status.busy": "2025-01-26T10:13:45.118644Z",
     "iopub.status.idle": "2025-01-26T10:13:45.144062Z",
     "shell.execute_reply": "2025-01-26T10:13:45.142968Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.119050Z"
    },
    "papermill": {
     "duration": 0.039648,
     "end_time": "2024-12-13T13:23:12.295144",
     "exception": false,
     "start_time": "2024-12-13T13:23:12.255496",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MD:\n",
    "    \n",
    "    def __init__(self, color, data, cat_cols, early_stop, penalizer, n_splits):\n",
    "        \n",
    "        self.eda = EDA(color, data)\n",
    "        self.targets = Targets(data, cat_cols, penalizer, n_splits)\n",
    "        \n",
    "        self.data = data\n",
    "        self.cat_cols = cat_cols\n",
    "        self._early_stop = early_stop\n",
    "        # self.label_encoders = {}\n",
    "        # self.fold_encoders = {}  # 各Foldのエンコーダーを保存する辞書\n",
    "\n",
    "    def create_targets(self):\n",
    "\n",
    "        self.data = self.targets.create_target1()\n",
    "        self.data = self.targets.create_target2()\n",
    "        self.data = self.targets.create_target3()\n",
    "        self.data = self.targets.create_target4()\n",
    "\n",
    "        return self.data\n",
    "        \n",
    "    def train_model(self, params, target, title, directory_path):\n",
    "        \n",
    "        \n",
    "        for col in self.cat_cols:\n",
    "            self.data[col] = self.data[col].astype('category')\n",
    "            \n",
    "        X = self.data.drop(['ID', 'efs', 'efs_time', 'target1', 'target2', 'target3', 'target4'], axis=1)\n",
    "        y = self.data[target]\n",
    "        w =  1 + self.data[\"efs\"]  # 最小1、最大2の重み\n",
    "        \n",
    "        models, fold_scores = [], []\n",
    "            \n",
    "        cv, oof_preds = self.targets._prepare_cv()\n",
    "\n",
    "        # if title.startswith('XGBoost'):\n",
    "        #     self.fold_encoders[target] = {}\n",
    "        \n",
    "    \n",
    "        for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n",
    "                \n",
    "            X_train = X.iloc[train_index]\n",
    "            X_valid = X.iloc[valid_index]\n",
    "                \n",
    "            y_train = y.iloc[train_index]\n",
    "            y_valid = y.iloc[valid_index]\n",
    "\n",
    "            w_train = w.iloc[train_index]\n",
    "            w_valid = w.iloc[valid_index]\n",
    "    \n",
    "            if title.startswith('LightGBM'):\n",
    "                        \n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                        \n",
    "                model.fit(\n",
    "                    X_train, \n",
    "                    y_train,\n",
    "                    sample_weight=w_train,\n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    eval_metric='rmse',\n",
    "                    callbacks=[lgb.early_stopping(self._early_stop, verbose=1), lgb.log_evaluation(period=1000)]\n",
    "                )\n",
    "\n",
    "                # モデルを保存\n",
    "                joblib.dump(model, directory_path / f'lightgbm_model_{target}_fold{fold}.pkl')\n",
    "                        \n",
    "            elif title.startswith('CatBoost'):\n",
    "                        \n",
    "                model = CatBoostRegressor(**params, verbose=0, cat_features=self.cat_cols)\n",
    "                        \n",
    "                model.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    sample_weight=w_train,\n",
    "                    eval_set=(X_valid, y_valid),\n",
    "                    early_stopping_rounds=self._early_stop, \n",
    "                    verbose=1000\n",
    "                ) \n",
    "                model_file_path = directory_path / f'catboost_model_{target}_fold{fold}.cbm'\n",
    "                if target == 'target4':\n",
    "                    if params['grow_policy'] == 'Depthwise':\n",
    "                        model_file_path = directory_path / f'catboost_model_{target}_Cox1_fold{fold}.cbm'\n",
    "                    else:\n",
    "                        model_file_path = directory_path / f'catboost_model_{target}_Cox2_fold{fold}.cbm'\n",
    "                # モデルを保存\n",
    "                model.save_model(model_file_path)\n",
    "                \n",
    "            elif title.startswith('XGBoost'):\n",
    "                \n",
    "                # このFoldのエンコーダーを保存する辞書\n",
    "                # self.fold_encoders[target][fold] = {}\n",
    "                # for col in self.cat_cols:\n",
    "                #     lbl = LabelEncoder()\n",
    "                #     # 訓練データとバリデーションデータを統合して fit\n",
    "                #     lbl.fit(pd.concat([X_train[col], X_valid[col]], axis=0))\n",
    "                #    # 訓練データで transform\n",
    "                #     X_train[col] = lbl.transform(X_train[col])\n",
    "                #     # 検証データでtransform\n",
    "                #     X_valid[col] = X_valid[col].map(lambda x: lbl.transform([x])[0] if x in lbl.classes_ else -1)\n",
    "\n",
    "                #     self.fold_encoders[target][fold][col] = lbl\n",
    "                # データセットの準備\n",
    "                X_train = xgb.DMatrix(X_train, label=y_train, weight=w_train, enable_categorical=True)\n",
    "                X_valid = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n",
    "\n",
    "                \n",
    "                \n",
    "                # モデルの学習\n",
    "                model = xgb.train(\n",
    "                    params=params,\n",
    "                    dtrain=X_train,\n",
    "                    num_boost_round=10000,                      # 最大イテレーション数\n",
    "                    evals=[(X_train, 'train'), (X_valid, 'valid')],  # 学習データと検証データのセット\n",
    "                    early_stopping_rounds=self._early_stop,    # CatBoostのearly_stopping_roundsに相当\n",
    "                    verbose_eval=1000                          # 学習ログの間隔\n",
    "                )\n",
    "\n",
    "                # モデルの保存\n",
    "                model_file_path = directory_path / f'xgboost_model_{target}_fold{fold}.model'\n",
    "                \n",
    "                # JSON形式で保存\n",
    "                model.save_model(model_file_path.with_suffix(\".json\"))\n",
    "\n",
    "                # モデルの読み込み\n",
    "                # model = xgb.Booster()\n",
    "                # model.load_model(\"xgboost_model_target1_fold0.json\")\n",
    "\n",
    "                \n",
    "                    \n",
    "            models.append(model)\n",
    "                \n",
    "            oof_preds[valid_index] = model.predict(X_valid)\n",
    "\n",
    "            y_true_fold = self.data.iloc[valid_index][['ID', 'efs', 'efs_time', 'race_group']].copy()\n",
    "            y_pred_fold = self.data.iloc[valid_index][['ID']].copy()\n",
    "            \n",
    "            y_pred_fold['prediction'] = oof_preds[valid_index]\n",
    "    \n",
    "            fold_score = score(y_true_fold, y_pred_fold, 'ID')\n",
    "            fold_scores.append(fold_score)\n",
    "    \n",
    "        self.eda._plot_cv(fold_scores, title)\n",
    "        self.targets.validate_model(oof_preds, title)\n",
    "        \n",
    "        return models, oof_preds\n",
    "\n",
    "    def infer_model(self, data, models):\n",
    "        \n",
    "        data = data.drop(['ID'], axis=1)\n",
    "\n",
    "        for col in self.cat_cols:\n",
    "            data[col] = data[col].astype('category')\n",
    "\n",
    "        return np.mean([model.predict(data) for model in models], axis=0)\n",
    "\n",
    "    def infer_model_xgb(self, data, models):\n",
    "        \n",
    "        for col in self.cat_cols:\n",
    "            data[col] = data[col].astype('category')\n",
    "            \n",
    "        data = data.drop(['ID'], axis=1)\n",
    "        # for col in self.cat_cols:\n",
    "        #     for fold, encoders in self.fold_encoders[target].items():\n",
    "        #         lbl = encoders[col]\n",
    "        #         data[col] = data[col].map(lambda x: lbl.transform([x])[0] if x in lbl.classes_ else -1)\n",
    "        # テストデータをDMatrixに変換\n",
    "        dtest = xgb.DMatrix(data, enable_categorical=True)\n",
    "\n",
    "        return np.mean([model.predict(dtest) for model in models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.146761Z",
     "iopub.status.busy": "2025-01-26T10:13:45.146305Z",
     "iopub.status.idle": "2025-01-26T10:13:45.205818Z",
     "shell.execute_reply": "2025-01-26T10:13:45.204734Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.146712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_data = pd.read_pickle('/kaggle/input/2025-01-26-train-data/2025-01-26_09-00-02_train_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.207363Z",
     "iopub.status.busy": "2025-01-26T10:13:45.207043Z",
     "iopub.status.idle": "2025-01-26T10:13:45.214307Z",
     "shell.execute_reply": "2025-01-26T10:13:45.212985Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.207331Z"
    },
    "papermill": {
     "duration": 0.01829,
     "end_time": "2024-12-13T13:23:12.32397",
     "exception": false,
     "start_time": "2024-12-13T13:23:12.30568",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "md = MD(CFG.color, train_data, cat_cols, CFG.early_stop, CFG.penalizer, CFG.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.216652Z",
     "iopub.status.busy": "2025-01-26T10:13:45.216289Z",
     "iopub.status.idle": "2025-01-26T10:13:45.227105Z",
     "shell.execute_reply": "2025-01-26T10:13:45.226135Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.216609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_data = md.create_targets()\n",
    "# 現在の日時を取得してフォーマット\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# train_data.to_pickle(f'{current_datetime}_train_data.pkl')\n",
    "\n",
    "# train_data = pd.read_pickle(f'/kaggle/input/feature-data/{current_datetime}_train_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.228921Z",
     "iopub.status.busy": "2025-01-26T10:13:45.228508Z",
     "iopub.status.idle": "2025-01-26T10:13:45.241075Z",
     "shell.execute_reply": "2025-01-26T10:13:45.239855Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.228869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# display(train_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.336102Z",
     "iopub.status.busy": "2025-01-26T10:13:45.335578Z",
     "iopub.status.idle": "2025-01-26T10:13:45.423905Z",
     "shell.execute_reply": "2025-01-26T10:13:45.422732Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.336064Z"
    },
    "papermill": {
     "duration": 39.008232,
     "end_time": "2024-12-13T13:23:51.341762",
     "exception": false,
     "start_time": "2024-12-13T13:23:12.33353",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "md.eda.distribution_plot('target1', 'Cox Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.426291Z",
     "iopub.status.busy": "2025-01-26T10:13:45.425941Z",
     "iopub.status.idle": "2025-01-26T10:13:45.512861Z",
     "shell.execute_reply": "2025-01-26T10:13:45.511284Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.426256Z"
    },
    "papermill": {
     "duration": 0.180799,
     "end_time": "2024-12-13T13:23:51.538524",
     "exception": false,
     "start_time": "2024-12-13T13:23:51.357725",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "md.eda.distribution_plot('target2', 'Kaplan-Meier Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.514593Z",
     "iopub.status.busy": "2025-01-26T10:13:45.514255Z",
     "iopub.status.idle": "2025-01-26T10:13:45.598164Z",
     "shell.execute_reply": "2025-01-26T10:13:45.596975Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.514559Z"
    },
    "papermill": {
     "duration": 0.169091,
     "end_time": "2024-12-13T13:23:51.718819",
     "exception": false,
     "start_time": "2024-12-13T13:23:51.549728",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "md.eda.distribution_plot('target3', 'Nelson-Aalen Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.599962Z",
     "iopub.status.busy": "2025-01-26T10:13:45.599601Z",
     "iopub.status.idle": "2025-01-26T10:13:45.682616Z",
     "shell.execute_reply": "2025-01-26T10:13:45.681397Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.599929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "md.eda.distribution_plot('target4', 'Target for Cox-Loss Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.685478Z",
     "iopub.status.busy": "2025-01-26T10:13:45.685120Z",
     "iopub.status.idle": "2025-01-26T10:13:45.744517Z",
     "shell.execute_reply": "2025-01-26T10:13:45.743381Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.685443Z"
    },
    "papermill": {
     "duration": 0.079694,
     "end_time": "2024-12-13T13:23:51.808887",
     "exception": false,
     "start_time": "2024-12-13T13:23:51.729193",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fe.info(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011062,
     "end_time": "2024-12-13T13:23:51.83194",
     "exception": false,
     "start_time": "2024-12-13T13:23:51.820878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Models with Cox Target</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.746198Z",
     "iopub.status.busy": "2025-01-26T10:13:45.745866Z",
     "iopub.status.idle": "2025-01-26T10:13:45.752161Z",
     "shell.execute_reply": "2025-01-26T10:13:45.750955Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.746165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ディレクトリ名を作成\n",
    "directory_name = f\"model_{current_datetime}\"\n",
    "\n",
    "# ディレクトリのパスを指定\n",
    "directory_path = Path(directory_name)\n",
    "\n",
    "# ディレクトリを作成\n",
    "directory_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:13:45.753922Z",
     "iopub.status.busy": "2025-01-26T10:13:45.753430Z",
     "iopub.status.idle": "2025-01-26T10:28:36.980381Z",
     "shell.execute_reply": "2025-01-26T10:28:36.977908Z",
     "shell.execute_reply.started": "2025-01-26T10:13:45.753848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb1_models, xgb1_oof_preds = md.train_model(CFG.xgb_params, target='target1', title='XGBoost', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:28:36.983796Z",
     "iopub.status.busy": "2025-01-26T10:28:36.983202Z",
     "iopub.status.idle": "2025-01-26T10:28:37.183567Z",
     "shell.execute_reply": "2025-01-26T10:28:37.181432Z",
     "shell.execute_reply.started": "2025-01-26T10:28:36.983723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb1_preds = md.infer_model_xgb(test_data, xgb1_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T10:28:37.185846Z",
     "iopub.status.busy": "2025-01-26T10:28:37.185419Z"
    },
    "papermill": {
     "duration": 4808.205155,
     "end_time": "2024-12-13T14:44:00.048534",
     "exception": false,
     "start_time": "2024-12-13T13:23:51.843379",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ctb1_models, ctb1_oof_preds = md.train_model(CFG.ctb_params_1, target='target1', title='CatBoost', directory_path=directory_path)\n",
    "# 現在のイテレーション数（エポック数）, トレーニングデータに対する損失関数の値, 検証データに対する損失関数の, 検証データでのこれまでの最良の損失関数の値と、それが達成されたイテレーション番号, トレーニングの開始から現在のイテレーションまでに経過した合計時間, 現在の速度でトレーニングが継続した場合に予想される残り時間 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lgb1_models, lgb1_oof_preds = md.train_model(CFG.lgb_params_1, target='target1', title='LightGBM', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.071675,
     "end_time": "2024-12-13T14:48:51.427728",
     "exception": false,
     "start_time": "2024-12-13T14:48:51.356053",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ctb1_preds = md.infer_model(test_data, ctb1_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lgb1_preds = md.infer_model(test_data, lgb1_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010491,
     "end_time": "2024-12-13T14:48:51.662203",
     "exception": false,
     "start_time": "2024-12-13T14:48:51.651712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Models with Kaplan-Meier Target</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2470.336385,
     "end_time": "2024-12-13T15:30:02.009405",
     "exception": false,
     "start_time": "2024-12-13T14:48:51.67302",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ctb2_models, ctb2_oof_preds = md.train_model(CFG.ctb_params_23, target='target2', title='CatBoost', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 38.457069,
     "end_time": "2024-12-13T15:30:40.477711",
     "exception": false,
     "start_time": "2024-12-13T15:30:02.020642",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lgb2_models, lgb2_oof_preds = md.train_model(CFG.lgb_params_23, target='target2', title='LightGBM', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.073959,
     "end_time": "2024-12-13T15:30:40.563068",
     "exception": false,
     "start_time": "2024-12-13T15:30:40.489109",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ctb2_preds = md.infer_model(test_data, ctb2_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.170712,
     "end_time": "2024-12-13T15:30:40.747856",
     "exception": false,
     "start_time": "2024-12-13T15:30:40.577144",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lgb2_preds = md.infer_model(test_data, lgb2_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb2_models, xgb2_oof_preds = md.train_model(CFG.xgb_params, target='target2', title='XGBoost', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb2_preds = md.infer_model_xgb(test_data, xgb2_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011016,
     "end_time": "2024-12-13T15:30:40.770359",
     "exception": false,
     "start_time": "2024-12-13T15:30:40.759343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Models with Nelson-Aalen Target</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2199.149038,
     "end_time": "2024-12-13T16:07:19.930791",
     "exception": false,
     "start_time": "2024-12-13T15:30:40.781753",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ctb3_models, ctb3_oof_preds = md.train_model(CFG.ctb_params_23, target='target3', title='CatBoost', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 35.221097,
     "end_time": "2024-12-13T16:07:55.163635",
     "exception": false,
     "start_time": "2024-12-13T16:07:19.942538",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lgb3_models, lgb3_oof_preds = md.train_model(CFG.lgb_params_23, target='target3', title='LightGBM', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.06646,
     "end_time": "2024-12-13T16:07:55.241797",
     "exception": false,
     "start_time": "2024-12-13T16:07:55.175337",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ctb3_preds = md.infer_model(test_data, ctb3_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.164669,
     "end_time": "2024-12-13T16:07:55.41872",
     "exception": false,
     "start_time": "2024-12-13T16:07:55.254051",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lgb3_preds = md.infer_model(test_data, lgb3_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb3_models, xgb3_oof_preds = md.train_model(CFG.xgb_params, target='target2', title='XGBoost', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb3_preds = md.infer_model_xgb(test_data, xgb2_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Cox-Loss Models</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cox1_models, cox1_oof_preds = md.train_model(CFG.cox1_params, target='target4', title='CatBoost', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cox2_models, cox2_oof_preds = md.train_model(CFG.cox2_params, target='target4', title='CatBoost', directory_path=directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  # データフレームに変換\n",
    "# importance_df = pd.DataFrame(columns = ['Feature', 'Importance'])\n",
    "\n",
    "# for module_models in models:\n",
    "#     for model in module_models:\n",
    "\n",
    "#         # 特徴量の重要度を取得\n",
    "#         importance = model.feature_importances_\n",
    "#         features = train_data.drop(['ID', 'efs', 'efs_time', 'target1', 'target2', 'target3', 'target4'], axis=1).columns  # 特徴量の名前を取得\n",
    "        \n",
    "#         # データフレームに変換\n",
    "#         importance_df_temp = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
    "\n",
    "#         # 重要度の合計を計算\n",
    "#         total_importance = importance_df_temp['Importance'].sum()\n",
    "        \n",
    "#         # 正規化を行う\n",
    "#         importance_df_temp['Normalized_Importance'] = importance_df_temp['Importance'] / total_importance\n",
    "        \n",
    "\n",
    "        \n",
    "# # 重要度でソート\n",
    "# importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # 数値で表示\n",
    "# print(importance_df)\n",
    "# # 特徴量の重要度を可視化\n",
    "        \n",
    "#         # plt.figure(figsize=(10, 10))\n",
    "#         # plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "#         # plt.xlabel('Importance')\n",
    "#         # plt.title('Feature Importance')\n",
    "#         # plt.gca().invert_yaxis()  # 特徴量名が上から表示されるように反転\n",
    "#         # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量の重要度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    ctb1_models, \n",
    "    lgb1_models,\n",
    "    xgb1_models,\n",
    "    ctb2_models, \n",
    "    lgb2_models,\n",
    "    xgb2_models,\n",
    "    ctb3_models, \n",
    "    lgb3_models,\n",
    "    xgb3_models,\n",
    "    cox1_models,\n",
    "    cox2_models\n",
    "]\n",
    "\n",
    "models_name = [\n",
    "    \"ctb1_models\", \n",
    "    \"lgb1_models\", \n",
    "    \"ctb2_models\", \n",
    "    \"lgb2_models\", \n",
    "    \"ctb3_models\", \n",
    "    \"lgb3_models\", \n",
    "    \"cox1_models\",\n",
    "    \"cox2_models\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# for module_models, model_name in zip(models, models_name):\n",
    "#     # 特徴量とその重要度を保存するリストを初期化\n",
    "#     importance_list = []\n",
    "#     for model in module_models:\n",
    "        \n",
    "#         # 特徴量の重要度を取得\n",
    "#         importance = model.feature_importances_\n",
    "#         features = train_data.drop(['ID', 'efs', 'efs_time', 'target1', 'target2', 'target3', 'target4'], axis=1).columns  # 特徴量の名前を取得\n",
    "        \n",
    "#         # データフレームに変換\n",
    "#         importance_df_temp = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
    "\n",
    "#         # 重要度の合計を計算\n",
    "#         total_importance = importance_df_temp['Importance'].sum()\n",
    "        \n",
    "#         # 正規化を行う\n",
    "#         importance_df_temp['Normalized_Importance'] = importance_df_temp['Importance'] / total_importance\n",
    "\n",
    "#         # 正規化された重要度をリストに追加\n",
    "#         for idx, row in importance_df_temp.iterrows():\n",
    "#             feature = row['Feature']\n",
    "#             normalized_importance = row['Normalized_Importance']\n",
    "#             importance_list.append({'Feature': feature, 'Normalized_Importance': normalized_importance})\n",
    "\n",
    "#     # DataFrameを作成\n",
    "#     importance_df = pd.DataFrame(importance_list)\n",
    "    \n",
    "#     # 各特徴量の正規化された重要度を合計\n",
    "#     final_importance_df = importance_df.groupby('Feature', as_index=False).sum()\n",
    "    \n",
    "#     # 重要度でソート\n",
    "#     final_importance_df = final_importance_df.sort_values(by='Normalized_Importance', ascending=False)\n",
    "    \n",
    "#     # 最終的な正規化された重要度を表示\n",
    "#     print(model_name)\n",
    "#     print(final_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# models = [\n",
    "#     ctb1_models,\n",
    "#     lgb1_models,\n",
    "#     xgb1_models,\n",
    "#     ctb2_models,\n",
    "#     lgb2_models,\n",
    "#     xgb2_models,\n",
    "#     ctb3_models,\n",
    "#     lgb3_models,\n",
    "#     xgb3_models,\n",
    "#     cox1_models,\n",
    "#     cox2_models\n",
    "# ]\n",
    "\n",
    "# models_name = [\n",
    "#     \"ctb1_models\",\n",
    "#     \"lgb1_models\",\n",
    "#     \"ctb2_models\",\n",
    "#     \"lgb2_models\",\n",
    "#     \"ctb3_models\",\n",
    "#     \"lgb3_models\",\n",
    "#     \"cox1_models\",\n",
    "#     \"cox2_models\"\n",
    "# ]\n",
    "\n",
    "# # 不要な列をドロップしたあとの \"学習に使う特徴量\" を取得\n",
    "# features = train_data.drop(\n",
    "#     ['ID', 'efs', 'efs_time', 'target1', 'target2', 'target3', 'target4'],\n",
    "#     axis=1\n",
    "# ).columns\n",
    "\n",
    "# for module_models, model_name in zip(models, models_name):\n",
    "#     # foldごとの特徴量重要度データフレームをまとめるリスト\n",
    "#     fold_importance_dfs = []\n",
    "    \n",
    "#     # クロスバリデーションの各モデルを順番に取り出す\n",
    "#     for model in module_models:\n",
    "#         # この fold の生の重要度を取得\n",
    "#         importance = model.feature_importances_\n",
    "\n",
    "#         # DataFrameに変換\n",
    "#         importance_df_temp = pd.DataFrame({\n",
    "#             'Feature': features,\n",
    "#             'Importance': importance\n",
    "#         })\n",
    "\n",
    "#         # この fold 内での合計重要度\n",
    "#         total_importance = importance_df_temp['Importance'].sum()\n",
    "\n",
    "#         # fold 内で正規化（sum=1 となるようにする）\n",
    "#         # 万が一、total_importance が 0 の場合はエラー回避\n",
    "#         if total_importance == 0:\n",
    "#             importance_df_temp['Normalized_Importance'] = 0\n",
    "#         else:\n",
    "#             importance_df_temp['Normalized_Importance'] = (\n",
    "#                 importance_df_temp['Importance'] / total_importance\n",
    "#             )\n",
    "\n",
    "#         # この fold の結果をリストに追加\n",
    "#         fold_importance_dfs.append(importance_df_temp[['Feature', 'Normalized_Importance']])\n",
    "\n",
    "#     # 全 fold の重要度データフレームを結合\n",
    "#     all_folds_importance = pd.concat(fold_importance_dfs, axis=0)\n",
    "\n",
    "#     # 特徴量ごとに「平均の正規化重要度」を計算\n",
    "#     # sum ではなく mean をとる方が、fold 数によらず比較しやすい\n",
    "#     final_importance_df = (\n",
    "#         all_folds_importance\n",
    "#         .groupby('Feature', as_index=False)['Normalized_Importance']\n",
    "#         .mean()\n",
    "#         .sort_values(by='Normalized_Importance', ascending=False)\n",
    "#     )\n",
    "\n",
    "#     # 結果を表示\n",
    "#     print(f\"Feature Importance for {model_name}\")\n",
    "#     print(final_importance_df)\n",
    "#     print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  # データフレームに変換\n",
    "# importance_df = pd.DataFrame(columns = ['Feature', 'Importance'])\n",
    "# importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cox1_preds = md.infer_model(test_data, cox1_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cox2_preds = md.infer_model(test_data, cox2_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01174,
     "end_time": "2024-12-13T16:07:55.442686",
     "exception": false,
     "start_time": "2024-12-13T16:07:55.430946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"background-color: rgb(247, 230, 202); font-size: 300%; text-align: center; border-radius: 40px 40px; color: rgb(162, 87, 79); font-weight: bold; font-family: 'Roboto'; border: 4px solid rgb(162, 87, 79);\">Ensemble Model</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(247, 230, 202); border: 4px solid rgb(162, 87, 79); border-radius: 40px; padding: 20px; font-family: 'Roboto'; color: rgb(162, 87, 79); text-align: left; font-size: 140%;\">\n",
    "    <b>Calculate C-Index score for Ensemble model using Out-of-Fold (OOF) predictions.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "oof_preds = [\n",
    "    ctb1_oof_preds, \n",
    "    lgb1_oof_preds,\n",
    "    xgb1_oof_preds,\n",
    "    ctb2_oof_preds, \n",
    "    lgb2_oof_preds,\n",
    "    xgb2_oof_preds,\n",
    "    ctb3_oof_preds, \n",
    "    lgb3_oof_preds,\n",
    "    xgb3_oof_preds,\n",
    "    cox1_oof_preds,\n",
    "    cox2_oof_preds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ranked_oof_preds = np.array([rankdata(p) for p in oof_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ensemble_oof_preds = np.dot(CFG.weights, ranked_oof_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "md.targets.validate_model(ensemble_oof_preds, 'Ensemble Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# best_score = 0\n",
    "# best_i = 0\n",
    "# best_k = 0\n",
    "# for i in range(51):\n",
    "#     for k in range(51-i):\n",
    "#         for j in range(51-i-k):\n",
    "#             if k >= 0 and i >= 0 and j >= 0 and 51-i-k-j >= 0:\n",
    "#                 CFG.weights = [i, i, k, k, j, j, 101-i-k-j, 101-i-k-j]\n",
    "#                 ensemble_oof_preds = np.dot(CFG.weights, ranked_oof_preds)\n",
    "#                 curent_score = md.targets.validate_model(ensemble_oof_preds, 'Ensemble Model')\n",
    "#                 if best_score < curent_score:\n",
    "#                     best_i = i\n",
    "#                     best_k = k\n",
    "#                     best_j = j\n",
    "#                     best_score = curent_score\n",
    "\n",
    "# print(f\"best_score->{best_score}, best_i->{best_i}, best_k->{best_k}, best_j->{best_j}\")\n",
    "# CFG.weights = [best_i, best_i, best_k, best_k, best_j, best_j, 101-best_i-best_k-best_j, 101-best_i-best_k-best_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "# 目的関数の定義\n",
    "def objective(trial):\n",
    "    # 各モデルの重みを提案 (0.0～1.0 の範囲)\n",
    "    weights = [trial.suggest_float(f'weight_{i}', 0.05, 1.0) for i in range(len(ranked_oof_preds))]\n",
    "    \n",
    "    # 重みの正規化（合計が1になるようにスケーリング）\n",
    "    weights = np.array(weights)\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    # アンサンブル予測を計算 (加重平均)\n",
    "    ensemble_oof_preds = np.dot(CFG.weights, ranked_oof_preds)\n",
    "    \n",
    "    curent_score = md.targets.validate_model(ensemble_oof_preds, 'Ensemble Model')\n",
    "    \n",
    "    return curent_score  # 最大化が目標\n",
    "\n",
    "# Optunaによる最適化\n",
    "study = optuna.create_study(direction='maximize')  # AUCを最大化\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# 最適な重み\n",
    "print(\"Best Weights:\", study.best_params)\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "\n",
    "for i in range(len(CFG.weights)):\n",
    "    CFG.weights[i] = study.best_params[f'weight_{i}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# oof_preds_name = [\n",
    "#     \"ctb1_oof_preds\", \n",
    "#     \"lgb1_oof_preds\", \n",
    "#     \"ctb2_oof_preds\", \n",
    "#     \"lgb2_oof_preds\", \n",
    "#     \"ctb3_oof_preds\", \n",
    "#     \"lgb3_oof_preds\", \n",
    "#     \"cox1_oof_preds\",\n",
    "#     \"cox2_oof_preds\"\n",
    "# ]\n",
    "\n",
    "# for i, oof_pred in enumerate(oof_preds):\n",
    "#     means = []\n",
    "#     for pred in oof_pred:\n",
    "#         means.append(np.mean(pred))\n",
    "        \n",
    "#     print(f\"{oof_preds_name[i]}->{np.mean(means)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CFG.weights[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(247, 230, 202); border: 4px solid rgb(162, 87, 79); border-radius: 40px; padding: 20px; font-family: 'Roboto'; color: rgb(162, 87, 79); text-align: left; font-size: 140%;\">\n",
    "    <b>Ensemble predictions for the test data.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.019704,
     "end_time": "2024-12-13T16:07:55.474253",
     "exception": false,
     "start_time": "2024-12-13T16:07:55.454549",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds = [\n",
    "    ctb1_preds, \n",
    "    lgb1_preds,\n",
    "    xgb1_preds,\n",
    "    ctb2_preds, \n",
    "    lgb2_preds,\n",
    "    xgb2_preds,\n",
    "    ctb3_preds, \n",
    "    lgb3_preds,\n",
    "    xgb3_preds,\n",
    "    cox1_preds,\n",
    "    cox2_preds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.023045,
     "end_time": "2024-12-13T16:07:55.510197",
     "exception": false,
     "start_time": "2024-12-13T16:07:55.487152",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# rankdataデータセットの要素を順位付けするための関数です。この関数を使用することで、数値データをランキング形式に変換\n",
    "# 例えば、リスト [10, 20, 20, 30] に対してランク付けを行うと [1.0, 3.0, 3.0, 4.0] という結果が得られます。\n",
    "\n",
    "ranked_preds = np.array([rankdata(p) for p in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.020168,
     "end_time": "2024-12-13T16:07:55.542141",
     "exception": false,
     "start_time": "2024-12-13T16:07:55.521973",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ensemble_preds = np.dot(CFG.weights, ranked_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.043145,
     "end_time": "2024-12-13T16:07:55.596856",
     "exception": false,
     "start_time": "2024-12-13T16:07:55.553711",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "subm_data = pd.read_csv(CFG.subm_path)\n",
    "subm_data['prediction'] = ensemble_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027273,
     "end_time": "2024-12-13T16:07:55.636582",
     "exception": false,
     "start_time": "2024-12-13T16:07:55.609309",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "subm_data.to_csv('submission.csv', index=False)\n",
    "display(subm_data.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "datasetId": 6544534,
     "sourceId": 10575779,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 211253469,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10108.862432,
   "end_time": "2024-12-13T16:07:57.072385",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-13T13:19:28.209953",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
