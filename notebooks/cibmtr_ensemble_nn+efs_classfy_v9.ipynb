{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"},{"sourceId":211253469,"sourceType":"kernelVersion"},{"sourceId":211322530,"sourceType":"kernelVersion"},{"sourceId":219607918,"sourceType":"kernelVersion"},{"sourceId":224040652,"sourceType":"kernelVersion"}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":10108.862432,"end_time":"2024-12-13T16:07:57.072385","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-13T13:19:28.209953","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<!-- <div style=\"background-color: rgb(247, 230, 202); border: 4px solid rgb(162, 87, 79); border-radius: 40px; padding: 20px; font-family: 'Roboto'; color: rgb(162, 87, 79); text-align: left; font-size: 120%;\">\n    <ul style=\"list-style-type: square; padding-left: 20px;\">\n        <li style=\"margin-top: 10px;\">HLA columns are recalculated as per <a href=\"https://www.kaggle.com/code/albansteff/cibmtr-eda-ensemble-model-recalculate-hla\" style=\"color: #A2574F; text-decoration: underline;\">this</a> notebook.</li>\n        <li style=\"margin-top: 10px;\">Missing values are replaced with:\n            <ul style=\"list-style-type: circle; margin-top: 10px; margin-bottom: 10px;\">\n                <li>-1 for numeric columns</li>\n                <li>Unknown for categorical columns</li>\n            </ul>\n        </li>\n        <li style=\"margin-top: 10px;\">\n            LightGBM and CatBoost are trained on 3 different targets, estimated from the survival models:\n            <ul style=\"list-style-type: circle; margin-top: 10px; margin-bottom: 10px;\">\n                <li>Cox</li>\n                <li>Kaplan-Meier</li>\n                <li>Nelson-Aalen</li>\n            </ul>\n        </li>\n        <li style=\"margin-top: 10px;\">Two additional CatBoost model are trained, with Cox loss function.</li>\n        <li style=\"margin-top: 10px;\">As per <a href=\"https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/553061\" style=\"color: #A2574F; text-decoration: underline;\">this</a> discussion post, the target is consisted of the Out-of-Fold predictions of the survival models on the validation folds to prevent target leakage.</li>\n        <li style=\"margin-top: 10px;\">\n            The ensemble prediction for each sample is computed as:\n            <ul style=\"list-style-type: circle; margin-top: 10px; margin-bottom: 10px;\">\n                <p style=\"margin-top: 10px; font-size: 110%; color: #A2574F; font-family: 'Roboto'; text-align: left;\">\n                    $ \\text{preds}_{\\text{ensemble}} = \\sum_{i=1}^{n} w_i \\cdot \\text{rankdata}(\\text{preds}_i) $\n                </p>\n                where $n$ is the number of models, $w_i$ is the weight assigned to the $i$-th model, and $\\text{rankdata}(\\text{preds}_i)$ is the rank of predictions from the $i$-th model.\n            </ul>\n        </li>\n        <li style=\"margin-top: 10px;\">Last but not least, since the competition metric evaluates only the order of predictions and not their magnitude, the model weights are not required to sum to 1, nor should the predictions fall within a predefined range.</li>\n    </ul>\n</div> -->","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 150%; text-align: left; border-radius: 40px 40px; color: rgb(232, 95, 9); font-weight: bold;\">参考にしたNotebook</p>  ","metadata":{}},{"cell_type":"markdown","source":"リンク  \n- アンサンブル：https://www.kaggle.com/code/andreasbis/cibmtr-eda-ensemble-model  \n- NN：https://www.kaggle.com/code/dreamingtree/single-nn-with-pairwise-ranking-loss-0-689-lb","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 120%; text-align: left; border-radius: 40px 40px; color: rgb(244, 62, 7); font-weight: bold;\">\n概要\n</p>  \n\n- 特徴量エンジニアリング\n    - 特徴量作成\n        - ``hla_high_xxx``や``hla_low_xxx``といった特徴量は自分で作り直す\n- 欠損値補完\n    - 数値列は``-1``\n    - カテゴリ列は``Unknown``\n- モデル\n    - 種類（計８つのモデル）\n        - ２つの予測モデル（LightGBMとCatBoost）×３つの生存確率推定モデル\n            - コックス比例ハザードモデル\n                - モデル概要\n                    - セミパラメトリックモデル\n                    - 共変量（複数の説明変数）を考慮できる\n                    - 返す値は「相対的なリスクの大きさ」\n                - データ処理\n                    - １値しか取らないカラムを削除\n                    - カテゴリ変数について``one hot encoding``を実施\n            - カプラン・マイヤー推定量\n                - ノンパラメトリックモデル（データからそのまま推定・特定の確率分布を仮定しない）\n                - 共変量を考慮しない単純な推定\n                - 生存確率を計算（０〜１）\n                $$\n                \\begin{align*}\n                S(t) &= \\prod_{t_i \\leq t} \\left( 1 - \\frac{d_i}{n_i} \\right)  \n                \\end{align*}\n                $$\n\n                $$\n                \\begin{align*}\n                t_i &\\text{ はイベント発生時点} \\\\\n                d_i &\\text{ はその時点でのイベント発生個体数} \\\\\n                n_i &\\text{ はその時点での生存個体数}\n                \\end{align*}\n                $$\n            - ネルソン・アーレン推定量\n                - ノンパラメトリックモデル（データからそのまま推定・特定の確率分布を仮定しない）\n                - 共変量を考慮しない単純な推定\n                - 累積ハザード確率を計算（０〜１）\n                $$\n                \\begin{align*}\n                H(t) &= \\sum_{t_i \\leq t} \\frac{d_i}{n_i}\n                \\end{align*}\n                $$\n                $$\n                \\begin{align*}\n                t_i &\\text{ はイベント発生時点} \\\\\n                d_i &\\text{ はその時点でのイベント発生個体数} \\\\\n                n_i &\\text{ はその時点での生存個体数}\n                \\end{align*}\n                $$\n            - カプラン・マイヤーとネルソン・アーレンの違い\n                - 「死亡のイベントが発生した時間に関数値が変化するという性質」は共通\n                - ネルソン・アーレンは、「小さいサイズの標本に対してよりよい性質をもつ」\n        - CatBoost（木の深さ優先 / 損失減少優先）× Cox回帰に基づく損失関数\n            - CatBoost\n                - ハイパーパラメータの設定の違い（木の深さ優先 / 損失減少優先）により2種類作成\n                - 損失関数として「Cox回帰」を指定\n            - Cox回帰に基づく損失関数\n                - targetの設定\n                    - 特定のイベントが発生せずに生存している期間を測定する指標を用いる\n                    - イベントが発生しなかったデータについては生存時間を負にすることで、イベントが発生しなかったデータポイントを明確に区別できる\n                - 予測モデル\n                    - CatBoostにおいて、Cox回帰に基づく損失関数を使用\n    - 特徴\n        - ターゲットリークを防ぐため、ターゲットは検証フォールドにおけるサバイバルモデルのOut-of-Fold予測で構成される\n            - https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions/discussion/553061\n        - アンサンブル予測は各サンプルについて以下のように計算される\n            - アンサンブル予測 = Σ(wi * rankdata(predsi)) \n                - ここで、wiは i 番目のモデルに割り当てられた重み（<font color = \"red\">分析者が指定</font>）\n                - rankdata(predsi)は i 番目のモデルの予測のランク\n            - 競争メトリックが予測の大きさではなく順序のみを評価するため、モデルの重みは1に合計する必要はなく、予測も予め定義された範囲内である必要はありません。\n- 精度評価\n    - 層別C統計量を使用\n        - 元のノートブックではmetricライブラリのscore関数を使用していたが、利用できないので代替\n        - 層別C統計量は以前から使用していたもの\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">ライブラリ</p>","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n!pip install /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_lightning-2.4.0-py3-none-any.whl\n!pip install /kaggle/input/download-lightning-and-pytorch-tabular/scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install /kaggle/input/download-lightning-and-pytorch-tabular/torchmetrics-1.5.2-py3-none-any.whl\n!pip install /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabnet-4.1.0-py3-none-any.whl\n!pip install /kaggle/input/download-lightning-and-pytorch-tabular/einops-0.7.0-py3-none-any.whl\n!pip install /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabular-1.1.1-py2.py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:37:36.631546Z","iopub.execute_input":"2025-02-22T14:37:36.631893Z","iopub.status.idle":"2025-02-22T14:38:39.476405Z","shell.execute_reply.started":"2025-02-22T14:37:36.631862Z","shell.execute_reply":"2025-02-22T14:38:39.474761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom metric import score\nimport pandas as pd\nimport numpy as np\nfrom warnings import filterwarnings\nimport joblib\nfrom pathlib import Path\nfilterwarnings('ignore')\n\nROOT_DATA_PATH = Path(r\"/kaggle/input/equity-post-HCT-survival-predictions\")\n\npd.set_option('display.max_columns', 100)\n\ntrain = pd.read_csv(ROOT_DATA_PATH.joinpath(\"train.csv\"))\ntest = pd.read_csv(ROOT_DATA_PATH.joinpath(\"test.csv\"))\n\nCATEGORICAL_VARIABLES = [\n    # Graft and HCT reasons\n    'dri_score', 'graft_type', 'prod_type', 'prim_disease_hct',\n\n    # Patient health status (risk factors)\n    'psych_disturb', 'diabetes', 'arrhythmia', 'vent_hist', 'renal_issue', 'pulm_moderate',\n    'pulm_severe', 'obesity', 'hepatic_mild', 'hepatic_severe', 'peptic_ulcer', 'rheum_issue',\n    'cardiac', 'prior_tumor', 'mrd_hct', 'tbi_status', 'cyto_score', 'cyto_score_detail', \n\n    # Patient demographics\n    'ethnicity', 'race_group',\n\n    # Biological matching with donor\n    'sex_match', 'donor_related', 'cmv_status', 'tce_imm_match', 'tce_match', 'tce_div_match',\n\n    # Medication/operation related data\n    'melphalan_dose', 'rituximab', 'gvhd_proph', 'in_vivo_tcd', 'conditioning_intensity'\n]\n\nHLA_COLUMNS = [\n    'hla_match_a_low', 'hla_match_a_high',\n    'hla_match_b_low', 'hla_match_b_high',\n    'hla_match_c_low', 'hla_match_c_high',\n    'hla_match_dqb1_low', 'hla_match_dqb1_high',\n    'hla_match_drb1_low', 'hla_match_drb1_high',\n    \n    # Matching at HLA-A(low), -B(low), -DRB1(high)\n    'hla_nmdp_6',\n    # Matching at HLA-A,-B,-DRB1 (low or high)\n    'hla_low_res_6', 'hla_high_res_6',\n    # Matching at HLA-A, -B, -C, -DRB1 (low or high)\n    'hla_low_res_8', 'hla_high_res_8',\n    # Matching at HLA-A, -B, -C, -DRB1, -DQB1 (low or high)\n    'hla_low_res_10', 'hla_high_res_10'\n]\n\nOTHER_NUMERICAL_VARIABLES = ['year_hct', 'donor_age', 'age_at_hct', 'comorbidity_score', 'karnofsky_score']\nNUMERICAL_VARIABLES = HLA_COLUMNS + OTHER_NUMERICAL_VARIABLES\n\nTARGET_VARIABLES = ['efs_time', 'efs']\nID_COLUMN = [\"ID\"]\n\n\ndef preprocess_data(df):\n    df[CATEGORICAL_VARIABLES] = df[CATEGORICAL_VARIABLES].fillna(\"Unknown\")\n    df[OTHER_NUMERICAL_VARIABLES] = df[OTHER_NUMERICAL_VARIABLES].fillna(df[OTHER_NUMERICAL_VARIABLES].median())\n\n    return df\n\ntrain = preprocess_data(train)\ntest = preprocess_data(test)\n\n\ndef features_engineering(df):\n    # Change year_hct to relative year from 2000\n    df['year_hct'] = df['year_hct'] - 2000\n    \n    return df\n\n\ntrain = features_engineering(train)\ntest = features_engineering(test)\n\ntrain[CATEGORICAL_VARIABLES] = train[CATEGORICAL_VARIABLES].astype('category')\ntest[CATEGORICAL_VARIABLES] = test[CATEGORICAL_VARIABLES].astype('category')\n\nFEATURES = train.drop(columns=['ID', 'efs', 'efs_time']).columns.tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBRegressor, XGBClassifier\nimport xgboost\nprint(\"Using XGBoost version\",xgboost.__version__)\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\n\nFOLDS = 5\nkf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\noof_xgb = np.zeros(len(train))\npred_efs = np.zeros(len(test))\n\nfor i, (train_index, test_index) in enumerate(kf.split(train, train[\"efs\"])):\n\n    print(\"#\"*25)\n    print(f\"### Fold {i+1}\")\n    print(\"#\"*25)\n    \n    x_train = train.loc[train_index, FEATURES].copy()\n    y_train = train.loc[train_index, \"efs\"]\n    x_valid = train.loc[test_index, FEATURES].copy()\n    y_valid = train.loc[test_index, \"efs\"]\n    x_test = test[FEATURES].copy()\n\n    model_xgb = XGBClassifier(\n        device=\"cuda\",\n        max_depth=3,  \n        colsample_bytree=0.7129400756425178, \n        subsample=0.8185881823156917, \n        n_estimators=20_000, \n        learning_rate=0.04425768131771064,  \n        eval_metric=\"auc\", \n        early_stopping_rounds=50, \n        objective='binary:logistic',\n        scale_pos_weight=1.5379160847615545,  \n        min_child_weight=4,\n        enable_categorical=True,\n        gamma=3.1330719334577584\n    )\n    model_xgb.fit(\n        x_train, y_train,\n        eval_set=[(x_valid, y_valid)],  \n        verbose=100\n    )\n\n\n    # INFER OOF (Probabilities -> Binary)\n    oof_xgb[test_index] = (model_xgb.predict_proba(x_valid)[:, 1] > 0.5).astype(int)\n    # INFER TEST (Probabilities -> Average Probs)\n    pred_efs += model_xgb.predict_proba(x_test)[:, 1]\n\n# COMPUTE AVERAGE TEST PREDS\npred_efs = (pred_efs / FOLDS > 0.5).astype(int)\n\n# EVALUATE PERFORMANCE\naccuracy = accuracy_score(train[\"efs\"], oof_xgb)\nf1 = f1_score(train[\"efs\"], oof_xgb)\nroc_auc = roc_auc_score(train[\"efs\"], oof_xgb)\ntrain_global = train.copy()\ntest_global = test.copy()\ntrain_global[\"efs_xgb_pred\"] = oof_xgb\ntest_global[\"efs_xgb_pred\"] = pred_efs\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC AUC Score: {roc_auc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom torch.utils.data import TensorDataset\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore')\n\n\ndef get_X_cat(df, cat_cols, transformers=None):\n    \"\"\"\n    Apply a specific categorical data transformer or a LabelEncoder if None.\n    すべてのデータをスキャンし、ユニークなカテゴリを取得して、対応する整数を割り当てる。\n    \"\"\"\n    if transformers is None:\n        transformers = [LabelEncoder().fit(df[col]) for col in cat_cols]\n    return transformers, np.array(\n        [transformer.transform(df[col]) for col, transformer in zip(cat_cols, transformers)]\n    ).T\n\n\ndef preprocess_data(train, val):\n    \"\"\"\n    Standardize numerical variables and transform (Label-encode) categoricals.\n    Fill NA values with mean for numerical.\n    Create torch dataloaders to prepare data for training and evaluation.\n    数値変数を標準化し、カテゴリ変数を変換（ラベルエンコード）する。\n数値データの NA（欠損値）は平均値で埋める。\n学習と評価のために、データを準備する torch のデータローダーを作成する。\n    \"\"\"\n    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(train, val)\n    scaler = StandardScaler()\n    # 数値データの NA（欠損値）は平均値で埋める。\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n    X_num_train = imp.fit_transform(train[numerical])\n    X_num_train = scaler.fit_transform(X_num_train)\n    X_num_val = imp.transform(val[numerical])\n    X_num_val = scaler.transform(X_num_val)\n    dl_train = init_dl(X_cat_train, X_num_train, train, training=True)\n    dl_val = init_dl(X_cat_val, X_num_val, val)\n    return X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n\n\ndef get_categoricals(train, val):\n    \"\"\"\n    Remove constant categorical columns and transform them using LabelEncoder.\n    Return the label-transformers for each categorical column, categorical dataframes and numerical columns.\n    \"\"\"\n    categorical_cols, numerical = get_feature_types(train)\n    remove = []\n    for col in categorical_cols:\n        if train[col].nunique() == 1:\n            remove.append(col)\n        ind = ~val[col].isin(train[col])\n        if ind.any():\n            val.loc[ind, col] = np.nan\n    categorical_cols = [col for col in categorical_cols if col not in remove]\n    transformers, X_cat_train = get_X_cat(train, categorical_cols)\n    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n    return X_cat_train, X_cat_val, numerical, transformers\n\n\ndef init_dl(X_cat, X_num, df, training=False):\n    \"\"\"\n    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n    Notice that efs_time is log-transformed.\n    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n    \"\"\"\n    ds_train = TensorDataset(\n        torch.tensor(X_cat, dtype=torch.long),\n        torch.tensor(X_num, dtype=torch.float32),\n        torch.tensor(df.efs_time.values, dtype=torch.float32).log(),\n        torch.tensor(df.efs.values, dtype=torch.long)\n    )\n    bs = 2048\n    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n    return dl_train\n\n\ndef get_feature_types(train):\n    \"\"\"\n    Utility function to return categorical and numerical column names.\n    \"\"\"\n    categorical_cols = [col for i, col in enumerate(train.columns) if ((train[col].dtype == \"object\") | (2 < train[col].nunique() < 25))]\n    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n    FEATURES = [c for c in train.columns if not c in RMV]\n    numerical = [i for i in FEATURES if i not in categorical_cols]\n    return categorical_cols, numerical\n\n\ndef add_features(df):\n    \"\"\"\n    Create some new features to help the model focus on specific patterns.\n    \"\"\"\n    # sex_match = df.sex_match.astype(str)\n    # sex_match = sex_match.str.split(\"-\").str[0] == sex_match.str.split(\"-\").str[1]\n    # df['sex_match_bool'] = sex_match\n    # df.loc[df.sex_match.isna(), 'sex_match_bool'] = np.nan\n    # df['big_age'] = df.age_at_hct > 16\n    # df.loc[df.year_hct == 2019, 'year_hct'] = 2020\n    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n    # df['strange_age'] = df.age_at_hct == 0.044\n    # df['age_bin'] = pd.cut(df.age_at_hct, [0, 0.0441, 16, 30, 50, 100])\n    # df['age_ts'] = df.age_at_hct / df.donor_age\n    df['year_hct'] -= 2000\n    \n    return df\n\n\ndef load_data():\n    \"\"\"\n    Load data and add features.\n    \"\"\"\n    test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n    test = add_features(test)\n    test[\"efs_xgb_pred\"] = test_global[\"efs_xgb_pred\"]\n    \n    print(\"Test shape:\", test.shape)\n    train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n    train = add_features(train)\n    train[\"efs_xgb_pred\"] = train_global[\"efs_xgb_pred\"]\n    print(\"Train shape:\", train.shape)\n    return test, train\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import functools\nfrom typing import List\n\nimport pytorch_lightning as pl\nimport numpy as np\nimport torch\nfrom lifelines.utils import concordance_index\nfrom pytorch_lightning.cli import ReduceLROnPlateau\nfrom pytorch_tabular.models.common.layers import ODST\nfrom torch import nn\nfrom pytorch_lightning.utilities import grad_norm\n\n\nclass CatEmbeddings(nn.Module):\n    \"\"\"\n    Embedding module for the categorical dataframe.\n    \"\"\"\n    def __init__(\n        self,\n        projection_dim: int,\n        categorical_cardinality: List[int],\n        embedding_dim: int\n    ):\n        \"\"\"\n        projection_dim: The dimension of the final output after projecting the concatenated embeddings into a lower-dimensional space.\n        categorical_cardinality: A list where each element represents the number of unique categories (cardinality) in each categorical feature.\n        embedding_dim: The size of the embedding space for each categorical feature.\n        self.embeddings: list of embedding layers for each categorical feature.\n        self.projection: sequential neural network that goes from the embedding to the output projection dimension with GELU activation.\n        \"\"\"\n        super(CatEmbeddings, self).__init__()\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(cardinality, embedding_dim)\n            for cardinality in categorical_cardinality\n        ])\n        self.projection = nn.Sequential(\n            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n            nn.GELU(),\n            nn.Linear(projection_dim, projection_dim)\n        )\n\n    def forward(self, x_cat):\n        \"\"\"\n        Apply the projection on concatened embeddings that contains all categorical features.\n        \"\"\"\n        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n        x_cat = torch.cat(x_cat, dim=1)\n        return self.projection(x_cat)\n\n\nclass NN(nn.Module):\n    \"\"\"\n    Train a model on both categorical embeddings and numerical data.\n    \"\"\"\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            dropout: float = 0\n    ):\n        \"\"\"\n        continuous_dim: The number of continuous features.\n        categorical_cardinality: A list of integers representing the number of unique categories in each categorical feature.\n        embedding_dim: The dimensionality of the embedding space for each categorical feature.\n        projection_dim: The size of the projected output space for the categorical embeddings.\n        hidden_dim: The number of neurons in the hidden layer of the MLP.\n        dropout: The dropout rate applied in the network.\n        self.embeddings: previous embeddings for categorical data.\n        self.mlp: defines an MLP model with an ODST layer followed by batch normalization and dropout.\n        self.out: linear output layer that maps the output of the MLP to a single value\n        self.dropout: defines dropout\n        Weights initialization with xavier normal algorithm and biases with zeros.\n        \"\"\"\n        super(NN, self).__init__()\n        self.embeddings = CatEmbeddings(projection_dim, categorical_cardinality, embedding_dim)\n        self.mlp = nn.Sequential(\n            ODST(projection_dim + continuous_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(dropout)\n        )\n        self.out = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n\n        # initialize weights\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x_cat, x_cont):\n        \"\"\"\n        Create embedding layers for categorical data, concatenate with continous variables.\n        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n        \"\"\"\n        x = self.embeddings(x_cat)\n        x = torch.cat([x, x_cont], dim=1)\n        x = self.dropout(x)\n        x = self.mlp(x)\n        return self.out(x), x\n\n\n@functools.lru_cache\ndef combinations(N):\n    \"\"\"\n    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1, \n    and caches the result using functools.lru_cache for optimization\n    \"\"\"\n    ind = torch.arange(N)\n    comb = torch.combinations(ind, r=2)\n    if torch.cuda.is_available():\n        return comb.cuda()\n    else:\n        return comb.to(\"cpu\")  # ここを変更\n        \n\n\nclass LitNN(pl.LightningModule):\n    \"\"\"\n    Main Model creation and losses definition to fully train the model.\n    \"\"\"\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            lr: float = 1e-3,\n            dropout: float = 0.2,\n            weight_decay: float = 1e-3,\n            aux_weight: float = 0.1,\n            margin: float = 0.5,\n            race_index: int = 0\n    ):\n        \"\"\"\n        continuous_dim: The number of continuous input features.\n        categorical_cardinality: A list of integers, where each element corresponds to the number of unique categories for each categorical feature.\n        embedding_dim: The dimension of the embeddings for the categorical features.\n        projection_dim: The dimension of the projected space after embedding concatenation.\n        hidden_dim: The size of the hidden layers in the feedforward network (MLP).\n        lr: The learning rate for the optimizer.\n        dropout: Dropout probability to avoid overfitting.\n        weight_decay: The L2 regularization term for the optimizer.\n        aux_weight: Weight used for auxiliary tasks.\n        margin: Margin used in some loss functions.\n        race_index: An index that refer to race_group in the input data.\n        \"\"\"\n        super(LitNN, self).__init__()\n        self.save_hyperparameters()\n\n        # Creates an instance of the NN model defined above\n        self.model = NN(\n            continuous_dim=self.hparams.continuous_dim,\n            categorical_cardinality=self.hparams.categorical_cardinality,\n            embedding_dim=self.hparams.embedding_dim,\n            projection_dim=self.hparams.projection_dim,\n            hidden_dim=self.hparams.hidden_dim,\n            dropout=self.hparams.dropout\n        )\n        self.targets = []\n\n        # Defines a small feedforward neural network that performs an auxiliary task with 1-dimensional output\n        self.aux_cls = nn.Sequential(\n            nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim // 3),\n            nn.GELU(),\n            nn.Linear(self.hparams.hidden_dim // 3, 1)\n        )\n\n    def on_before_optimizer_step(self, optimizer):\n        \"\"\"\n        Compute the 2-norm for each layer\n        If using mixed precision, the gradients are already unscaled here\n        \"\"\"\n        norms = grad_norm(self.model, norm_type=2)\n        self.log_dict(norms)\n\n    def forward(self, x_cat, x_cont):\n        \"\"\"\n        Forward pass that outputs the 1-dimensional prediction and the embeddings (raw output)\n        \"\"\"\n        x, emb = self.model(x_cat, x_cont)\n        return x.squeeze(1), emb\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        defines how the model processes each batch of data during training.\n        A batch is a combination of : categorical data, continuous data, efs_time (y) and efs event.\n        y_hat is the efs_time prediction on all data and aux_pred is auxiliary prediction on embeddings.\n        Calculates loss and race_group loss on full data.\n        Auxiliary loss is calculated with an event mask, ignoring efs=0 predictions and taking the average.\n        Returns loss and aux_loss multiplied by weight defined above.\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        aux_pred = self.aux_cls(emb).squeeze(1)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        aux_loss = nn.functional.mse_loss(aux_pred, y, reduction='none')\n        aux_mask = efs == 1\n        aux_loss = (aux_loss * aux_mask).sum() / aux_mask.sum()\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"race_loss\", race_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        self.log(\"aux_loss\", aux_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        return loss + aux_loss * self.hparams.aux_weight\n\n    def get_full_loss(self, efs, x_cat, y, y_hat):\n        \"\"\"\n        Output loss and race_group loss.\n        \"\"\"\n        loss = self.calc_loss(y, y_hat, efs)\n        race_loss = self.get_race_losses(efs, x_cat, y, y_hat)\n        loss += 0.1 * race_loss\n        return loss, race_loss\n\n    def get_race_losses(self, efs, x_cat, y, y_hat):\n        \"\"\"\n        Calculate loss for each race_group based on deviation/variance.\n        \"\"\"\n        races = torch.unique(x_cat[:, self.hparams.race_index])\n        race_losses = []\n        for race in races:\n            ind = x_cat[:, self.hparams.race_index] == race\n            race_losses.append(self.calc_loss(y[ind], y_hat[ind], efs[ind]))\n        race_loss = sum(race_losses) / len(race_losses)\n        races_loss_std = sum((r - race_loss)**2 for r in race_losses) / len(race_losses)\n        return torch.sqrt(races_loss_std)\n\n    def calc_loss(self, y, y_hat, efs):\n        \"\"\"\n        Most important part of the model : loss function used for training.\n        We face survival data with event indicators along with time-to-event.\n\n        This function computes the main loss by the following the steps :\n        * create all data pairs with \"combinations\" function (= all \"two subjects\" combinations)\n        * make sure that we have at least 1 event in each pair\n        * convert y to +1 or -1 depending on the correct ranking\n        * loss is computed using a margin-based hinge loss\n        * mask is applied to ensure only valid pairs are being used (censored data can't be ranked with event in some cases)\n        * average loss on all pairs is returned\n        \"\"\"\n        N = y.shape[0]\n        comb = combinations(N)\n        comb = comb[(efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)]\n        pred_left = y_hat[comb[:, 0]]\n        pred_right = y_hat[comb[:, 1]]\n        y_left = y[comb[:, 0]]\n        y_right = y[comb[:, 1]]\n        y = 2 * (y_left > y_right).int() - 1\n        loss = nn.functional.relu(-y * (pred_left - pred_right) + self.hparams.margin)\n        mask = self.get_mask(comb, efs, y_left, y_right)\n        loss = (loss.double() * (mask.double())).sum() / mask.sum()\n        return loss\n\n    def get_mask(self, comb, efs, y_left, y_right):\n        \"\"\"\n        Defines all invalid comparisons :\n        * Case 1: \"Left outlived Right\" but Right is censored\n        * Case 2: \"Right outlived Left\" but Left is censored\n        Masks for case 1 and case 2 are combined using |= operator and inverted using ~ to create a \"valid pair mask\"\n        \"\"\"\n        left_outlived = y_left >= y_right\n        left_1_right_0 = (efs[comb[:, 0]] == 1) & (efs[comb[:, 1]] == 0)\n        mask2 = (left_outlived & left_1_right_0)\n        right_outlived = y_right >= y_left\n        right_1_left_0 = (efs[comb[:, 1]] == 1) & (efs[comb[:, 0]] == 0)\n        mask2 |= (right_outlived & right_1_left_0)\n        mask2 = ~mask2\n        mask = mask2\n        return mask\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        This method defines how the model processes each batch during validation\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"\n        At the end of the validation epoch, it computes and logs the concordance index\n        \"\"\"\n        cindex, metric = self._calc_cindex()\n        self.log(\"cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n    def _calc_cindex(self):\n        \"\"\"\n        Calculate c-index accounting for each race_group or global.\n        \"\"\"\n        y = torch.cat([t[0] for t in self.targets]).cpu().numpy()\n        y_hat = torch.cat([t[1] for t in self.targets]).cpu().numpy()\n        efs = torch.cat([t[2] for t in self.targets]).cpu().numpy()\n        races = torch.cat([t[3] for t in self.targets]).cpu().numpy()\n        metric = self._metric(efs, races, y, y_hat)\n        cindex = concordance_index(y, y_hat, efs)\n        return cindex, metric\n\n    def _metric(self, efs, races, y, y_hat):\n        \"\"\"\n        Calculate c-index accounting for each race_group\n        \"\"\"\n        metric_list = []\n        for race in np.unique(races):\n            y_ = y[races == race]\n            y_hat_ = y_hat[races == race]\n            efs_ = efs[races == race]\n            metric_list.append(concordance_index(y_, y_hat_, efs_))\n        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n        return metric\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"\n        Same as training step but to log test data\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def on_test_epoch_end(self) -> None:\n        \"\"\"\n        At the end of the test epoch, calculates and logs the concordance index for the test set\n        \"\"\"\n        cindex, metric = self._calc_cindex()\n        self.log(\"test_cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"test_cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n\n    def configure_optimizers(self):\n        \"\"\"\n        configures the optimizer and learning rate scheduler:\n        * Optimizer: Adam optimizer with weight decay (L2 regularization).\n        * Scheduler: Cosine Annealing scheduler, which adjusts the learning rate according to a cosine curve.\n        \"\"\"\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n        scheduler_config = {\n            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer,\n                T_max=45,\n                eta_min=6e-3\n            ),\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n            \"strict\": False,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pytorch_lightning as pl\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\nfrom pytorch_lightning.callbacks import StochasticWeightAveraging\nfrom sklearn.model_selection import StratifiedKFold\n\npl.seed_everything(42)\n\n# デバイスを決定（GPUがあればcuda、なければcpu）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef main(hparams):\n    test, train_original = load_data()\n    test['efs_time'] = 1\n    test['efs'] = 1\n    oof_nn_pairwise = np.zeros(len(train_original))\n    test_pred = np.zeros(test.shape[0])\n    categorical_cols, numerical = get_feature_types(train_original)\n    kf = StratifiedKFold(n_splits=5, shuffle=True)\n    \n    for i, (train_index, test_index) in enumerate(\n        kf.split(train_original, train_original.race_group.astype(str) + (train_original.age_at_hct == 0.044).astype(str))\n    ):\n        tt = train_original.copy()\n        train = tt.iloc[train_index]\n        val = tt.iloc[test_index]\n        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, val)\n        \n        model = train_final(X_num_train, dl_train, dl_val, transformers, categorical_cols=categorical_cols).to(device)\n\n        oof_pred, _ = model.eval()(\n            torch.tensor(X_cat_val, dtype=torch.long).to(device),\n            torch.tensor(X_num_val, dtype=torch.float32).to(device)\n        )\n        oof_nn_pairwise[test_index] = oof_pred.detach().cpu().numpy()\n\n        # Create submission\n        train = tt.iloc[train_index]\n        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, test)\n\n        pred, _ = model.eval()(\n            torch.tensor(X_cat_val, dtype=torch.long).to(device),\n            torch.tensor(X_num_val, dtype=torch.float32).to(device)\n        )\n        test_pred += pred.detach().cpu().numpy()\n    \n    return -test_pred, -oof_nn_pairwise\n\n\ndef train_final(X_num_train, dl_train, dl_val, transformers, hparams=None, categorical_cols=None):\n    if hparams is None:\n        hparams = {\n            \"embedding_dim\": 16,\n            \"projection_dim\": 112,\n            \"hidden_dim\": 56,\n            \"lr\": 0.06464861983337984,\n            \"dropout\": 0.05463240181423116,\n            \"aux_weight\": 0.26545778308743806,\n            \"margin\": 0.2588153271003354,\n            \"weight_decay\": 0.0002773544957610778\n        }\n    \n    model = LitNN(\n        continuous_dim=X_num_train.shape[1],\n        categorical_cardinality=[len(t.classes_) for t in transformers],\n        race_index=categorical_cols.index(\"race_group\"),\n        **hparams\n    ).to(device)  # モデルを適切なデバイスに移動\n\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        monitor=\"val_loss\",  # val_loss が最小のモデルを保存\n        save_top_k=1,  # 最も良いモデルだけを保存\n    )\n\n    trainer = pl.Trainer(\n        accelerator=device.type,  # 自動で GPU or CPU を選択\n        max_epochs=60,\n        log_every_n_steps=6,\n        callbacks=[\n            checkpoint_callback,\n            LearningRateMonitor(logging_interval='epoch'),\n            TQDMProgressBar(),\n            StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n        ],\n    )\n\n    trainer.fit(model, dl_train)\n    trainer.test(model, dl_val)\n\n    return model.eval().to(device)  # モデルを適切なデバイスに移動\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hparams = None\npairwise_ranking_pred, pairwise_ranking_oof = main(hparams)\n\ny_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\ny_pred = train[[\"ID\"]].copy()\ny_pred[\"prediction\"] = pairwise_ranking_oof\nm = score(y_true.copy(), y_pred.copy(), \"ID\")\nprint(f\"\\nPairwise ranking NN CV =\",m)\n\n# Update predictions with classifier mask\npairwise_ranking_oof[oof_xgb == 1] += 0.2\ny_pred[\"prediction\"] = pairwise_ranking_oof\nm = score(y_true.copy(), y_pred.copy(), \"ID\")\nprint(f\"\\nPairwise ranking NN with classifier mask -> CV =\",m)\n\npairwise_ranking_pred[pred_efs == 1] += 0.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# subm_data = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n# subm_data['prediction'] = pairwise_ranking_pred\n# subm_data.to_csv('submission2.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import polars as pl #ライブラリのインポートに非常に時間がかかりそうだったので割愛\n# import plotly.colors as pc\n# import plotly.express as px\n# import plotly.graph_objects as go\n# import plotly.io as pio\n# pio.renderers.default = 'iframe'\n\n# 生存関数推定モデル\nfrom lifelines import CoxPHFitter\nfrom lifelines import KaplanMeierFitter\nfrom lifelines import NelsonAalenFitter\nfrom lifelines.utils import concordance_index #C-indexの計算\n\n# sklearn\nimport sklearn.base\nfrom sklearn.base import _fit_context\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\n# 予測モデル\nimport lightgbm as lgb\nimport optuna\nfrom catboost import CatBoostRegressor\nfrom catboost import CatBoostClassifier\nfrom scipy.stats import rankdata\n\n#NN用\nimport torch\nfrom lifelines import KaplanMeierFitter, NelsonAalenFitter\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom torch.utils.data import TensorDataset\n\nimport functools\nfrom typing import List\n\nimport pytorch_lightning as pl\nimport numpy as np\nimport torch\nfrom lifelines.utils import concordance_index\nfrom pytorch_lightning.cli import ReduceLROnPlateau\nfrom pytorch_tabular.models.common.layers import ODST\nfrom torch import nn\nfrom pytorch_lightning.utilities import grad_norm\n\nimport json\nimport joblib\nimport os\nfrom datetime import datetime\nimport xgboost as xgb\n\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\nfrom pytorch_lightning.callbacks import StochasticWeightAveraging\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:38:39.478108Z","iopub.execute_input":"2025-02-22T14:38:39.478605Z","iopub.status.idle":"2025-02-22T14:38:59.902621Z","shell.execute_reply.started":"2025-02-22T14:38:39.478557Z","shell.execute_reply":"2025-02-22T14:38:59.901454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">データインポート</p>","metadata":{"papermill":{"duration":0.006361,"end_time":"2024-12-13T13:19:30.95227","exception":false,"start_time":"2024-12-13T13:19:30.945909","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# INPUT_DIRにディレクトリを指定\nINPUT_DIR = \"xxx\"\npd.options.display.max_columns = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:38:59.903737Z","iopub.execute_input":"2025-02-22T14:38:59.904027Z","iopub.status.idle":"2025-02-22T14:38:59.909355Z","shell.execute_reply.started":"2025-02-22T14:38:59.904002Z","shell.execute_reply":"2025-02-22T14:38:59.908055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if \"COLAB_GPU\" in os.environ:\n  import sys\n  sys.path.append('/content/drive/MyDrive/Kaggle/20250215_CIBMTR')\n\n\n  print(\"Google Colab で実行中\")\n  from google.colab import drive\n  drive.mount('/content/drive')\n  # CSVファイルのパスを指定\n  csv_file_path = \"/content/drive/MyDrive/Kaggle/20250215_CIBMTR/death_rate.csv\"\n  # 現在の日時を取得してフォーマット\n  current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n  # ディレクトリ名を作成\n  directory_name = f\"/content/drive/MyDrive/Kaggle/20250215_CIBMTR/model_{current_datetime}\"\n\n  # ディレクトリのパスを指定\n  directory_path = Path(directory_name)\n\n  # ディレクトリを作成\n  directory_path.mkdir(parents=True, exist_ok=True)\n\n  train_path = Path('/content/drive/MyDrive/Kaggle/20250215_CIBMTR/data/train.csv')\n  test_path = Path('/content/drive/MyDrive/Kaggle/20250215_CIBMTR/data/test.csv')\n  subm_path = Path('/content/drive/MyDrive/Kaggle/20250215_CIBMTR/data/sample_submission.csv')\n\n\nelif \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n  !pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n  !pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n  !pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n  !pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n  !pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n\n\n\n\n  # CSVファイルのパスを指定\n  csv_file_path = \"/kaggle/input/world-age-death-rate/death_rate.csv\"\n  # 現在の日時を取得してフォーマット\n  current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n  # ディレクトリ名を作成\n  directory_name = f\"model_{current_datetime}\"\n\n  # ディレクトリのパスを指定\n  directory_path = Path(directory_name)\n\n  # ディレクトリを作成\n  directory_path.mkdir(parents=True, exist_ok=True)\n  print(\"Kaggle Notebooks で実行中\")\n\n  train_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\n  test_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\n  subm_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv')\nelse:\n  print(\"ローカル環境または他の環境で実行中\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:38:59.910674Z","iopub.execute_input":"2025-02-22T14:38:59.911136Z","iopub.status.idle":"2025-02-22T14:39:25.974509Z","shell.execute_reply.started":"2025-02-22T14:38:59.911092Z","shell.execute_reply":"2025-02-22T14:39:25.972897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">パラメータ設定【CFG】</p>","metadata":{"papermill":{"duration":0.00881,"end_time":"2024-12-13T13:23:11.307141","exception":false,"start_time":"2024-12-13T13:23:11.298331","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"重みや正則化パラメータは自分で決定","metadata":{}},{"cell_type":"code","source":"# クラス変数（クラス全体で共有される変数）の定義\nclass CFG:\n    train_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/train.csv') \n    test_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/test.csv') \n    subm_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv') \n    # train_path = INPUT_DIR + \"/train.csv\"\n    # test_path = INPUT_DIR + \"/test.csv\"\n    # subm_path = INPUT_DIR + \"/sample_submission.csv\"\n\n    # colorscale = 'red' # 可視化に使用するカラースケールの指定\n    color = '#A2574F' # 可視化に使用するカラーパレットの指定\n\n    early_stop = 300 # 早期終了のパラメータ（一定エポック改善がない場合に終了）\n    penalizer = 0.01 # 正則化パラメータの設定\n    n_splits = 5 # クロスバリデーションの分割数\n\n    weights = [\n        2,\n        1,\n        6,\n        3,\n        6,\n        3,\n        6,\n        6,\n        6,\n    ] # 特定のモデルやデータに対する重みのリスト\n\n\n    # GPUが利用可能か判定\n    use_gpu = torch.cuda.is_available()\n    \n    # CatBoost (二値分類)\n    ctb_params_efs = {\n        \"loss_function\": \"Logloss\",\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'GPU' if use_gpu else 'CPU',  # GPUが使えればGPU、使えなければCPU\n        'num_trees': 6000,\n        'reg_lambda': 8.0,\n        'depth': 8,\n        'eval_metric': \"Logloss\",\n    }\n    \n    # CatBoost (回帰)\n    ctb_params = {\n        'loss_function': 'RMSE',\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'GPU' if use_gpu else 'CPU',\n        'num_trees': 6000,\n        'reg_lambda': 8.0,\n        'depth': 8,\n    }\n    \n    # LightGBM (二値分類)\n    lgb_params_efs = {\n        'objective': 'binary',\n        'min_child_samples': 32,\n        'num_iterations': 6000,\n        'learning_rate': 0.03,\n        'extra_trees': True,\n        'reg_lambda': 8.0,\n        'reg_alpha': 0.1,\n        'num_leaves': 64,\n        'metric': 'logloss',\n        'max_depth': 8,\n        'device': 'gpu' if use_gpu else 'cpu',\n        'max_bin': 128,\n        'verbose': -1,\n        'seed': 42,\n        **({'gpu_use_dp': True} if use_gpu else {})  # GPU使用時は `gpu_use_dp=True` を追加\n    }\n    \n    # LightGBM (回帰)\n    lgb_params = {\n        'objective': 'regression',\n        'min_child_samples': 32,\n        'num_iterations': 6000,\n        'learning_rate': 0.03,\n        'extra_trees': True,\n        'reg_lambda': 8.0,\n        'reg_alpha': 0.1,\n        'num_leaves': 64,\n        'metric': 'binary_logloss',\n        'max_depth': 8,\n        'device': 'gpu' if use_gpu else 'cpu',\n        'max_bin': 128,\n        'verbose': -1,\n        'seed': 42,\n        **({'gpu_use_dp': True} if use_gpu else {})\n    }\n    \n    # XGBoost (回帰)\n    xgb_params = {\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'learning_rate': 0.03,\n        'max_depth': 7,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'lambda': 7.0,\n        'alpha': 0.0,\n        'n_estimators': 6000,\n        'tree_method': 'gpu_hist' if use_gpu else 'hist',\n        'random_state': 42,\n    }\n    \n    # XGBoost (二値分類)\n    xgb_params_efs = {\n        'objective': \"binary:logistic\",\n        'eval_metric': 'logloss',\n        'learning_rate': 0.03,\n        'max_depth': 7,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'lambda': 7.0,\n        'alpha': 0.0,\n        'n_estimators': 6000,\n        'tree_method': 'gpu_hist' if use_gpu else 'hist',\n        'random_state': 42,\n    }\n    \n    # Coxモデル (Depthwise)\n    cox1_params = {\n        'grow_policy': 'Depthwise',\n        'min_child_samples': 8,\n        'loss_function': 'Cox',\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'GPU' if use_gpu else 'CPU',\n        'num_trees': 6000,\n        'reg_lambda': 8.0,\n        'depth': 8,\n    }\n    \n    # Coxモデル (Lossguide)\n    cox2_params = {\n        'grow_policy': 'Lossguide',\n        'min_child_samples': 2,\n        'loss_function': 'Cox',\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'GPU' if use_gpu else 'CPU',\n        'num_trees': 6000,\n        'reg_lambda': 8.0,\n        'num_leaves': 32,\n        'depth': 8,\n    }\n","metadata":{"papermill":{"duration":0.019849,"end_time":"2024-12-13T13:23:11.335144","exception":false,"start_time":"2024-12-13T13:23:11.315295","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:25.977533Z","iopub.execute_input":"2025-02-22T14:39:25.977956Z","iopub.status.idle":"2025-02-22T14:39:25.992762Z","shell.execute_reply.started":"2025-02-22T14:39:25.977921Z","shell.execute_reply":"2025-02-22T14:39:25.991343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">特徴量エンジニアリング【FE】</p>","metadata":{"papermill":{"duration":0.007982,"end_time":"2024-12-13T13:23:11.351542","exception":false,"start_time":"2024-12-13T13:23:11.34356","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FE:\n    def __init__(self):\n        pass # 初期化メソッドには何も処理を行わない\n\n    def _load_data(self, path):\n        df = pd.read_csv(path)\n        return df\n\n    def _update_hla_columns(self, df): # \"hla_high_xxx\"、\"hla_low_xxx\"系の特徴量について、既知のデータを用いずに自分で作成\n        df['hla_nmdp_6'] = (\n            df['hla_match_a_low'].fillna(0)\n            + df['hla_match_b_low'].fillna(0)\n            + df['hla_match_drb1_high'].fillna(0)\n        )\n        df['hla_low_res_6'] = (\n            df['hla_match_a_low'].fillna(0)\n            + df['hla_match_b_low'].fillna(0)\n            + df['hla_match_drb1_low'].fillna(0)\n        )\n        df['hla_high_res_6'] = (\n            df['hla_match_a_high'].fillna(0)\n            + df['hla_match_b_high'].fillna(0)\n            + df['hla_match_drb1_high'].fillna(0)\n        )\n        df['hla_low_res_8'] = (\n            df['hla_match_a_low'].fillna(0)\n            + df['hla_match_b_low'].fillna(0)\n            + df['hla_match_c_low'].fillna(0)\n            + df['hla_match_drb1_low'].fillna(0)\n        )\n        df['hla_high_res_8'] = (\n            df['hla_match_a_high'].fillna(0)\n            + df['hla_match_b_high'].fillna(0)\n            + df['hla_match_c_high'].fillna(0)\n            + df['hla_match_drb1_high'].fillna(0)\n        )\n        df['hla_low_res_10'] = (\n            df['hla_match_a_low'].fillna(0)\n            + df['hla_match_b_low'].fillna(0)\n            + df['hla_match_c_low'].fillna(0)\n            + df['hla_match_drb1_low'].fillna(0)\n            + df['hla_match_dqb1_low'].fillna(0)\n        )\n        df['hla_high_res_10'] = (\n            df['hla_match_a_high'].fillna(0)\n            + df['hla_match_b_high'].fillna(0)\n            + df['hla_match_c_high'].fillna(0)\n            + df['hla_match_drb1_high'].fillna(0)\n            + df['hla_match_dqb1_high'].fillna(0)\n        )\n        return df\n\n    def _cast_datatypes(self, df): # 欠損値の補完\n        num_cols = [\n            'hla_high_res_8', 'hla_low_res_8', 'hla_high_res_6', 'hla_low_res_6',\n            'hla_high_res_10', 'hla_low_res_10', 'hla_match_dqb1_high',\n            'hla_match_dqb1_low', 'hla_match_drb1_high', 'hla_match_drb1_low',\n            'hla_nmdp_6', 'year_hct', 'hla_match_a_high', 'hla_match_a_low',\n            'hla_match_b_high', 'hla_match_b_low', 'hla_match_c_high',\n            'hla_match_c_low', 'donor_age', 'age_at_hct', 'comorbidity_score',\n            'karnofsky_score', 'efs', 'efs_time'\n        ]\n        for col in df.columns:\n            if col in num_cols:\n                df[col] = df[col].fillna(-1).astype('float32') # 数値型の場合は欠損値を-1で補完\n            else:\n                df[col] = df[col].fillna('Unknown').astype('category') # カテゴリ型の場合は欠損値を'Unknown'で補完\n        df['ID'] = df['ID'].astype('int32') # データ型の変更\n        return df\n    \n    def add_features(df):\n        sex_match = df.sex_match.astype(str)\n        sex_match = sex_match.str.split(\"-\").str[0] == sex_match.str.split(\"-\").str[1]\n        df['sex_match_bool'] = sex_match\n        df.loc[df.sex_match.isna(), 'sex_match_bool'] = np.nan\n        df.loc[df.year_hct == 2019, 'year_hct'] = 2020\n        df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n        df['age_bin'] = pd.cut(df.age_at_hct, [0, 0.0441, 16, 30, 50, 100])\n        df['age_ts'] = df.age_at_hct / df.donor_age\n        df['age_comorbidity'] = df['age_at_hct'] * df['comorbidity_score']\n        df['age_karnofsky'] = df['age_at_hct'] * df['karnofsky_score']\n        df['karnofsky_squared'] = df['karnofsky_score'] ** 2\n        df['cos_year'] = np.cos(df['year_hct'] * (2 * np.pi) / 100)\n        df['diff_age_vs_donor'] = df['age_at_hct'] - df['donor_age']\n        # sex_one: 性別一致情報から患者の性別を抽出\n        df['sex_one'] = df['sex_match'].str[0]  # 最初の文字を抽出\n        # sex_two: 性別一致情報からドナーの性別を抽出\n        df['sex_two'] = df['sex_match'].str[2]  # 3番目の文字を抽出\n        # SameSex: 患者とドナーの性別が一致しているか\n        df['same_sex'] = (df['sex_one'] == df['sex_two']).astype(int)\n\n        \n        return df\n\n    def info(self, df): #データフレームのメモリ使用量を確認\n        print(f'\\nShape of dataframe: {df.shape}')\n        mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage: {:.2f} MB\\n'.format(mem))\n        print(df.head())\n\n    def apply_fe(self, path): # 上記関数の適用\n        df = self._load_data(path) # pathのデータを読み込む\n        df = self._update_hla_columns(df) # hla_high_xxx、hla_low_xxx系の特徴量を追加\n        df = self._cast_datatypes(df) # 欠損値の補完\n        self.info(df) # データフレームのメモリ確認\n        cat_cols = [col for col in df.columns if df[col].dtype == 'category']\n        return df, cat_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:25.995365Z","iopub.execute_input":"2025-02-22T14:39:25.995776Z","iopub.status.idle":"2025-02-22T14:39:26.042188Z","shell.execute_reply.started":"2025-02-22T14:39:25.995747Z","shell.execute_reply":"2025-02-22T14:39:26.040516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# インスタンス作成\nfe = FE()","metadata":{"_kg_hide-input":false,"papermill":{"duration":0.016709,"end_time":"2024-12-13T13:23:11.407504","exception":false,"start_time":"2024-12-13T13:23:11.390795","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:26.043504Z","iopub.execute_input":"2025-02-22T14:39:26.043862Z","iopub.status.idle":"2025-02-22T14:39:26.064234Z","shell.execute_reply.started":"2025-02-22T14:39:26.043782Z","shell.execute_reply":"2025-02-22T14:39:26.063028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \ndf_train, cat_cols = fe.apply_fe(CFG.train_path)","metadata":{"papermill":{"duration":0.726911,"end_time":"2024-12-13T13:23:12.142797","exception":false,"start_time":"2024-12-13T13:23:11.415886","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:26.0655Z","iopub.execute_input":"2025-02-22T14:39:26.065869Z","iopub.status.idle":"2025-02-22T14:39:26.742281Z","shell.execute_reply.started":"2025-02-22T14:39:26.065829Z","shell.execute_reply":"2025-02-22T14:39:26.740942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data, _ = fe.apply_fe(CFG.test_path)","metadata":{"papermill":{"duration":0.075873,"end_time":"2024-12-13T13:23:12.227636","exception":false,"start_time":"2024-12-13T13:23:12.151763","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:26.74358Z","iopub.execute_input":"2025-02-22T14:39:26.744074Z","iopub.status.idle":"2025-02-22T14:39:26.831675Z","shell.execute_reply.started":"2025-02-22T14:39:26.744022Z","shell.execute_reply":"2025-02-22T14:39:26.830519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">可視化【EDA】</p>","metadata":{"papermill":{"duration":0.009157,"end_time":"2024-12-13T13:23:12.246272","exception":false,"start_time":"2024-12-13T13:23:12.237115","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class EDA:\n    def __init__(self, color, data):\n        # self._colorscale = sns.color_palette(colorscale)\n        self._color = color\n        self.data = data\n\n    def _template(self, ax, title):\n        ax.set_title(title, fontsize=16, color=self._color, ha='center')\n        # ax.set_facecolor('rgba(247, 230, 202, 1)')\n        ax.grid(True, axis='x', color='grey', linestyle='-', linewidth=0.5)\n        ax.grid(True, axis='y', color='grey', linestyle='-', linewidth=0.5)\n        ax.tick_params(axis='both', colors=self._color)\n        ax.set_xlabel('Values', color=self._color)\n        ax.set_ylabel('Count', color=self._color)\n        return ax\n\n    def distribution_plot(self, col, title):\n        fig, ax = plt.subplots(figsize=(8, 6))\n        sns.histplot(self.data[col], bins=100, color=self._color, ax=ax)\n        ax = self._template(ax, f'{title}')\n        plt.show()\n\n    def bar_chart(self, col):\n        value_counts = self.data[col].value_counts().reset_index()\n        value_counts.columns = [col, 'count']\n\n        fig, ax = plt.subplots(figsize=(8, 6))\n        sns.barplot(data=value_counts, x='count', y=col, ax=ax)\n\n        ax = self._template(ax, f'{col}')\n        ax.set_xlabel('Count')\n        ax.set_ylabel('')\n        plt.show()\n\n    def _plot_cv(self, scores, title, metric='Stratified C-Index'):\n        fold_scores = [round(score, 3) for score in scores]\n        mean_score = round(np.mean(scores), 3)\n\n        fig, ax = plt.subplots(figsize=(8, 6))\n\n        ax.scatter(range(1, len(fold_scores) + 1), fold_scores, color=self._color, s=100, label='Fold Scores', marker='D')\n\n        ax.plot([1, len(fold_scores)], [mean_score, mean_score], color='#B22222', linestyle='--', label=f'Mean: {mean_score:.3f}')\n        \n        ax.set_title(f'{title} | Cross-validation Mean {metric} Score: {mean_score}', fontsize=16, color=self._color, ha='center')\n        ax.set_xlabel('Fold', color=self._color)\n        ax.set_ylabel(f'{metric} Score', color=self._color)\n        \n        ax.legend()\n        ax.set_xticks(range(1, len(fold_scores) + 1))\n        ax.set_ylim(min(fold_scores) - 0.05, max(fold_scores) + 0.05)\n\n        ax = self._template(ax, f'{title} Cross-validation')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:26.832887Z","iopub.execute_input":"2025-02-22T14:39:26.833161Z","iopub.status.idle":"2025-02-22T14:39:26.846119Z","shell.execute_reply.started":"2025-02-22T14:39:26.833138Z","shell.execute_reply":"2025-02-22T14:39:26.844592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">生存確率（目的変数）の推定【Targets】</p>","metadata":{}},{"cell_type":"markdown","source":"特徴的な点  \n- カテゴリ変数にone hot encodingを実施\n- cvにおいて一つの値しか取らないカラムを削除","metadata":{}},{"cell_type":"code","source":"import torch\nclass Targets:\n    def __init__(self, data, cat_cols, penalizer, n_splits):\n        self.data = data\n        self.cat_cols = cat_cols\n        self._length = len(self.data)\n        self._penalizer = penalizer\n        self._n_splits = n_splits\n\n    def _prepare_cv(self): # cvのためのデータ分割\n        oof_preds = np.zeros(self._length)\n        cv = KFold(n_splits=self._n_splits, shuffle=True, random_state=42)\n        return cv, oof_preds\n\n    def score(self, df_true, df_prediction, event_label, interval_label, prediction_label): #層別C-indexの計算\n        # ID列を削除\n        del df_true[\"ID\"]\n        del df_prediction[\"ID\"]\n\n        merged_df = pd.concat([df_true, df_prediction], axis=1)\n        merged_df.reset_index(inplace=True)\n        merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups) # 人種ごとに該当するデータのインデックスを辞書形式で保管\n        metric_list = []\n        for race in merged_df_race_dict.keys(): #人種ごと\n            indices = sorted(merged_df_race_dict[race]) #該当する人種のインデックスを取得\n            merged_df_race = merged_df.iloc[indices] # 該当するインデックスのデータを取得\n            c_index_race = concordance_index( # concordance_indexはefs, efs_time, predicton（生存確率）からC-indexを計算\n                merged_df_race[interval_label],\n                -merged_df_race[prediction_label], # 符号を反転（通常 C-Index は「リスクが高いほど短命」と仮定するため）\n                merged_df_race[event_label])\n            metric_list.append(c_index_race)\n        return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))\n\n    def validate_model(self, preds, title):\n        y_true = self.data[['ID', 'efs', 'efs_time', 'race_group']].copy()\n        y_pred = self.data[['ID']].copy()\n        y_pred['prediction'] = preds\n        c_index_score = self.score(y_true.copy(), y_pred.copy(), \"efs\", \"efs_time\", \"prediction\")\n        print(f'Overall Stratified C-Index Score for {title}: {c_index_score:.4f}')\n\n        return c_index_score\n\n    def create_target1(self): # Cox比例ハザードモデル\n        '''\n        Constant columns are dropped if they exist in a fold. Otherwise, the code produces error:\n        delta contains nan value(s). Convergence halted. Please see the following tips in the lifelines documentation: \n        https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n        '''\n        cv, oof_preds = self._prepare_cv() #データ分割\n        data = pd.get_dummies(self.data, columns=self.cat_cols, drop_first=True).drop('ID', axis=1) # カテゴリ変数にone hot encodingを実施\n\n        for fold, (train_index, valid_index) in enumerate(cv.split(data), 1):\n            df_train = data.iloc[train_index]\n            valid_data = data.iloc[valid_index]\n            unique_columns = df_train.nunique() > 1\n            removed_columns = df_train.columns[~unique_columns] # 一つの値しか取らないカラムを削除\n            df_train = df_train.loc[:, unique_columns]\n            valid_data = valid_data[df_train.columns]\n            print(f\"Fold {fold}:\")\n            print(f\"Removed columns: {list(removed_columns)}\") # 削除されたカラムとその数を表示\n            print(f\"Number of removed columns: {len(removed_columns)}\")\n            cph = CoxPHFitter(penalizer=self._penalizer) # Cox比例ハザードモデルのインスタンス作成（penalizer：正則化の強さ（過学習を防止する目的））\n            cph.fit(df_train, duration_col='efs_time', event_col='efs')\n            oof_preds[valid_index] = cph.predict_partial_hazard(valid_data)\n\n        self.data['target1'] = oof_preds\n        c_index_score = self.validate_model(oof_preds, 'Cox')\n        return self.data\n\n    def create_target2(self): # Kaplan-Meier生存関数モデル\n        cv, oof_preds = self._prepare_cv() #データ分割\n\n        for train_index, valid_index in cv.split(self.data):\n            df_train = self.data.iloc[train_index]\n            valid_data = self.data.iloc[valid_index]\n            kmf = KaplanMeierFitter()\n            kmf.fit(durations=df_train['efs_time'], event_observed=df_train['efs'])\n            oof_preds[valid_index] = kmf.survival_function_at_times(valid_data['efs_time']).values\n\n        self.data['target2'] = oof_preds\n        c_index_score = self.validate_model(oof_preds, 'Kaplan-Meier')\n\n        return self.data\n\n    def create_target3(self): # Nelson-Aalen生存関数モデル\n        cv, oof_preds = self._prepare_cv()\n        for train_index, valid_index in cv.split(self.data):\n            df_train = self.data.iloc[train_index]\n            valid_data = self.data.iloc[valid_index]\n            naf = NelsonAalenFitter()\n            naf.fit(durations=df_train['efs_time'], event_observed=df_train['efs'])\n            oof_preds[valid_index] = -naf.cumulative_hazard_at_times(valid_data['efs_time']).values\n\n        self.data['target3'] = oof_preds\n        self.validate_model(oof_preds, 'Nelson-Aalen')\n        return self.data\n\n    def create_target4(self): # \n        self.data['target4'] = self.data.efs_time.copy()\n        self.data.loc[self.data.efs == 0, 'target4'] *= -1\n        return self.data\n\n    def create_target5(self):\n        # GPU が利用可能なら \"cuda\" を、なければ \"cpu\" を使用\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        # pandas のカラム \"efs\" と \"efs_time\" を PyTorch テンソルに変換\n        efs = torch.tensor(self.data[\"efs\"].values, device=device)\n        efs_time = torch.tensor(self.data[\"efs_time\"].values, device=device)\n        \n        N = len(self.data)\n        \n        # 全ての (i, j) (i < j) の組み合わせを生成（torch.combinations は全ペアを返す）\n        pairs = torch.combinations(torch.arange(N, device=device), r=2)\n        i_idx, j_idx = pairs[:, 0], pairs[:, 1]\n        \n        # 各ペアの値を抽出\n        efs_i, efs_j = efs[i_idx], efs[j_idx]\n        time_i, time_j = efs_time[i_idx], efs_time[j_idx]\n        \n        # 各サンプルの勝利数を格納するテンソル（初期値0）\n        win_counts = torch.zeros(N, device=device)\n        \n        # (1,1) のペアの場合：時間が長い方が勝者\n        condition_11 = (efs_i == 1) & (efs_j == 1)\n        win_counts.index_add_(0, i_idx[condition_11 & (time_i > time_j)], torch.ones((((condition_11) & (time_i > time_j)).sum(),), device=device))\n        win_counts.index_add_(0, j_idx[condition_11 & (time_j > time_i)], torch.ones((((condition_11) & (time_j > time_i)).sum(),), device=device))\n        \n        # (1,0) の場合：efs_i==1 かつ efs_j==0 で、time_j > time_i のとき、j が勝者\n        condition_10 = (efs_i == 1) & (efs_j == 0) & (time_j > time_i)\n        win_counts.index_add_(0, j_idx[condition_10], torch.ones((condition_10.sum(),), device=device))\n        \n        # (0,1) の場合：efs_i==0 かつ efs_j==1 で、time_i > time_j のとき、i が勝者\n        condition_01 = (efs_i == 0) & (efs_j == 1) & (time_i > time_j)\n        win_counts.index_add_(0, i_idx[condition_01], torch.ones((condition_01.sum(),), device=device))\n        \n        # 結果を負の値に変換してデータフレームに格納\n        self.data[\"target5\"] = -win_counts.cpu().numpy()\n        \n        # 検証処理（元の処理と統一）\n        self.validate_model(self.data[\"target5\"], 'pair_wise')\n    \n        return self.data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:26.847352Z","iopub.execute_input":"2025-02-22T14:39:26.847827Z","iopub.status.idle":"2025-02-22T14:39:26.871564Z","shell.execute_reply.started":"2025-02-22T14:39:26.847765Z","shell.execute_reply":"2025-02-22T14:39:26.870112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">モデル構築【MD】</p>","metadata":{}},{"cell_type":"code","source":"class MD:\n    def __init__(self,color, data, cat_cols, early_stop, penalizer, n_splits):\n        self.eda = EDA(color, data) # EDAクラスのインスタンス作成\n        self.targets = Targets(data, cat_cols, penalizer, n_splits) # Targetsクラスのインスタンス作成\n        self.data = data\n        self.cat_cols = cat_cols\n        self.early_stop = early_stop\n        self.efs_lgb_models = []\n        self.efs_cbt_models = []\n        self.efs_xgb_models = []\n\n    def create_targets(self, ctb_params_efs, lgb_params_efs, xgb_params_efs, directory_path): # 4種類の生存確率\n        self.data = self.targets.create_target1()\n        self.data = self.targets.create_target2()\n        self.data = self.targets.create_target3()\n        self.data = self.targets.create_target4()\n        self.data = self.targets.create_target5()\n\n        self.data[\"efs_pred_lgb\"] = 0  # 事前にカラムを用意（任意）cat\n        self.data[\"efs_pred_cbt\"] = 0  # 事前にカラムを用意（任意）\n        self.data[\"efs_pred_xgb\"] = 0  # 事前にカラムを用意（任意）cat\n        # epsの予測モデルを追加\n        for col in self.cat_cols:\n            self.data[col] = self.data[col].astype('category')\n\n        \n\n        cv, oof_preds = self.targets._prepare_cv()\n\n        print(self.data.columns)\n        X = self.data.drop(['ID', 'efs', 'efs_time', 'target1', 'target2', 'target3', 'target4','target5',\n                            \"efs_pred_lgb\", \"efs_pred_cbt\", \"efs_pred_xgb\"\n                           ], axis=1, errors='ignore')\n        y = self.data['efs']\n        w =  1 + self.data[\"efs\"]  # 最小1、最大2の重み\n\n        models, fold_scores = [], []\n\n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n            # 訓練データとバリデーションデータの分割\n            X_train = X.iloc[train_index]\n            X_valid = X.iloc[valid_index]\n            y_train = y.iloc[train_index]\n            y_valid = y.iloc[valid_index]\n            w_train = w.iloc[train_index]\n\n            # 未使用カテゴリを削除\n            for col in X_train.select_dtypes(include=['category']).columns:\n                X_train[col] = X_train[col].cat.remove_unused_categories()\n                X_valid[col] = X_valid[col].cat.remove_unused_categories()\n\n# lgb-------------------------------------------------------------------------------------------------\n\n            model = lgb.LGBMClassifier(**lgb_params_efs)\n\n            model.fit(\n                X_train,\n                y_train,\n                sample_weight=w_train,\n                eval_set=[(X_valid, y_valid)],\n                eval_metric='logloss',\n                callbacks=[lgb.early_stopping(self.early_stop, verbose=1), lgb.log_evaluation(period=1000)],\n            )\n            # モデルを保存\n            joblib.dump(model, directory_path / f'lightgbm_model_efs_fold{fold}.pkl')\n\n            self.efs_lgb_models.append(model)\n\n            oof_preds[valid_index] = model.predict_proba(X_valid)[:, 1]\n\n            self.data.loc[valid_index, \"efs_pred_lgb\"] = oof_preds[valid_index]\n\n        print(\"lgb_efs完了\")\n# catboost-----------------------------------------------------------------------------------------------------\n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n            # 訓練データとバリデーションデータの分割\n            X_train = X.iloc[train_index]\n            X_valid = X.iloc[valid_index]\n            y_train = y.iloc[train_index]\n            y_valid = y.iloc[valid_index]\n            w_train = w.iloc[train_index]\n            model = CatBoostClassifier(**ctb_params_efs, verbose=0, cat_features=self.cat_cols)\n\n            model.fit(\n                X_train,\n                y_train,\n                sample_weight=w_train,\n                eval_set=(X_valid, y_valid),\n                early_stopping_rounds=self.early_stop,\n                verbose=1000,\n            )\n            model_file_path = directory_path / f'catboost_model_efs_fold{fold}.cbm'\n            # モデルを保存\n            model.save_model(model_file_path)\n            self.efs_cbt_models.append(model)\n\n            oof_preds[valid_index] = model.predict_proba(X_valid)[:, 1]\n            self.data.loc[valid_index, \"efs_pred_cbt\"] = oof_preds[valid_index]\n        print(\"ctb_efs完了\")\n# xgb-----------------------------------------------------------------------------------------------\n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n            # 訓練データとバリデーションデータの分割\n            X_train = X.iloc[train_index]\n            X_valid = X.iloc[valid_index]\n            y_train = y.iloc[train_index]\n            y_valid = y.iloc[valid_index]\n            w_train = w.iloc[train_index]\n            X_train_xgb = xgb.DMatrix(X_train.copy(), label=y_train, weight=w_train, enable_categorical=True)\n            X_valid_xgb = xgb.DMatrix(X_valid.copy(), label=y_valid, enable_categorical=True)\n\n             # モデルの学習\n            model = xgb.train(\n                params=xgb_params_efs,\n                dtrain=X_train_xgb,\n                num_boost_round=10000,                      # 最大イテレーション数\n                evals=[(X_train_xgb, 'train'), (X_valid_xgb, 'valid')],  # 学習データと検証データのセット\n                early_stopping_rounds=self.early_stop,    # CatBoostのearly_stopping_roundsに相当\n                verbose_eval=1000,                      # 学習ログの間隔\n                )\n\n             # モデルの保存\n            model_file_path = directory_path / f'xgboost_model_efs_fold{fold}.model'\n\n            # JSON形式で保存\n            model.save_model(model_file_path.with_suffix(\".json\"))\n\n            oof_preds[valid_index] = model.predict(X_valid_xgb)\n            self.data.loc[valid_index, \"efs_pred_xgb\"] = oof_preds[valid_index]\n\n            self.efs_xgb_models.append(model)\n\n            # モデルの読み込み\n            # model = xgb.Booster()\n            # model.load_model(\"xgboost_model_target1_fold0.json\")\n        print(\"xgb_efs完了\")\n\n        \n        return self.data\n\n    def score(self, df_true, df_prediction, event_label, interval_label, prediction_label): #層別C-indexの計算\n        # ID列を削除\n        del df_true[\"ID\"]\n        del df_prediction[\"ID\"]\n\n        merged_df = pd.concat([df_true, df_prediction], axis=1)\n        merged_df.reset_index(inplace=True)\n        merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups) # 人種ごとに該当するデータのインデックスを辞書形式で保管\n        metric_list = []\n        for race in merged_df_race_dict.keys(): #人種ごと\n            indices = sorted(merged_df_race_dict[race]) #該当する人種のインデックスを取得\n            merged_df_race = merged_df.iloc[indices] # 該当するインデックスのデータを取得\n            c_index_race = concordance_index( # concordance_indexはefs, efs_time, predicton（生存確率）からC-indexを計算\n                merged_df_race[interval_label],\n                -merged_df_race[prediction_label], # 符号を反転（通常 C-Index は「リスクが高いほど短命」と仮定するため）\n                merged_df_race[event_label])\n            metric_list.append(c_index_race)\n        return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))\n\n    def train_model(self, params, target, title, directory_path):\n        for col in self.cat_cols:\n            self.data[col] = self.data[col].astype('category')\n        X = self.data.drop(['ID',\n                            'efs',\n                            'efs_time',\n                            'target1',\n                            'target2',\n                            'target3',\n                            'target4',\n                            'target5',\n                           ], axis=1)\n        y = self.data[target]\n        w =  1 + self.data[\"efs\"]  # 最小1、最大2の重み\n        models, fold_scores = [], []\n        cv, oof_preds = self.targets._prepare_cv()\n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n            X_train = X.iloc[train_index]\n            X_valid = X.iloc[valid_index]\n            y_train = y.iloc[train_index]\n            y_valid = y.iloc[valid_index]\n            w_train = w.iloc[train_index]\n            if title.startswith('LightGBM'):\n                model = lgb.LGBMRegressor(**params)\n                model.fit(\n                    X_train,\n                    y_train,\n                    sample_weight=w_train,\n                    eval_set=[(X_valid, y_valid)],\n                    eval_metric='rmse',\n                    callbacks=[lgb.early_stopping(self.early_stop, verbose=0), lgb.log_evaluation(0)]\n                )\n                # モデルを保存\n                joblib.dump(model, directory_path / f'lightgbm_model_{target}_fold{fold}.pkl')\n            elif title.startswith('CatBoost'):\n                model = CatBoostRegressor(**params, verbose=0, cat_features=self.cat_cols)\n                model.fit(\n                    X_train,\n                    y_train,\n                    sample_weight=w_train,\n                    eval_set=(X_valid, y_valid),\n                    early_stopping_rounds=self.early_stop, \n                    verbose=0\n                )\n                model_file_path = directory_path / f'catboost_model_{target}_fold{fold}.cbm'\n                if target == 'target4':\n                    if params['grow_policy'] == 'Depthwise':\n                        model_file_path = directory_path / f'catboost_model_{target}_Cox1_fold{fold}.cbm'\n                    else:\n                        model_file_path = directory_path / f'catboost_model_{target}_Cox2_fold{fold}.cbm'\n                # モデルを保存\n                model.save_model(model_file_path)\n\n            models.append(model) # モデルのリストに追加\n            oof_preds[valid_index] = model.predict(X_valid) # 予測値を格納\n\n            y_true_fold = self.data.iloc[valid_index][['ID', 'efs', 'efs_time', 'race_group']].copy()\n            y_pred_fold = self.data.iloc[valid_index][['ID']].copy()\n            y_pred_fold['prediction'] = oof_preds[valid_index] # 予測値を格納\n\n            fold_score = self.score(y_true_fold, y_pred_fold, \"efs\", \"efs_time\", \"prediction\")\n            fold_scores.append(fold_score) # スコアのリストに追加\n        self.eda._plot_cv(fold_scores, title)\n        c_index_score = self.targets.validate_model(oof_preds, title)\n        return models, oof_preds\n\n    def infer_model(self, data, models):\n        data = data.drop(['ID'], axis=1)\n        for col in self.cat_cols: # カテゴリ変数をcategory型に変換\n            data[col] = data[col].astype('category')\n\n        efs_pred_lgb = np.mean([model.predict(data) for model in self.efs_lgb_models], axis=0)\n        efs_pred_cbt = np.mean([model.predict(data) for model in self.efs_cbt_models], axis=0)\n        efs_pred_xgb = np.mean([model.predict(xgb.DMatrix(data, enable_categorical=True)) for model in self.efs_xgb_models], axis=0)\n\n        data[\"efs_pred_lgb\"] = efs_pred_lgb\n        data[\"efs_pred_cbt\"] = efs_pred_cbt\n        data[\"efs_pred_xgb\"] = efs_pred_xgb\n        \n        return np.mean([model.predict(data) for model in models], axis=0)","metadata":{"_kg_hide-input":false,"papermill":{"duration":0.039648,"end_time":"2024-12-13T13:23:12.295144","exception":false,"start_time":"2024-12-13T13:23:12.255496","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:26.873079Z","iopub.execute_input":"2025-02-22T14:39:26.873501Z","iopub.status.idle":"2025-02-22T14:39:26.905829Z","shell.execute_reply.started":"2025-02-22T14:39:26.87347Z","shell.execute_reply":"2025-02-22T14:39:26.904333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MDクラスのインスタンス作成\nmd = MD(CFG.color, df_train, cat_cols, CFG.early_stop, CFG.penalizer, CFG.n_splits)","metadata":{"_kg_hide-input":false,"papermill":{"duration":0.01829,"end_time":"2024-12-13T13:23:12.32397","exception":false,"start_time":"2024-12-13T13:23:12.30568","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:26.907227Z","iopub.execute_input":"2025-02-22T14:39:26.907663Z","iopub.status.idle":"2025-02-22T14:39:26.927857Z","shell.execute_reply.started":"2025-02-22T14:39:26.907625Z","shell.execute_reply":"2025-02-22T14:39:26.926419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = md.create_targets(CFG.ctb_params_efs, CFG.lgb_params_efs, CFG.xgb_params_efs, directory_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:39:26.92936Z","iopub.execute_input":"2025-02-22T14:39:26.929739Z","iopub.status.idle":"2025-02-22T15:11:41.694916Z","shell.execute_reply.started":"2025-02-22T14:39:26.929712Z","shell.execute_reply":"2025-02-22T15:11:41.693706Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"各推定量のC統計量のうち、Cox比例ハザードモデルものが低い原因（あくまでも推定）\n- 過学習を防ぐ目的で`_penalizer`を追加しているため？？\n> Overall Stratified C-Index Score for Cox: 0.6564  \n> Overall Stratified C-Index Score for Kaplan-Meier: 0.9983  \n> Overall Stratified C-Index Score for Nelson-Aalen: 0.9983  \n","metadata":{}},{"cell_type":"code","source":"md.eda.bar_chart('race_group')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:41.699237Z","iopub.execute_input":"2025-02-22T15:11:41.699586Z","iopub.status.idle":"2025-02-22T15:11:41.983666Z","shell.execute_reply.started":"2025-02-22T15:11:41.699557Z","shell.execute_reply":"2025-02-22T15:11:41.981851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"md.eda.distribution_plot('target1', 'Cox Target')","metadata":{"papermill":{"duration":39.008232,"end_time":"2024-12-13T13:23:51.341762","exception":false,"start_time":"2024-12-13T13:23:12.33353","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:41.986744Z","iopub.execute_input":"2025-02-22T15:11:41.987074Z","iopub.status.idle":"2025-02-22T15:11:42.367246Z","shell.execute_reply.started":"2025-02-22T15:11:41.987049Z","shell.execute_reply":"2025-02-22T15:11:42.365844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"md.eda.distribution_plot('target2', 'Kaplan-Meier Target')","metadata":{"papermill":{"duration":0.180799,"end_time":"2024-12-13T13:23:51.538524","exception":false,"start_time":"2024-12-13T13:23:51.357725","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:42.368454Z","iopub.execute_input":"2025-02-22T15:11:42.368752Z","iopub.status.idle":"2025-02-22T15:11:42.73106Z","shell.execute_reply.started":"2025-02-22T15:11:42.368725Z","shell.execute_reply":"2025-02-22T15:11:42.729864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"md.eda.distribution_plot('target3', 'Nelson-Aalen Target')","metadata":{"papermill":{"duration":0.169091,"end_time":"2024-12-13T13:23:51.718819","exception":false,"start_time":"2024-12-13T13:23:51.549728","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:42.732572Z","iopub.execute_input":"2025-02-22T15:11:42.733058Z","iopub.status.idle":"2025-02-22T15:11:43.120159Z","shell.execute_reply.started":"2025-02-22T15:11:42.733016Z","shell.execute_reply":"2025-02-22T15:11:43.118901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"md.eda.distribution_plot('target4', 'Cox-Loss Target')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:43.121319Z","iopub.execute_input":"2025-02-22T15:11:43.121605Z","iopub.status.idle":"2025-02-22T15:11:43.920228Z","shell.execute_reply.started":"2025-02-22T15:11:43.121579Z","shell.execute_reply":"2025-02-22T15:11:43.918915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"md.eda.distribution_plot('target5', 'Pairwise Target')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fe.info(df_train)","metadata":{"papermill":{"duration":0.079694,"end_time":"2024-12-13T13:23:51.808887","exception":false,"start_time":"2024-12-13T13:23:51.729193","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:43.921048Z","iopub.execute_input":"2025-02-22T15:11:43.921308Z","iopub.status.idle":"2025-02-22T15:11:43.957118Z","shell.execute_reply.started":"2025-02-22T15:11:43.921285Z","shell.execute_reply":"2025-02-22T15:11:43.955918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">Cox比例ハザードモデルによる予測</p>\n\n- 目的変数\n    - target1\n- 予測モデル\n    - lgbm\n    - catboost","metadata":{"papermill":{"duration":0.011062,"end_time":"2024-12-13T13:23:51.83194","exception":false,"start_time":"2024-12-13T13:23:51.820878","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# CatBoost\nctb1_models, ctb1_oof_preds = md.train_model(CFG.ctb_params, target='target1', title='CatBoost', directory_path=directory_path)","metadata":{"papermill":{"duration":4808.205155,"end_time":"2024-12-13T14:44:00.048534","exception":false,"start_time":"2024-12-13T13:23:51.843379","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:11:43.958392Z","iopub.execute_input":"2025-02-22T15:11:43.958737Z","execution_failed":"2025-02-22T15:51:06.981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LightGBM\nlgb1_models, lgb1_oof_preds = md.train_model(CFG.lgb_params, target='target1', title='LightGBM', directory_path=directory_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ctb1_preds = md.infer_model(test_data, ctb1_models)","metadata":{"papermill":{"duration":0.071675,"end_time":"2024-12-13T14:48:51.427728","exception":false,"start_time":"2024-12-13T14:48:51.356053","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb1_preds = md.infer_model(test_data, lgb1_models)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">カプラン・マイヤー推定量による予測</p>\n\n- 目的変数\n    - target2\n- 予測モデル\n    - lgbm\n    - catboost","metadata":{"papermill":{"duration":0.010491,"end_time":"2024-12-13T14:48:51.662203","exception":false,"start_time":"2024-12-13T14:48:51.651712","status":"completed"},"tags":[]}},{"cell_type":"code","source":"ctb2_models, ctb2_oof_preds = md.train_model(CFG.ctb_params, target='target2', title='CatBoost', directory_path=directory_path)","metadata":{"papermill":{"duration":2470.336385,"end_time":"2024-12-13T15:30:02.009405","exception":false,"start_time":"2024-12-13T14:48:51.67302","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb2_models, lgb2_oof_preds = md.train_model(CFG.lgb_params, target='target2', title='LightGBM', directory_path=directory_path)","metadata":{"papermill":{"duration":38.457069,"end_time":"2024-12-13T15:30:40.477711","exception":false,"start_time":"2024-12-13T15:30:02.020642","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ctb2_preds = md.infer_model(test_data, ctb2_models)","metadata":{"papermill":{"duration":0.073959,"end_time":"2024-12-13T15:30:40.563068","exception":false,"start_time":"2024-12-13T15:30:40.489109","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb2_preds = md.infer_model(test_data, lgb2_models)","metadata":{"papermill":{"duration":0.170712,"end_time":"2024-12-13T15:30:40.747856","exception":false,"start_time":"2024-12-13T15:30:40.577144","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">ネルソン・アーレン推定量による予測</p>\n\n- 目的変数\n    - target3\n- 予測モデル\n    - lgbm\n    - catboost","metadata":{}},{"cell_type":"code","source":"ctb3_models, ctb3_oof_preds = md.train_model(CFG.ctb_params, target='target3', title='CatBoost', directory_path=directory_path)","metadata":{"papermill":{"duration":2199.149038,"end_time":"2024-12-13T16:07:19.930791","exception":false,"start_time":"2024-12-13T15:30:40.781753","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb3_models, lgb3_oof_preds = md.train_model(CFG.lgb_params, target='target3', title='LightGBM', directory_path=directory_path)","metadata":{"papermill":{"duration":35.221097,"end_time":"2024-12-13T16:07:55.163635","exception":false,"start_time":"2024-12-13T16:07:19.942538","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ctb3_preds = md.infer_model(test_data, ctb3_models)","metadata":{"papermill":{"duration":0.06646,"end_time":"2024-12-13T16:07:55.241797","exception":false,"start_time":"2024-12-13T16:07:55.175337","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb3_preds = md.infer_model(test_data, lgb3_models)","metadata":{"papermill":{"duration":0.164669,"end_time":"2024-12-13T16:07:55.41872","exception":false,"start_time":"2024-12-13T16:07:55.254051","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">Cox-Lossによる予測</p>\n\n- 目的変数\n    - target4\n- 予測モデル\n    - catboost\n        - 木の深さ優先\n        - 損失最小優先","metadata":{}},{"cell_type":"code","source":"# 木の深さ優先\ncox1_models, cox1_oof_preds = md.train_model(CFG.cox1_params, target='target4', title='CatBoost', directory_path=directory_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 損失最小優先\ncox2_models, cox2_oof_preds = md.train_model(CFG.cox2_params, target='target4', title='CatBoost', directory_path=directory_path)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cox1_preds = md.infer_model(test_data, cox1_models)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cox2_preds = md.infer_model(test_data, cox2_models)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ctb5_models, ctb5_oof_preds = md.train_model(CFG.ctb_params, target='target5', title='CatBoost', directory_path=directory_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb5_models, lgb5_oof_preds = md.train_model(CFG.lgb_params, target='target5', title='LightGBM', directory_path=directory_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ctb5_preds = md.infer_model(test_data, ctb5_models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb5_preds = md.infer_model(test_data, lgb5_models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">ニューラルネットワーク</p>","metadata":{}},{"cell_type":"code","source":"def transform_target(df_train):\n    #efs_timeを対数変換（結局これが目的変数）\n    df_train['y'] = np.log(1 + df_train.efs_time)\n    return df_train\n\n#ラベルエンコーディングを行い，カテゴリカル変数を整数変数に変換\ndef get_X_cat(df_train, cat_cols, transformers=None):\n    if transformers is None:\n        transformers = [LabelEncoder().fit(df_train[col]) for col in cat_cols]\n    return transformers, np.array(\n        [transformer.transform(df_train[col]) for col, transformer in zip(cat_cols, transformers)]\n    ).T\n\n\ndef preprocess_data(df_train, val):\n    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(df_train, val)\n    #標準化\n    scaler = StandardScaler()\n    #欠損値を平均値で補完\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n    X_num_train = imp.fit_transform(df_train[numerical])\n    X_num_train = scaler.fit_transform(X_num_train)\n    X_num_val = imp.transform(val[numerical])\n    X_num_val = scaler.transform(X_num_val)\n    dl_train = init_dl(X_cat_train, X_num_train, df_train, training=True)\n    dl_val = init_dl(X_cat_val, X_num_val, val)\n    return X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n\n\ndef get_categoricals(df_train, val):\n    categorical_cols, numerical = get_feature_types(df_train)\n    remove = []\n    for col in categorical_cols:\n        if df_train[col].nunique() == 1:\n            remove.append(col)\n        ind = ~val[col].isin(df_train[col])\n        if ind.any():\n            val.loc[ind, col] = np.nan\n    categorical_cols = [col for col in categorical_cols if col not in remove]\n    transformers, X_cat_train = get_X_cat(df_train, categorical_cols)\n    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n    return X_cat_train, X_cat_val, numerical, transformers\n\n\ndef init_dl(X_cat, X_num, df_train, training=False):\n    ds_train = TensorDataset(\n        torch.tensor(X_cat, dtype=torch.long),\n        torch.tensor(X_num, dtype=torch.float32),\n        torch.tensor(df_train.efs_time.values, dtype=torch.float32).log(),\n        torch.tensor(df_train.efs.values, dtype=torch.long)\n    )\n    bs = 2048\n    # if not training:\n    #     bs = 2048 * 8\n    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n    return dl_train\n\n\ndef get_feature_types(df_train):\n    categorical_cols = [col for i, col in enumerate(df_train.columns) if ((df_train[col].dtype == \"object\") | (2 < df_train[col].nunique() < 25))]\n    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n    FEATURES = [c for c in df_train.columns if not c in RMV]\n    print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n    numerical = [i for i in FEATURES if i not in categorical_cols]\n    return categorical_cols, numerical\n\n#dataフレームに特徴量を追加\ndef add_features(df):\n    sex_match = df.sex_match.astype(str)\n    sex_match = sex_match.str.split(\"-\").str[0] == sex_match.str.split(\"-\").str[1]\n    df['sex_match_bool'] = sex_match\n    df.loc[df.sex_match.isna(), 'sex_match_bool'] = np.nan\n    df.loc[df.year_hct == 2019, 'year_hct'] = 2020\n    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n    df['age_bin'] = pd.cut(df.age_at_hct, [0, 0.0441, 16, 30, 50, 100])\n    df['age_ts'] = df.age_at_hct / df.donor_age\n    df['age_comorbidity'] = df['age_at_hct'] * df['comorbidity_score']\n    df['age_karnofsky'] = df['age_at_hct'] * df['karnofsky_score']\n    df['karnofsky_squared'] = df['karnofsky_score'] ** 2\n    df['cos_year'] = np.cos(df['year_hct'] * (2 * np.pi) / 100)\n    df['diff_age_vs_donor'] = df['age_at_hct'] - df['donor_age']\n    # sex_one: 性別一致情報から患者の性別を抽出\n    df['sex_one'] = df['sex_match'].str[0]  # 最初の文字を抽出\n    # sex_two: 性別一致情報からドナーの性別を抽出\n    df['sex_two'] = df['sex_match'].str[2]  # 3番目の文字を抽出\n    # SameSex: 患者とドナーの性別が一致しているか\n    df['same_sex'] = (df['sex_one'] == df['sex_two']).astype(int)\n    return df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CatEmbeddings(nn.Module):\n    def __init__(\n        self,\n        projection_dim: int,\n        categorical_cardinality: List[int],\n        embedding_dim: int\n    ):\n        super(CatEmbeddings, self).__init__()\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(cardinality, embedding_dim)\n            for cardinality in categorical_cardinality\n        ])\n        self.projection = nn.Sequential(\n            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n            nn.GELU(),\n            nn.Linear(projection_dim, projection_dim)\n        )\n\n    def forward(self, x_cat):\n        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n        x_cat = torch.cat(x_cat, dim=1)\n        return self.projection(x_cat)\n\n\nclass NN(nn.Module):\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            dropout: float = 0\n    ):\n        super(NN, self).__init__()\n        self.embeddings = CatEmbeddings(projection_dim, categorical_cardinality, embedding_dim)\n        self.mlp = nn.Sequential(\n            ODST(projection_dim + continuous_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(dropout)\n        )\n        self.out = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n\n        # initialize weights\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x_cat, x_cont):\n        x = self.embeddings(x_cat)\n        x = torch.cat([x, x_cont], dim=1)\n        x = self.dropout(x)\n        x = self.mlp(x)\n        return self.out(x), x\n\n\n@functools.lru_cache\ndef combinations(N):\n    ind = torch.arange(N)\n    comb = torch.combinations(ind, r=2)\n    return comb  # CPU 上でそのまま使う\n\n\nclass LitNN(pl.LightningModule):\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            lr: float = 1e-3,\n            dropout: float = 0.2,\n            weight_decay: float = 1e-3,\n            aux_weight: float = 0.1,\n            margin: float = 0.5,\n            race_index: int = 0\n    ):\n        super(LitNN, self).__init__()\n        self.save_hyperparameters()\n        self.model = NN(\n            continuous_dim=self.hparams.continuous_dim,\n            categorical_cardinality=self.hparams.categorical_cardinality,\n            embedding_dim=self.hparams.embedding_dim,\n            projection_dim=self.hparams.projection_dim,\n            hidden_dim=self.hparams.hidden_dim,\n            dropout=self.hparams.dropout\n        )\n        self.targets = []\n        self.aux_cls = nn.Sequential(\n            nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim // 3),\n            nn.GELU(),\n            nn.Linear(self.hparams.hidden_dim // 3, 1)\n        )\n\n    def on_before_optimizer_step(self, optimizer):\n        # Compute the 2-norm for each layer\n        # If using mixed precision, the gradients are already unscaled here\n        norms = grad_norm(self.model, norm_type=2)\n        self.log_dict(norms)\n\n    def forward(self, x_cat, x_cont):\n        x, emb = self.model(x_cat, x_cont)\n        return x.squeeze(1), emb\n\n    def training_step(self, batch, batch_idx):\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        aux_pred = self.aux_cls(emb).squeeze(1)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        aux_loss = nn.functional.mse_loss(aux_pred, y, reduction='none')\n        aux_mask = efs == 1\n        aux_loss = (aux_loss * aux_mask).sum() / aux_mask.sum()\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"race_loss\", race_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        self.log(\"aux_loss\", aux_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        return loss + aux_loss * self.hparams.aux_weight\n\n    def get_full_loss(self, efs, x_cat, y, y_hat):\n        loss = self.calc_loss(y, y_hat, efs)\n        race_loss = self.get_race_losses(efs, x_cat, y, y_hat)\n        loss += 0.1 * race_loss\n        return loss, race_loss\n\n    def get_race_losses(self, efs, x_cat, y, y_hat):\n        races = torch.unique(x_cat[:, self.hparams.race_index])\n        race_losses = []\n        for race in races:\n            ind = x_cat[:, self.hparams.race_index] == race\n            race_losses.append(self.calc_loss(y[ind], y_hat[ind], efs[ind]))\n        race_loss = sum(race_losses) / len(race_losses)\n        races_loss_std = sum((r - race_loss)**2 for r in race_losses) / len(race_losses)\n        return torch.sqrt(races_loss_std)\n\n    def calc_loss(self, y, y_hat, efs):\n        N = y.shape[0]\n        comb = combinations(N)\n        comb = comb[(efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)]\n        pred_left = y_hat[comb[:, 0]]\n        pred_right = y_hat[comb[:, 1]]\n        y_left = y[comb[:, 0]]\n        y_right = y[comb[:, 1]]\n        y = 2 * (y_left > y_right).int() - 1\n        loss = nn.functional.relu(-y * (pred_left - pred_right) + self.hparams.margin)\n        mask = self.get_mask(comb, efs, y_left, y_right)\n        loss = (loss.double() * (mask.double())).sum() / mask.sum()\n        return loss\n\n    def get_mask(self, comb, efs, y_left, y_right):\n        # mask1 = (efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)\n        left_outlived = y_left >= y_right\n        left_1_right_0 = (efs[comb[:, 0]] == 1) & (efs[comb[:, 1]] == 0)\n        mask2 = (left_outlived & left_1_right_0)\n        right_outlived = y_right >= y_left\n        right_1_left_0 = (efs[comb[:, 1]] == 1) & (efs[comb[:, 0]] == 0)\n        mask2 |= (right_outlived & right_1_left_0)\n        mask2 = ~mask2\n        mask = mask2\n        return mask\n\n    def validation_step(self, batch, batch_idx):\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n    def on_validation_epoch_end(self):\n        cindex, metric = self._calc_cindex()\n        self.log(\"cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n    def _calc_cindex(self):\n        y = torch.cat([t[0] for t in self.targets]).cpu().numpy()\n        y_hat = torch.cat([t[1] for t in self.targets]).cpu().numpy()\n        efs = torch.cat([t[2] for t in self.targets]).cpu().numpy()\n        races = torch.cat([t[3] for t in self.targets]).cpu().numpy()\n        metric = self._metric(efs, races, y, y_hat)\n        cindex = concordance_index(y, y_hat, efs)\n        return cindex, metric\n\n    def _metric(self, efs, races, y, y_hat):\n        metric_list = []\n        for race in np.unique(races):\n            y_ = y[races == race]\n            y_hat_ = y_hat[races == race]\n            efs_ = efs[races == race]\n            metric_list.append(concordance_index(y_, y_hat_, efs_))\n        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n        return metric\n\n    def test_step(self, batch, batch_idx):\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def on_test_epoch_end(self) -> None:\n        cindex, metric = self._calc_cindex()\n        self.log(\"test_cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"test_cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n        scheduler_config = {\n            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer,\n                T_max=45,\n                eta_min=6e-3\n            ),\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n            \"strict\": False,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pl.seed_everything(42)\n\ndef train_final(X_num_train, dl_train, dl_val, transformers, hparams=None, categorical_cols=None):\n\n    # ハイパーパラメータのデフォルト値設定\n    if hparams is None:\n        hparams = {\n            \"embedding_dim\": 16,\n            \"projection_dim\": 112,\n            \"hidden_dim\": 56,\n            \"lr\": 0.06464861983337984,\n            \"dropout\": 0.05463240181423116,\n            \"aux_weight\": 0.26545778308743806,\n            \"margin\": 0.2588153271003354,\n            \"weight_decay\": 0.0002773544957610778\n        }\n\n    # モデルインスタンスの生成\n    model = LitNN(\n        continuous_dim=X_num_train.shape[1],\n        categorical_cardinality=[len(t.classes_) for t in transformers],\n        race_index=categorical_cols.index(\"race_group\"),\n        **hparams\n    )\n\n    # チェックポイントコールバック：検証ロス（val_loss）が最小のモデルを保存\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        monitor=\"val_loss\",\n        mode=\"min\",\n        save_top_k=1,\n        filename=\"best-checkpoint\"\n    )\n\n    # Trainerの初期化\n    trainer = pl.Trainer(\n        accelerator='cpu',  # GPUが使用可能なら \"gpu\" に変更\n        max_epochs=60,\n        callbacks=[\n            checkpoint_callback,\n            LearningRateMonitor(logging_interval='epoch'),\n            TQDMProgressBar(),\n            StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n        ],\n    )\n\n    # 学習の実施（引数名を明示的にして分かりやすく）\n    trainer.fit(model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n\n    # チェックポイントから最良モデルを復元（best_model_pathが存在する場合）\n    best_model_path = checkpoint_callback.best_model_path\n    if best_model_path:\n        print(f\"Loading best model from: {best_model_path}\")\n        model = LitNN.load_from_checkpoint(best_model_path)\n    else:\n        print(\"No checkpoint found. Using last model state.\")\n\n    # 検証データでテスト実行（結果表示などが必要ならここで利用）\n    trainer.test(model, dataloaders=dl_val)\n\n    # 評価モードへ切替え\n    model.eval()\n    return model\n\n\ndef main(hparams):\n    FE_test = FE()\n    test= FE_test._load_data(CFG.test_path)\n    test=add_features(test)\n    FE_train = FE()\n    train_original = FE_train._load_data(CFG.train_path)\n    train_original=add_features(train_original)\n    test['efs_time'] = 1\n    test['efs'] = 1\n\n    # 学習データの行数に合わせたOOF配列を用意\n    oof_nn_preds = np.zeros(train_original.shape[0])\n    test_nn_preds = np.zeros(test.shape[0])\n    \n    # StratifiedKFoldのセットアップ（ここでは例として5fold）\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    for fold, (train_index, valid_index) in enumerate(\n        kf.split(train_original, train_original.race_group.astype(str) + (train_original.age_at_hct == 0.044).astype(str))\n    ):\n        # foldごとにデータを分割\n        train = train_original.iloc[train_index].copy()\n        valid = train_original.iloc[valid_index].copy()\n        \n        # データ前処理（数値部分・カテゴリ部分の整形等）\n        X_cat_valid, X_num_train, X_num_valid, dl_train, dl_valid, transformers = preprocess_data(train, valid)\n        categorical_cols, _ = get_feature_types(train)\n        \n        # モデルの学習\n        model = train_final(X_num_train, dl_train, dl_valid, transformers, hparams, categorical_cols=categorical_cols)\n        \n        # 検証データに対する予測\n        # ※入力テンソルの作成（GPU使用の場合はto(device)等を追加）\n        with torch.no_grad():\n            pred_valid, _ = model.cpu().eval()(\n                torch.tensor(X_cat_valid, dtype=torch.long),\n                torch.tensor(X_num_valid, dtype=torch.float32)\n            )\n        # もともとのコードでは提出用に符号反転していたので同様に\n        oof_nn_preds[valid_index] = -pred_valid.detach().cpu().numpy().reshape(-1)\n        \n        # testデータに対する予測（foldごとに同じ手順で処理）\n        # ここでは train のうち fold 用の部分を使ってテストに対する予測を実施\n        X_cat_test, _, X_num_test, _, _, _ = preprocess_data(train, test)\n        with torch.no_grad():\n            pred_test, _ = model.cpu().eval()(\n                torch.tensor(X_cat_test, dtype=torch.long),\n                torch.tensor(X_num_test, dtype=torch.float32)\n            )\n        test_nn_preds += -pred_test.detach().cpu().numpy().reshape(-1)\n        \n    # 各foldのtest予測の平均を最終のNNテスト予測とする\n    test_nn_preds /= kf.n_splits\n\n    \n    # NNモデルの結果として、OOFとTest予測の両方を返す\n    return oof_nn_preds, test_nn_preds\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nn_oof_preds, nn_test_preds = main(hparams=None)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CIBMTR Yunbase\n","metadata":{}},{"cell_type":"code","source":"!pip install -q --requirement /kaggle/input/yunbase/Yunbase/requirements.txt  \\\n--no-index --find-links file:/kaggle/input/yunbase/\n\n!pip install -q /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install -q /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source_file_path = '/kaggle/input/yunbase/Yunbase/baseline.py'\ntarget_file_path = '/kaggle/working/baseline.py'\nwith open(source_file_path, 'r', encoding='utf-8') as file:\n    content = file.read()\nwith open(target_file_path, 'w', encoding='utf-8') as file:\n    file.write(content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from baseline import Yunbase\nimport pandas as pd#read csv,parquet\nimport numpy as np#for scientific computation of matrices\nfrom  lightgbm import LGBMRegressor,LGBMClassifier,log_evaluation,early_stopping\nfrom catboost import CatBoostRegressor,CatBoostClassifier\nfrom xgboost import XGBRegressor,XGBClassifier\nfrom lifelines import KaplanMeierFitter\nimport warnings#avoid some negligible errors\n#The filterwarnings () method is used to set warning filters, which can control the output method and level of warning information.\nwarnings.filterwarnings('ignore')\nimport random#provide some function to generate random_seed.\n#set random seed,to make sure model can be recurrented.\ndef seed_everything(seed):\n    np.random.seed(seed)#numpy's random seed\n    random.seed(seed)#python built-in random seed\nseed_everything(seed=2025)\n\ntrain=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\ntrain_solution=train[['ID','efs','efs_time','race_group']].copy()\n\ndef logit(p):\n    return np.log(p) - np.log(1 - p)\nmax_efs_time,min_efs_time=80,-100\ntrain['efs_time']=train['efs_time']/(max_efs_time-min_efs_time)\ntrain['efs_time']=train['efs_time'].apply(lambda x:logit(x))\ntrain['efs_time']+=10\nprint(train['efs_time'].max(),train['efs_time'].min())\n\nrace2weight={'American Indian or Alaska Native':0.68,\n'Asian':0.7,'Black or African-American':0.67,\n'More than one race':0.68,\n'Native Hawaiian or other Pacific Islander':0.66,\n'White':0.64}\ntrain['weight']=0.5*train['efs']+0.5\ntrain['raceweight']=train['race_group'].apply(lambda x:race2weight.get(x,1))\ntrain['weight']=train['weight']/train['raceweight']\ntrain.drop(['raceweight'],axis=1,inplace=True)\n\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\ndef transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n\n    kmf = KaplanMeierFitter()\n    \n    kmf.fit(df[time_col], event_observed=df[event_col])\n    \n    survival_probabilities = kmf.survival_function_at_times(df[time_col]).values.flatten()\n\n    return survival_probabilities\n\nrace_group=sorted(train['race_group'].unique())\nfor race in race_group:\n    train.loc[train['race_group']==race,\"target\"] = transform_survival_probability(train[train['race_group']==race], time_col='efs_time', event_col='efs')\n    gap=0.7*(train.loc[(train['race_group']==race)&(train['efs']==0)]['target'].max()-train.loc[(train['race_group']==race)&(train['efs']==1)]['target'].min())/2\n    train.loc[(train['race_group']==race)&(train['efs']==0),'target']-=gap\n\nsns.histplot(data=train, x='target', hue='efs', element='step', stat='density', common_norm=False)\nplt.legend(title='efs')\nplt.title('Distribution of Target by EFS')\nplt.xlabel('Target')\nplt.ylabel('Density')\nplt.show()\n\ntrain.drop(['efs','efs_time'],axis=1,inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#nunique=2\nnunique2=[col for col in train.columns if train[col].nunique()==2 and col!='efs']\n#nunique<50\nnunique50=[col for col in train.columns if train[col].nunique()<50 and col not in ['efs','weight']]+['age_group','dri_score_NA']\n\ndef FE(df):\n    print(\"< deal with outlier >\")\n    df['nan_value_each_row'] = df.isnull().sum(axis=1)\n    #year_hct=2020 only 4 rows.\n    df['year_hct']=df['year_hct'].replace(2020,2019)\n    df['age_group']=df['age_at_hct']//10\n    #karnofsky_score 40 only 10 rows.\n    df['karnofsky_score']=df['karnofsky_score'].replace(40,50)\n    #hla_high_res_8=2 only 2 rows.\n    df['hla_high_res_8']=df['hla_high_res_8'].replace(2,3)\n    #hla_high_res_6=0 only 1 row.\n    df['hla_high_res_6']=df['hla_high_res_6'].replace(0,2)\n    #hla_high_res_10=3 only 1 row.\n    df['hla_high_res_10']=df['hla_high_res_10'].replace(3,4)\n    #hla_low_res_8=2 only 1 row.\n    df['hla_low_res_8']=df['hla_low_res_8'].replace(2,3)\n    df['dri_score']=df['dri_score'].replace('Missing disease status','N/A - disease not classifiable')\n    df['dri_score_NA']=df['dri_score'].apply(lambda x:int('N/A' in str(x)))\n    for col in ['diabetes','pulm_moderate','cardiac']:\n        df.loc[df[col].isna(),col]='Not done'\n\n    print(\"< cross feature >\")\n    df['donor_age-age_at_hct']=df['donor_age']-df['age_at_hct']\n    df['comorbidity_score+karnofsky_score']=df['comorbidity_score']+df['karnofsky_score']\n    df['comorbidity_score-karnofsky_score']=df['comorbidity_score']-df['karnofsky_score']\n    df['comorbidity_score*karnofsky_score']=df['comorbidity_score']*df['karnofsky_score']\n    df['comorbidity_score/karnofsky_score']=df['comorbidity_score']/df['karnofsky_score']\n    \n    print(\"< fillna >\")\n    df[nunique50]=df[nunique50].astype(str).fillna('NaN')\n    \n    print(\"< combine category feature >\")\n    for i in range(len(nunique2)):\n        for j in range(i+1,len(nunique2)):\n            df[nunique2[i]+nunique2[j]]=df[nunique2[i]].astype(str)+df[nunique2[j]].astype(str)\n    \n    print(\"< drop useless columns >\")\n    df.drop(['ID'],axis=1,inplace=True,errors='ignore')\n    return df\n\ncombine_category_cols=[]\nfor i in range(len(nunique2)):\n    for j in range(i+1,len(nunique2)):\n        combine_category_cols.append(nunique2[i]+nunique2[j])  \n\ntotal_category_feature=nunique50+combine_category_cols\n\ntarget_stat=[]\nfor j in range(len(total_category_feature)):\n   for col in ['donor_age','age_at_hct','target']:\n    target_stat.append( (total_category_feature[j],col,['count','mean','max','std','skew']) )\n\nnum_folds=10\n\nimport torch\n\n# GPUが利用可能か判定\nuse_gpu = torch.cuda.is_available()\n\n# LightGBM のパラメータ設定\nlgb_params = {\n    \"boosting_type\": \"gbdt\",\n    \"metric\": 'mae',\n    'random_state': 2025,\n    \"max_depth\": 9,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 768,\n    \"colsample_bytree\": 0.6,\n    \"colsample_bynode\": 0.6,\n    \"verbose\": -1,\n    \"reg_alpha\": 0.2,\n    \"reg_lambda\": 5,\n    \"extra_trees\": True,\n    'num_leaves': 64,\n    \"max_bin\": 255,\n    'importance_type': 'gain',  # better than 'split'\n    'device': 'gpu' if use_gpu else 'cpu',  # GPUが使えればGPU、使えなければCPU\n}\n\n# GPU使用時の追加設定 (LightGBM)\nif use_gpu:\n    lgb_params['gpu_use_dp'] = True\n\n# CatBoost のパラメータ設定\ncat_params = {\n    'random_state': 2025,\n    'eval_metric': 'MAE',\n    'bagging_temperature': 0.50,\n    'iterations': 650,\n    'learning_rate': 0.1,\n    'max_depth': 8,\n    'l2_leaf_reg': 1.25,\n    'min_data_in_leaf': 24,\n    'random_strength': 0.25,\n    'verbose': 0,\n    'task_type': 'GPU' if use_gpu else 'CPU',  # GPUが使えればGPU、使えなければCPU\n}\n\n# XGBoost のパラメータ設定 (コメント解除で使用可能)\nxgb_params = {\n    'random_state': 2025,\n    'n_estimators': 256,\n    'learning_rate': 0.1,\n    'max_depth': 6,\n    'reg_alpha': 0.08,\n    'reg_lambda': 0.8,\n    'subsample': 0.95,\n    'colsample_bytree': 0.6,\n    'min_child_weight': 3,\n    'early_stopping_rounds': 1024,\n    'enable_categorical': True,\n    'tree_method': 'gpu_hist' if use_gpu else 'hist',  # GPUが使えればGPU、使えなければCPU\n}\n\nyunbase=Yunbase(num_folds=num_folds,\n                  models=[(LGBMRegressor(**lgb_params),'lgb'),\n                          (CatBoostRegressor(**cat_params),'cat'),\n                          # (XGBRegressor(**xgb_params),'xgb')\n                         ],\n                  FE=FE,\n                  seed=2025,\n                  objective='regression',\n                  metric='mae',\n                  target_col='target',\n                  device='cpu',\n                  # device='gpu',\n                  one_hot_max=-1,\n                  early_stop=1000,\n                  cross_cols=['donor_age','age_at_hct'],\n                  target_stat=target_stat,\n                  use_data_augmentation=True,\n                  use_scaler=True,\n                  log=250,\n                  plot_feature_importance=True,\n                  #print metric score when model training\n                  use_eval_metric=False,\n)\nyunbase.fit(train,category_cols=nunique2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas.api.types\nfrom lifelines.utils import concordance_index\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \n    del solution[row_id_column_name]\n    del submission[row_id_column_name]\n    \n    event_label = 'efs'\n    interval_label = 'efs_time'\n    prediction_label = 'prediction'\n    for col in submission.columns:\n        if not pandas.api.types.is_numeric_dtype(submission[col]):\n            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n    # Merging solution and submission dfs on ID\n    merged_df = pd.concat([solution, submission], axis=1)\n    merged_df.reset_index(inplace=True)\n    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n    metric_list = []\n    for race in merged_df_race_dict.keys():\n        # Retrieving values from y_test based on index\n        indices = sorted(merged_df_race_dict[race])\n        merged_df_race = merged_df.iloc[indices]\n        # Calculate the concordance index\n        c_index_race = concordance_index(\n                        merged_df_race[interval_label],\n                        -merged_df_race[prediction_label],\n                        merged_df_race[event_label])\n        metric_list.append(c_index_race)\n    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))\n\nweights = [0.5,0.5]\n\nlgb_prediction=np.load(f\"Yunbase_info/lgb_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\nlgb_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':lgb_prediction})\nprint(f\"lgb_score:{score(train_solution.copy(),lgb_prediction.copy(),row_id_column_name='ID')}\")\n# xgb_prediction=np.load(f\"Yunbase_info/xgb_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\n# xgb_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':xgb_prediction})\n# print(f\"xgb_score:{score(train_solution.copy(),xgb_prediction.copy(),row_id_column_name='ID')}\")\ncat_prediction=np.load(f\"Yunbase_info/cat_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\ncat_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':cat_prediction})\nprint(f\"cat_score:{score(train_solution.copy(),cat_prediction.copy(),row_id_column_name='ID')}\")\n\ny_preds=[\n    lgb_prediction.copy(),\n    # xgb_prediction.copy(),\n    cat_prediction.copy()\n]\n\nfinal_prediction=lgb_prediction.copy()\nfinal_prediction['prediction']=0\nfor i in range(len(y_preds)):\n    final_prediction['prediction']+=weights[i]*y_preds[i]['prediction']\nmetric=score(train_solution.copy(),final_prediction.copy(),row_id_column_name='ID')\nprint(f\"final_CV:{metric}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds=yunbase.predict(test,weights=weights)\nyunbase.target_col='prediction'\nyunbase.submit(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\",test_preds,\n               save_name='submission1'\n              )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">アンサンブル</p>","metadata":{"papermill":{"duration":0.01174,"end_time":"2024-12-13T16:07:55.442686","exception":false,"start_time":"2024-12-13T16:07:55.430946","status":"completed"},"tags":[]}},{"cell_type":"code","source":"oof_preds = [\n    ctb1_oof_preds, \n    lgb1_oof_preds, \n    ctb2_oof_preds, \n    lgb2_oof_preds, \n    ctb3_oof_preds, \n    lgb3_oof_preds,\n    cox1_oof_preds,\n    cox2_oof_preds,\n    ctb5_oof_preds, \n    lgb5_oof_preds,\n    nn_oof_preds,\n    np.array(pairwise_ranking_oof),\n    final_prediction['prediction'].to_numpy()\n]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sub2 = pd.read_csv('/kaggle/working/submission2.csv')\n\n\npreds = [\n    ctb1_preds, \n    lgb1_preds, \n    ctb2_preds, \n    lgb2_preds, \n    ctb3_preds, \n    lgb3_preds,\n    cox1_preds,\n    cox2_preds,\n    ctb5_preds, \n    lgb5_preds,\n    nn_test_preds,\n    np.array(pairwise_ranking_pred),\n    test_preds,\n]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">アンサンブル結果の精度評価</p>","metadata":{}},{"cell_type":"code","source":"# oof_preds（各予測モデル×）の中身について順位付け（値の小さい順）した結果を格納\nranked_oof_preds = np.array([rankdata(p) for p in oof_preds])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\n# 目的関数の定義\ndef objective(trial):\n    # 各モデルの重みを提案 (0.0～1.0 の範囲)\n    weights = [trial.suggest_float(f'weight_{i}', 0.05, 1.0) for i in range(len(ranked_oof_preds))]\n\n    # 重みの正規化（合計が1になるようにスケーリング）\n    weights = np.array(weights)\n    weights /= np.sum(weights)\n\n    # アンサンブル予測を計算 (加重平均)\n    ensemble_oof_preds = np.dot(weights, ranked_oof_preds)\n\n    curent_score = md.targets.validate_model(ensemble_oof_preds, 'Ensemble Model')\n\n    return curent_score  # 最大化が目標\n\n# Optunaによる最適化\nstudy = optuna.create_study(direction='maximize')  # AUCを最大化\nstudy.optimize(objective, n_trials=200)\n\n# 最適な重み\nprint(\"Best Weights:\", study.best_params)\nprint(\"Best AUC:\", study.best_value)\n\nCFG.weights = [1 for _ in range(len(ranked_oof_preds))]\nnp.array(CFG.weights)\n\nfor i in range(len(CFG.weights)):\n    CFG.weights[i] = study.best_params[f'weight_{i}']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_oof_preds = np.dot(CFG.weights, ranked_oof_preds)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"md.targets.validate_model(ensemble_oof_preds, 'Ensemble Model')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<p style=\"font-size: 125%; text-align: left; border-radius: 40px 40px;font-weight: bold;\">テストデータの予測</p>","metadata":{}},{"cell_type":"code","source":"ranked_preds = np.array([rankdata(p) for p in preds])","metadata":{"papermill":{"duration":0.023045,"end_time":"2024-12-13T16:07:55.510197","exception":false,"start_time":"2024-12-13T16:07:55.487152","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_preds = np.dot(CFG.weights, ranked_preds)","metadata":{"papermill":{"duration":0.020168,"end_time":"2024-12-13T16:07:55.542141","exception":false,"start_time":"2024-12-13T16:07:55.521973","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm_data = pd.read_csv(CFG.subm_path)\nsubm_data['prediction'] = ensemble_preds","metadata":{"papermill":{"duration":0.043145,"end_time":"2024-12-13T16:07:55.596856","exception":false,"start_time":"2024-12-13T16:07:55.553711","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm_data.to_csv('submission.csv', index=False)\ndisplay(subm_data.head())","metadata":{"papermill":{"duration":0.027273,"end_time":"2024-12-13T16:07:55.636582","exception":false,"start_time":"2024-12-13T16:07:55.609309","status":"completed"},"tags":[],"trusted":true,"execution":{"execution_failed":"2025-02-22T15:51:06.984Z"}},"outputs":[],"execution_count":null}]}