{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"},{"sourceId":10708371,"sourceType":"datasetVersion","datasetId":6636605},{"sourceId":211253469,"sourceType":"kernelVersion"},{"sourceId":211322530,"sourceType":"kernelVersion"},{"sourceId":219607918,"sourceType":"kernelVersion"},{"sourceId":221291379,"sourceType":"kernelVersion"}],"dockerImageVersionId":30840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Refactoring Pairwise Ranking Network\n\nThis notebook focuses on explaining what's behind each step of training the amazing network from [dreamingtree](https://www.kaggle.com/dreamingtree), some code refactoring and small improvements.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install -q /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install -q /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_lightning-2.4.0-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/torchmetrics-1.5.2-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabnet-4.1.0-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/einops-0.7.0-py3-none-any.whl\n!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabular-1.1.1-py2.py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source_file_path = '/kaggle/input/yunbase/Yunbase/baseline.py'\ntarget_file_path = '/kaggle/working/baseline.py'\nwith open(source_file_path, 'r', encoding='utf-8') as file:\n    content = file.read()\nwith open(target_file_path, 'w', encoding='utf-8') as file:\n    file.write(content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q --requirement /kaggle/input/yunbase/Yunbase/requirements.txt  \\\n--no-index --find-links file:/kaggle/input/yunbase/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from baseline import Yunbase\nimport pandas as pd#read csv,parquet\nimport numpy as np#for scientific computation of matrices\nfrom  lightgbm import LGBMRegressor,LGBMClassifier,log_evaluation,early_stopping\nfrom catboost import CatBoostRegressor,CatBoostClassifier\nfrom xgboost import XGBRegressor,XGBClassifier\nfrom lifelines import KaplanMeierFitter\nimport warnings#avoid some negligible errors\n#The filterwarnings () method is used to set warning filters, which can control the output method and level of warning information.\nwarnings.filterwarnings('ignore')\nimport random#provide some function to generate random_seed.\n#set random seed,to make sure model can be recurrented.\ndef seed_everything(seed):\n    np.random.seed(seed)#numpy's random seed\n    random.seed(seed)#python built-in random seed\nseed_everything(seed=2025)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\ntrain_solution=train[['ID','efs','efs_time','race_group']].copy()\n\ndef logit(p):\n    return np.log(p) - np.log(1 - p)\nmax_efs_time,min_efs_time=80,-100\ntrain['efs_time']=train['efs_time']/(max_efs_time-min_efs_time)\ntrain['efs_time']=train['efs_time'].apply(lambda x:logit(x))\ntrain['efs_time']+=10\nprint(train['efs_time'].max(),train['efs_time'].min())\n\nrace2weight={'American Indian or Alaska Native':0.68,\n'Asian':0.7,'Black or African-American':0.67,\n'More than one race':0.68,\n'Native Hawaiian or other Pacific Islander':0.66,\n'White':0.64}\ntrain['weight']=0.5*train['efs']+0.5\ntrain['raceweight']=train['race_group'].apply(lambda x:race2weight.get(x,1))\ntrain['weight']=train['weight']/train['raceweight']\ntrain.drop(['raceweight'],axis=1,inplace=True)\n\ntrain.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\ndef transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n\n    kmf = KaplanMeierFitter()\n    \n    kmf.fit(df[time_col], event_observed=df[event_col])\n    \n    survival_probabilities = kmf.survival_function_at_times(df[time_col]).values.flatten()\n\n    return survival_probabilities\n\nrace_group=sorted(train['race_group'].unique())\nfor race in race_group:\n    train.loc[train['race_group']==race,\"target\"] = transform_survival_probability(train[train['race_group']==race], time_col='efs_time', event_col='efs')\n    gap=0.7*(train.loc[(train['race_group']==race)&(train['efs']==0)]['target'].max()-train.loc[(train['race_group']==race)&(train['efs']==1)]['target'].min())/2\n    train.loc[(train['race_group']==race)&(train['efs']==0),'target']-=gap\n\nsns.histplot(data=train, x='target', hue='efs', element='step', stat='density', common_norm=False)\nplt.legend(title='efs')\nplt.title('Distribution of Target by EFS')\nplt.xlabel('Target')\nplt.ylabel('Density')\nplt.show()\n\ntrain.drop(['efs','efs_time'],axis=1,inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#nunique=2\nnunique2=[col for col in train.columns if train[col].nunique()==2 and col!='efs']\n#nunique<50\nnunique50=[col for col in train.columns if train[col].nunique()<50 and col not in ['efs','weight']]+['age_group','dri_score_NA']\n\ndef FE(df):\n    print(\"< deal with outlier >\")\n    df['nan_value_each_row'] = df.isnull().sum(axis=1)\n    #year_hct=2020 only 4 rows.\n    df['year_hct']=df['year_hct'].replace(2020,2019)\n    df['age_group']=df['age_at_hct']//10\n    #karnofsky_score 40 only 10 rows.\n    df['karnofsky_score']=df['karnofsky_score'].replace(40,50)\n    #hla_high_res_8=2 only 2 rows.\n    df['hla_high_res_8']=df['hla_high_res_8'].replace(2,3)\n    #hla_high_res_6=0 only 1 row.\n    df['hla_high_res_6']=df['hla_high_res_6'].replace(0,2)\n    #hla_high_res_10=3 only 1 row.\n    df['hla_high_res_10']=df['hla_high_res_10'].replace(3,4)\n    #hla_low_res_8=2 only 1 row.\n    df['hla_low_res_8']=df['hla_low_res_8'].replace(2,3)\n    df['dri_score']=df['dri_score'].replace('Missing disease status','N/A - disease not classifiable')\n    df['dri_score_NA']=df['dri_score'].apply(lambda x:int('N/A' in str(x)))\n    for col in ['diabetes','pulm_moderate','cardiac']:\n        df.loc[df[col].isna(),col]='Not done'\n\n    print(\"< cross feature >\")\n    df['donor_age-age_at_hct']=df['donor_age']-df['age_at_hct']\n    df['comorbidity_score+karnofsky_score']=df['comorbidity_score']+df['karnofsky_score']\n    df['comorbidity_score-karnofsky_score']=df['comorbidity_score']-df['karnofsky_score']\n    df['comorbidity_score*karnofsky_score']=df['comorbidity_score']*df['karnofsky_score']\n    df['comorbidity_score/karnofsky_score']=df['comorbidity_score']/df['karnofsky_score']\n    \n    print(\"< fillna >\")\n    df[nunique50]=df[nunique50].astype(str).fillna('NaN')\n    \n    print(\"< combine category feature >\")\n    for i in range(len(nunique2)):\n        for j in range(i+1,len(nunique2)):\n            df[nunique2[i]+nunique2[j]]=df[nunique2[i]].astype(str)+df[nunique2[j]].astype(str)\n    \n    print(\"< drop useless columns >\")\n    df.drop(['ID'],axis=1,inplace=True,errors='ignore')\n    return df\n\ncombine_category_cols=[]\nfor i in range(len(nunique2)):\n    for j in range(i+1,len(nunique2)):\n        combine_category_cols.append(nunique2[i]+nunique2[j])  \n\ntotal_category_feature=nunique50+combine_category_cols\n\ntarget_stat=[]\nfor j in range(len(total_category_feature)):\n   for col in ['donor_age','age_at_hct','target']:\n    target_stat.append( (total_category_feature[j],col,['count','mean','max','std','skew']) )\n\nnum_folds=10\n\nlgb_params={\"boosting_type\": \"gbdt\",\"metric\": 'mae',\n            'random_state': 2025,  \"max_depth\": 9,\"learning_rate\": 0.1,\n            \"n_estimators\": 768,\"colsample_bytree\": 0.6,\"colsample_bynode\": 0.6,\n            \"verbose\": -1,\"reg_alpha\": 0.2,\n            \"reg_lambda\": 5,\"extra_trees\":True,'num_leaves':64,\"max_bin\":255,\n            'importance_type': 'gain',#better than 'split'\n            'device':'gpu','gpu_use_dp':True\n           }\n\ncat_params={'random_state':2025,'eval_metric' : 'MAE',\n            'bagging_temperature': 0.50,'iterations': 650,\n            'learning_rate': 0.1,'max_depth': 8,\n            'l2_leaf_reg': 1.25,'min_data_in_leaf': 24,\n            'random_strength' : 0.25, 'verbose': 0,\n            'task_type':'GPU',\n            }\nxgb_params={'random_state': 2025, 'n_estimators': 256, \n            'learning_rate': 0.1, 'max_depth': 6,\n            'reg_alpha': 0.08, 'reg_lambda': 0.8, \n            'subsample': 0.95, 'colsample_bytree': 0.6, \n            'min_child_weight': 3,'early_stopping_rounds':1024,\n             'enable_categorical':True,'tree_method':'gpu_hist'\n            }\n\nyunbase=Yunbase(num_folds=num_folds,\n                  models=[(LGBMRegressor(**lgb_params),'lgb'),\n                          (CatBoostRegressor(**cat_params),'cat'),\n                          (XGBRegressor(**xgb_params),'xgb')\n                         ],\n                  FE=FE,\n                  seed=2025,\n                  objective='regression',\n                  metric='mae',\n                  target_col='target',\n                  device='gpu',\n                  one_hot_max=-1,\n                  early_stop=1000,\n                  cross_cols=['donor_age','age_at_hct'],\n                  target_stat=target_stat,\n                  use_data_augmentation=True,\n                  use_scaler=True,\n                  log=250,\n                  plot_feature_importance=True,\n                  #print metric score when model training\n                  use_eval_metric=False,\n)\nyunbase.fit(train,category_cols=nunique2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas.api.types\nfrom lifelines.utils import concordance_index\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \n    del solution[row_id_column_name]\n    del submission[row_id_column_name]\n    \n    event_label = 'efs'\n    interval_label = 'efs_time'\n    prediction_label = 'prediction'\n    for col in submission.columns:\n        if not pandas.api.types.is_numeric_dtype(submission[col]):\n            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n    # Merging solution and submission dfs on ID\n    merged_df = pd.concat([solution, submission], axis=1)\n    merged_df.reset_index(inplace=True)\n    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n    metric_list = []\n    for race in merged_df_race_dict.keys():\n        # Retrieving values from y_test based on index\n        indices = sorted(merged_df_race_dict[race])\n        merged_df_race = merged_df.iloc[indices]\n        # Calculate the concordance index\n        c_index_race = concordance_index(\n                        merged_df_race[interval_label],\n                        -merged_df_race[prediction_label],\n                        merged_df_race[event_label])\n        metric_list.append(c_index_race)\n    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))\n\nweights = [0.4,0.2,0.4]\n\nlgb_prediction=np.load(f\"Yunbase_info/lgb_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\nlgb_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':lgb_prediction})\nprint(f\"lgb_score:{score(train_solution.copy(),lgb_prediction.copy(),row_id_column_name='ID')}\")\nxgb_prediction=np.load(f\"Yunbase_info/xgb_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\nxgb_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':xgb_prediction})\nprint(f\"xgb_score:{score(train_solution.copy(),xgb_prediction.copy(),row_id_column_name='ID')}\")\ncat_prediction=np.load(f\"Yunbase_info/cat_seed{yunbase.seed}_repeat0_fold{yunbase.num_folds}_{yunbase.target_col}.npy\")\ncat_prediction=pd.DataFrame({'ID':train_solution['ID'],'prediction':cat_prediction})\nprint(f\"cat_score:{score(train_solution.copy(),cat_prediction.copy(),row_id_column_name='ID')}\")\n\ny_preds=[lgb_prediction.copy(),xgb_prediction.copy(),cat_prediction.copy()]\nfinal_prediction=lgb_prediction.copy()\nfinal_prediction['prediction']=0\nfor i in range(len(y_preds)):\n    final_prediction['prediction']+=weights[i]*y_preds[i]['prediction']\nmetric=score(train_solution.copy(),final_prediction.copy(),row_id_column_name='ID')\nprint(f\"final_CV:{metric}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds=yunbase.predict(test,weights=weights)\nyunbase.target_col='prediction'\nyunbase.submit(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\",test_preds,\n               save_name='submission'\n              )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mv submission.csv submission2.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare data\n\nBelow are a few utility functions to load and prepare the data for training with pytorch.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom torch.utils.data import TensorDataset\n\n\ndef get_X_cat(df, cat_cols, transformers=None):\n    \"\"\"\n    Apply a specific categorical data transformer or a LabelEncoder if None.\n    \"\"\"\n    if transformers is None:\n        transformers = [LabelEncoder().fit(df[col]) for col in cat_cols]\n    return transformers, np.array(\n        [transformer.transform(df[col]) for col, transformer in zip(cat_cols, transformers)]\n    ).T\n\n\ndef preprocess_data(train, val):\n    \"\"\"\n    Standardize numerical variables and transform (Label-encode) categoricals.\n    Fill NA values with mean for numerical.\n    Create torch dataloaders to prepare data for training and evaluation.\n    \"\"\"\n    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(train, val)\n    scaler = StandardScaler()\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n    X_num_train = imp.fit_transform(train[numerical])\n    X_num_train = scaler.fit_transform(X_num_train)\n    X_num_val = imp.transform(val[numerical])\n    X_num_val = scaler.transform(X_num_val)\n    dl_train = init_dl(X_cat_train, X_num_train, train, training=True)\n    dl_val = init_dl(X_cat_val, X_num_val, val)\n    return X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n\n\ndef get_categoricals(train, val):\n    \"\"\"\n    Remove constant categorical columns and transform them using LabelEncoder.\n    Return the label-transformers for each categorical column, categorical dataframes and numerical columns.\n    \"\"\"\n    categorical_cols, numerical = get_feature_types(train)\n    remove = []\n    for col in categorical_cols:\n        if train[col].nunique() == 1:\n            remove.append(col)\n        ind = ~val[col].isin(train[col])\n        if ind.any():\n            val.loc[ind, col] = np.nan\n    categorical_cols = [col for col in categorical_cols if col not in remove]\n    transformers, X_cat_train = get_X_cat(train, categorical_cols)\n    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n    return X_cat_train, X_cat_val, numerical, transformers\n\n\ndef init_dl(X_cat, X_num, df, training=False):\n    \"\"\"\n    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n    Notice that efs_time is log-transformed.\n    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n    \"\"\"\n    ds_train = TensorDataset(\n        torch.tensor(X_cat, dtype=torch.long),\n        torch.tensor(X_num, dtype=torch.float32),\n        torch.tensor(df.efs_time.values, dtype=torch.float32).log(),\n        torch.tensor(df.efs.values, dtype=torch.long)\n    )\n    bs = 2048\n    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n    return dl_train\n\n\ndef get_feature_types(train):\n    \"\"\"\n    Utility function to return categorical and numerical column names.\n    \"\"\"\n    categorical_cols = [col for i, col in enumerate(train.columns) if ((train[col].dtype == \"object\") | (2 < train[col].nunique() < 25))]\n    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n    FEATURES = [c for c in train.columns if not c in RMV]\n    print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n    numerical = [i for i in FEATURES if i not in categorical_cols]\n    return categorical_cols, numerical\n\n\ndef add_features(df):\n    \"\"\"\n    Create some new features to help the model focus on specific patterns.\n    \"\"\"\n    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n    df['year_hct'] -= 2000\n    \n    return df\n\n\ndef load_data():\n    \"\"\"\n    Load data and add features.\n    \"\"\"\n    test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n    test = add_features(test)\n    print(\"Test shape:\", test.shape)\n    train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n    train = add_features(train)\n    print(\"Train shape:\", train.shape)\n    return test, train\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define models with pairwise ranking loss\n\nThe model is defined in 3 steps :\n* Embedding class for categorical data\n* MLP for numerical and categorical data\n* Final model trained with pairwise ranking loss with selection of valid pairs","metadata":{}},{"cell_type":"code","source":"import functools\nfrom typing import List\n\nimport pytorch_lightning as pl\nimport numpy as np\nimport torch\nfrom lifelines.utils import concordance_index\nfrom pytorch_lightning.cli import ReduceLROnPlateau\nfrom pytorch_tabular.models.common.layers import ODST\nfrom torch import nn\nfrom pytorch_lightning.utilities import grad_norm\n\n\nclass CatEmbeddings(nn.Module):\n    \"\"\"\n    Embedding module for the categorical dataframe.\n    \"\"\"\n    def __init__(\n        self,\n        projection_dim: int,\n        categorical_cardinality: List[int],\n        embedding_dim: int\n    ):\n        \"\"\"\n        projection_dim: The dimension of the final output after projecting the concatenated embeddings into a lower-dimensional space.\n        categorical_cardinality: A list where each element represents the number of unique categories (cardinality) in each categorical feature.\n        embedding_dim: The size of the embedding space for each categorical feature.\n        self.embeddings: list of embedding layers for each categorical feature.\n        self.projection: sequential neural network that goes from the embedding to the output projection dimension with GELU activation.\n        \"\"\"\n        super(CatEmbeddings, self).__init__()\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(cardinality, embedding_dim)\n            for cardinality in categorical_cardinality\n        ])\n        self.projection = nn.Sequential(\n            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n            nn.GELU(),\n            nn.Linear(projection_dim, projection_dim)\n        )\n\n    def forward(self, x_cat):\n        \"\"\"\n        Apply the projection on concatened embeddings that contains all categorical features.\n        \"\"\"\n        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n        x_cat = torch.cat(x_cat, dim=1)\n        return self.projection(x_cat)\n\n\nclass NN(nn.Module):\n    \"\"\"\n    Train a model on both categorical embeddings and numerical data.\n    \"\"\"\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            dropout: float = 0\n    ):\n        \"\"\"\n        continuous_dim: The number of continuous features.\n        categorical_cardinality: A list of integers representing the number of unique categories in each categorical feature.\n        embedding_dim: The dimensionality of the embedding space for each categorical feature.\n        projection_dim: The size of the projected output space for the categorical embeddings.\n        hidden_dim: The number of neurons in the hidden layer of the MLP.\n        dropout: The dropout rate applied in the network.\n        self.embeddings: previous embeddings for categorical data.\n        self.mlp: defines an MLP model with an ODST layer followed by batch normalization and dropout.\n        self.out: linear output layer that maps the output of the MLP to a single value\n        self.dropout: defines dropout\n        Weights initialization with xavier normal algorithm and biases with zeros.\n        \"\"\"\n        super(NN, self).__init__()\n        self.embeddings = CatEmbeddings(projection_dim, categorical_cardinality, embedding_dim)\n        self.mlp = nn.Sequential(\n            ODST(projection_dim + continuous_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(dropout)\n        )\n        self.out = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n\n        # initialize weights\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x_cat, x_cont):\n        \"\"\"\n        Create embedding layers for categorical data, concatenate with continous variables.\n        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n        \"\"\"\n        x = self.embeddings(x_cat)\n        x = torch.cat([x, x_cont], dim=1)\n        x = self.dropout(x)\n        x = self.mlp(x)\n        return self.out(x), x\n\n\n@functools.lru_cache\ndef combinations(N):\n    \"\"\"\n    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1, \n    and caches the result using functools.lru_cache for optimization\n    \"\"\"\n    ind = torch.arange(N)\n    comb = torch.combinations(ind, r=2)\n    return comb.cuda()\n\n\nclass LitNN(pl.LightningModule):\n    \"\"\"\n    Main Model creation and losses definition to fully train the model.\n    \"\"\"\n    def __init__(\n            self,\n            continuous_dim: int,\n            categorical_cardinality: List[int],\n            embedding_dim: int,\n            projection_dim: int,\n            hidden_dim: int,\n            lr: float = 1e-3,\n            dropout: float = 0.2,\n            weight_decay: float = 1e-3,\n            aux_weight: float = 0.1,\n            margin: float = 0.5,\n            race_index: int = 0\n    ):\n        \"\"\"\n        continuous_dim: The number of continuous input features.\n        categorical_cardinality: A list of integers, where each element corresponds to the number of unique categories for each categorical feature.\n        embedding_dim: The dimension of the embeddings for the categorical features.\n        projection_dim: The dimension of the projected space after embedding concatenation.\n        hidden_dim: The size of the hidden layers in the feedforward network (MLP).\n        lr: The learning rate for the optimizer.\n        dropout: Dropout probability to avoid overfitting.\n        weight_decay: The L2 regularization term for the optimizer.\n        aux_weight: Weight used for auxiliary tasks.\n        margin: Margin used in some loss functions.\n        race_index: An index that refer to race_group in the input data.\n        \"\"\"\n        super(LitNN, self).__init__()\n        self.save_hyperparameters()\n\n        # Creates an instance of the NN model defined above\n        self.model = NN(\n            continuous_dim=self.hparams.continuous_dim,\n            categorical_cardinality=self.hparams.categorical_cardinality,\n            embedding_dim=self.hparams.embedding_dim,\n            projection_dim=self.hparams.projection_dim,\n            hidden_dim=self.hparams.hidden_dim,\n            dropout=self.hparams.dropout\n        )\n        self.targets = []\n\n        # Defines a small feedforward neural network that performs an auxiliary task with 1-dimensional output\n        self.aux_cls = nn.Sequential(\n            nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim // 3),\n            nn.GELU(),\n            nn.Linear(self.hparams.hidden_dim // 3, 1)\n        )\n\n    def on_before_optimizer_step(self, optimizer):\n        \"\"\"\n        Compute the 2-norm for each layer\n        If using mixed precision, the gradients are already unscaled here\n        \"\"\"\n        norms = grad_norm(self.model, norm_type=2)\n        self.log_dict(norms)\n\n    def forward(self, x_cat, x_cont):\n        \"\"\"\n        Forward pass that outputs the 1-dimensional prediction and the embeddings (raw output)\n        \"\"\"\n        x, emb = self.model(x_cat, x_cont)\n        return x.squeeze(1), emb\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        defines how the model processes each batch of data during training.\n        A batch is a combination of : categorical data, continuous data, efs_time (y) and efs event.\n        y_hat is the efs_time prediction on all data and aux_pred is auxiliary prediction on embeddings.\n        Calculates loss and race_group loss on full data.\n        Auxiliary loss is calculated with an event mask, ignoring efs=0 predictions and taking the average.\n        Returns loss and aux_loss multiplied by weight defined above.\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        aux_pred = self.aux_cls(emb).squeeze(1)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        aux_loss = nn.functional.mse_loss(aux_pred, y, reduction='none')\n        aux_mask = efs == 1\n        aux_loss = (aux_loss * aux_mask).sum() / aux_mask.sum()\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"race_loss\", race_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        self.log(\"aux_loss\", aux_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n        return loss + aux_loss * self.hparams.aux_weight\n\n    def get_full_loss(self, efs, x_cat, y, y_hat):\n        \"\"\"\n        Output loss and race_group loss.\n        \"\"\"\n        loss = self.calc_loss(y, y_hat, efs)\n        race_loss = self.get_race_losses(efs, x_cat, y, y_hat)\n        loss += 0.1 * race_loss\n        return loss, race_loss\n\n    def get_race_losses(self, efs, x_cat, y, y_hat):\n        \"\"\"\n        Calculate loss for each race_group based on deviation/variance.\n        \"\"\"\n        races = torch.unique(x_cat[:, self.hparams.race_index])\n        race_losses = []\n        for race in races:\n            ind = x_cat[:, self.hparams.race_index] == race\n            race_losses.append(self.calc_loss(y[ind], y_hat[ind], efs[ind]))\n        race_loss = sum(race_losses) / len(race_losses)\n        races_loss_std = sum((r - race_loss)**2 for r in race_losses) / len(race_losses)\n        return torch.sqrt(races_loss_std)\n\n    def calc_loss(self, y, y_hat, efs):\n        \"\"\"\n        Most important part of the model : loss function used for training.\n        We face survival data with event indicators along with time-to-event.\n\n        This function computes the main loss by the following the steps :\n        * create all data pairs with \"combinations\" function (= all \"two subjects\" combinations)\n        * make sure that we have at least 1 event in each pair\n        * convert y to +1 or -1 depending on the correct ranking\n        * loss is computed using a margin-based hinge loss\n        * mask is applied to ensure only valid pairs are being used (censored data can't be ranked with event in some cases)\n        * average loss on all pairs is returned\n        \"\"\"\n        N = y.shape[0]\n        comb = combinations(N)\n        comb = comb[(efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)]\n        pred_left = y_hat[comb[:, 0]]\n        pred_right = y_hat[comb[:, 1]]\n        y_left = y[comb[:, 0]]\n        y_right = y[comb[:, 1]]\n        y = 2 * (y_left > y_right).int() - 1\n        loss = nn.functional.relu(-y * (pred_left - pred_right) + self.hparams.margin)\n        mask = self.get_mask(comb, efs, y_left, y_right)\n        loss = (loss.double() * (mask.double())).sum() / mask.sum()\n        return loss\n\n    def get_mask(self, comb, efs, y_left, y_right):\n        \"\"\"\n        Defines all invalid comparisons :\n        * Case 1: \"Left outlived Right\" but Right is censored\n        * Case 2: \"Right outlived Left\" but Left is censored\n        Masks for case 1 and case 2 are combined using |= operator and inverted using ~ to create a \"valid pair mask\"\n        \"\"\"\n        left_outlived = y_left >= y_right\n        left_1_right_0 = (efs[comb[:, 0]] == 1) & (efs[comb[:, 1]] == 0)\n        mask2 = (left_outlived & left_1_right_0)\n        right_outlived = y_right >= y_left\n        right_1_left_0 = (efs[comb[:, 1]] == 1) & (efs[comb[:, 0]] == 0)\n        mask2 |= (right_outlived & right_1_left_0)\n        mask2 = ~mask2\n        mask = mask2\n        return mask\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        This method defines how the model processes each batch during validation\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"\n        At the end of the validation epoch, it computes and logs the concordance index\n        \"\"\"\n        cindex, metric = self._calc_cindex()\n        self.log(\"cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n    def _calc_cindex(self):\n        \"\"\"\n        Calculate c-index accounting for each race_group or global.\n        \"\"\"\n        y = torch.cat([t[0] for t in self.targets]).cpu().numpy()\n        y_hat = torch.cat([t[1] for t in self.targets]).cpu().numpy()\n        efs = torch.cat([t[2] for t in self.targets]).cpu().numpy()\n        races = torch.cat([t[3] for t in self.targets]).cpu().numpy()\n        metric = self._metric(efs, races, y, y_hat)\n        cindex = concordance_index(y, y_hat, efs)\n        return cindex, metric\n\n    def _metric(self, efs, races, y, y_hat):\n        \"\"\"\n        Calculate c-index accounting for each race_group\n        \"\"\"\n        metric_list = []\n        for race in np.unique(races):\n            y_ = y[races == race]\n            y_hat_ = y_hat[races == race]\n            efs_ = efs[races == race]\n            metric_list.append(concordance_index(y_, y_hat_, efs_))\n        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n        return metric\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"\n        Same as training step but to log test data\n        \"\"\"\n        x_cat, x_cont, y, efs = batch\n        y_hat, emb = self(x_cat, x_cont)\n        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def on_test_epoch_end(self) -> None:\n        \"\"\"\n        At the end of the test epoch, calculates and logs the concordance index for the test set\n        \"\"\"\n        cindex, metric = self._calc_cindex()\n        self.log(\"test_cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"test_cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n        self.targets.clear()\n\n\n    def configure_optimizers(self):\n        \"\"\"\n        configures the optimizer and learning rate scheduler:\n        * Optimizer: Adam optimizer with weight decay (L2 regularization).\n        * Scheduler: Cosine Annealing scheduler, which adjusts the learning rate according to a cosine curve.\n        \"\"\"\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n        scheduler_config = {\n            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer,\n                T_max=45,\n                eta_min=6e-3\n            ),\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n            \"strict\": False,\n        }\n\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pytorch_lightning as pl\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\nfrom pytorch_lightning.callbacks import StochasticWeightAveraging\nfrom sklearn.model_selection import StratifiedKFold\n\npl.seed_everything(3027)\n\ndef main(hparams):\n    \"\"\"\n    Main function to train the model.\n    The steps are as following :\n    * load data and fill efs and efs time for test data with 1\n    * initialize pred array with 0\n    * get categorical and numerical columns\n    * split the train data on the stratified criterion : race_group * newborns yes/no\n    * preprocess the fold data (create dataloaders)\n    * train the model and create final submission output\n    \"\"\"\n    test, train_original = load_data()\n    test['efs_time'] = 1\n    test['efs'] = 1\n    test_pred = np.zeros(test.shape[0])\n    categorical_cols, numerical = get_feature_types(train_original)\n    kf = StratifiedKFold(n_splits=5, shuffle=True, )\n    for i, (train_index, test_index) in enumerate(\n        kf.split(\n            train_original, train_original.race_group.astype(str) + (train_original.age_at_hct == 0.044).astype(str)\n        )\n    ):\n        tt = train_original.copy()\n        train = tt.iloc[train_index]\n        val = tt.iloc[test_index]\n        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, val)\n        model = train_final(X_num_train, dl_train, dl_val, transformers, categorical_cols=categorical_cols)\n        # Create submission\n        train = tt.iloc[train_index]\n        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, test)\n        pred, _ = model.cuda().eval()(\n            torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n            torch.tensor(X_num_val, dtype=torch.float32).cuda()\n        )\n        test_pred += pred.detach().cpu().numpy()\n        \n    subm_data = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n    subm_data['prediction'] = -test_pred\n    subm_data.to_csv('submission.csv', index=False)\n    \n    display(subm_data.head())\n    return \n\n\n\ndef train_final(X_num_train, dl_train, dl_val, transformers, hparams=None, categorical_cols=None):\n    \"\"\"\n    Defines model hyperparameters and fit the model.\n    \"\"\"\n    if hparams is None:\n        hparams = {\n            \"embedding_dim\": 16,\n            \"projection_dim\": 112,\n            \"hidden_dim\": 56,\n            \"lr\": 0.06464861983337984,\n            \"dropout\": 0.05463240181423116,\n            \"aux_weight\": 0.26545778308743806,\n            \"margin\": 0.2588153271003354,\n            \"weight_decay\": 0.0002773544957610778\n        }\n    model = LitNN(\n        continuous_dim=X_num_train.shape[1],\n        categorical_cardinality=[len(t.classes_) for t in transformers],\n        race_index=categorical_cols.index(\"race_group\"),\n        **hparams\n    )\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1)\n    trainer = pl.Trainer(\n        accelerator='cuda',\n        max_epochs=60,\n        callbacks=[\n            checkpoint_callback,\n            LearningRateMonitor(logging_interval='epoch'),\n            TQDMProgressBar(),\n            StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n        ],\n    )\n    trainer.fit(model, dl_train)\n    trainer.test(model, dl_val)\n    return model.eval()\n\n\nhparams = None\nres = main(hparams)\nprint(\"done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# HLA : Human Leukocyte Antigen matching levels.\n# Homozygous chromosomes have the same allele at a given locus (fixed position on a chromosome where a particular gene is located).\n# Those that have different alleles at a given locus are called heterozygous.\n# Values Explanation\n# 0 - No Match: Neither of the donor's two HLA antigens/alleles matches the recipient's HLA.\n# This indicates a complete mismatch at the locus, which increases the risk of complications like graft rejection or graft-versus-host disease (GVHD).\n# 1 - Partial Match: One of the donor's HLA antigens/alleles matches one of the recipient's.\n# This represents a half-match (heterozygous compatibility) at the locus. It's better than a full mismatch but still carries a moderate risk of immune complications.\n# 2 - Full Match: Both of the donor's HLA match both of the recipient's HLA.\n# This is the optimal scenario, indicating full compatibility at the locus and minimizing the risk of immune complications.\n\n# High VS Low resolution\n\n# High-Resolution Typing: Identifies specific alleles (e.g., HLA-A*02:01).\n# Provides the most precise match and is essential for unrelated donor transplants.\n\n# Low-Resolution Typing: Identifies broader antigen groups (e.g., HLA-A2).\n# May suffice for related donor transplants where genetic similarity is inherently higher.\n\nHLA_COLUMNS = [\n    # MHC class I molecules are one of two primary classes of major histocompatibility complex (MHC) molecules and are found on the cell surface of all nucleated cells.\n    # In humans, the HLAs corresponding to MHC class I are HLA-A, HLA-B, and HLA-C.\n    'hla_match_a_low', 'hla_match_a_high',\n    'hla_match_b_low', 'hla_match_b_high',\n    'hla_match_c_low', 'hla_match_c_high',\n\n    # MHC Class II molecules are a class of major histocompatibility complex (MHC) molecules normally found only on professional antigen-presenting cells \n    # such as dendritic cells, macrophages, some endothelial cells, thymic epithelial cells, and B cells.\n    # Antigens presented by MHC class II molecules are exogenous, originating from extracellular proteins rather than cytosolic and endogenous sources like \n    # those presented by MHC class I.\n    # HLAs corresponding to MHC class II are HLA-DP, HLA-DM, HLA-DOA, HLA-DOB, HLA-DQ, and HLA-DR.\n    # In this competition, we only have HLA-DR and HLA-DQ\n    \n    # Visit https://en.wikipedia.org/wiki/HLA-DQB1\n\n    'hla_match_dqb1_low', 'hla_match_dqb1_high',\n    'hla_match_drb1_low', 'hla_match_drb1_high',\n\n    # Combination of matches : sum of matches between multiple categories\n\n    # Matching at HLA-A(low), -B(low), -DRB1(high)\n    'hla_nmdp_6',\n    # Matching at HLA-A,-B,-DRB1 (low or high)\n    'hla_low_res_6', 'hla_high_res_6',\n    # Matching at HLA-A, -B, -C, -DRB1 (low or high)\n    'hla_low_res_8', 'hla_high_res_8',\n    # Matching at HLA-A, -B, -C, -DRB1, -DQB1 (low or high)\n    'hla_low_res_10', 'hla_high_res_10'\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport polars as pl\nimport pandas as pd\nimport plotly.colors as pc\nimport plotly.express as px\nimport plotly.graph_objects as go","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.io as pio\npio.renderers.default = 'iframe'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.options.display.max_columns = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lifelines import CoxPHFitter\nfrom lifelines import KaplanMeierFitter\nfrom lifelines import NelsonAalenFitter\nimport lightgbm as lgb\nfrom metric import score\nfrom scipy.stats import rankdata \nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n\n    train_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\n    test_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\n    subm_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv')\n\n    color = '#A2574F'\n\n    batch_size = 32768\n    early_stop = 300\n    penalizer = 0.01\n    n_splits = 5\n\n    weights = [1.0, 1.0, 8, 4.0, 7.5, 4.0, 6.0, 6.0]\n\n    ctb_params = {\n        'loss_function': 'RMSE',\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'CPU',\n        'num_trees': 6000,\n        'subsample': 0.85,\n        'reg_lambda': 8.0,\n        'depth': 8\n    }\n\n    lgb_params = {\n        'objective': 'regression',\n        'min_child_samples': 32,\n        'num_iterations': 6000,\n        'learning_rate': 0.03,\n        'extra_trees': True,\n        'reg_lambda': 8.0,\n        'reg_alpha': 0.1,\n        'num_leaves': 64,\n        'metric': 'rmse',\n        'max_depth': 8,\n        'device': 'cpu',\n        'max_bin': 128,\n        'verbose': -1,\n        'seed': 42\n    }\n\n    # Parameters for the first CatBoost model with Cox loss function\n    cox1_params = {\n        'grow_policy': 'Depthwise',\n        'min_child_samples': 8,\n        'loss_function': 'Cox',\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'CPU',\n        'num_trees': 6000,\n        'subsample': 0.85,\n        'reg_lambda': 8.0,\n        'depth': 8\n    }\n\n    # Parameters for the second CatBoost model with Cox loss function\n    cox2_params = {\n        'grow_policy': 'Lossguide',\n        'loss_function': 'Cox',\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'CPU',\n        'num_trees': 6000,\n        'subsample': 0.85,\n        'reg_lambda': 8.0,\n        'num_leaves': 32,\n        'depth': 8\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FE:\n\n    def __init__(self, batch_size):\n        self._batch_size = batch_size\n\n    def load_data(self, path):\n\n        return pl.read_csv(path, batch_size=self._batch_size)\n\n    def recalculate_hla_sums(self, df):\n        \n        df = df.with_columns(\n            (pl.col(\"hla_match_a_low\").fill_null(0) + pl.col(\"hla_match_b_low\").fill_null(0) + \n             pl.col(\"hla_match_drb1_high\").fill_null(0)).alias(\"hla_nmdp_6\"),\n            \n            (pl.col(\"hla_match_a_low\").fill_null(0) + pl.col(\"hla_match_b_low\").fill_null(0) + \n             pl.col(\"hla_match_drb1_low\").fill_null(0)).alias(\"hla_low_res_6\"),\n            \n            (pl.col(\"hla_match_a_high\").fill_null(0) + pl.col(\"hla_match_b_high\").fill_null(0) + \n             pl.col(\"hla_match_drb1_high\").fill_null(0)).alias(\"hla_high_res_6\"),\n            \n            (pl.col(\"hla_match_a_low\").fill_null(0) + pl.col(\"hla_match_b_low\").fill_null(0) + \n             pl.col(\"hla_match_c_low\").fill_null(0) + pl.col(\"hla_match_drb1_low\").fill_null(0)\n            ).alias(\"hla_low_res_8\"),\n            \n            (pl.col(\"hla_match_a_high\").fill_null(0) + pl.col(\"hla_match_b_high\").fill_null(0) + \n             pl.col(\"hla_match_c_high\").fill_null(0) + pl.col(\"hla_match_drb1_high\").fill_null(0)\n            ).alias(\"hla_high_res_8\"),\n            \n            (pl.col(\"hla_match_a_low\").fill_null(0) + pl.col(\"hla_match_b_low\").fill_null(0) + \n             pl.col(\"hla_match_c_low\").fill_null(0) + pl.col(\"hla_match_drb1_low\").fill_null(0) +\n             pl.col(\"hla_match_dqb1_low\").fill_null(0)).alias(\"hla_low_res_10\"),\n            \n            (pl.col(\"hla_match_a_high\").fill_null(0) + pl.col(\"hla_match_b_high\").fill_null(0) + \n             pl.col(\"hla_match_c_high\").fill_null(0) + pl.col(\"hla_match_drb1_high\").fill_null(0) +\n             pl.col(\"hla_match_dqb1_high\").fill_null(0)).alias(\"hla_high_res_10\"),\n        )\n\n        return df\n\n    def cast_datatypes(self, df):\n\n        num_cols = [\n            'hla_high_res_8',\n            'hla_low_res_8',\n            'hla_high_res_6',\n            'hla_low_res_6',\n            'hla_high_res_10',\n            'hla_low_res_10',\n            'hla_match_dqb1_high',\n            'hla_match_dqb1_low',\n            'hla_match_drb1_high',\n            'hla_match_drb1_low',\n            'hla_nmdp_6',\n            'year_hct',\n            'hla_match_a_high',\n            'hla_match_a_low',\n            'hla_match_b_high',\n            'hla_match_b_low',\n            'hla_match_c_high',\n            'hla_match_c_low',\n            'donor_age',\n            'age_at_hct',\n            'comorbidity_score',\n            'karnofsky_score',\n            'efs',\n            'efs_time'\n        ]\n\n        for col in df.columns:\n\n            if col in num_cols:\n                df = df.with_columns(pl.col(col).fill_null(-1).cast(pl.Float32))  \n\n            else:\n                df = df.with_columns(pl.col(col).fill_null('Unknown').cast(pl.String))  \n\n        return df.with_columns(pl.col('ID').cast(pl.Int32))\n\n    def info(self, df):\n        \n        print(f'\\nShape of dataframe: {df.shape}') \n        \n        mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage: {:.2f} MB\\n'.format(mem))\n\n        display(df.head())\n\n    def apply_fe(self, path):\n\n        df = self.load_data(path)\n        df = self.recalculate_hla_sums(df)\n        df = self.cast_datatypes(df)\n        df = df.to_pandas()\n\n        self.info(df)\n        \n        cat_cols = [col for col in df.columns if df[col].dtype == pl.String]\n\n        return df, cat_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fe = FE(CFG.batch_size)\ntrain_data, cat_cols = fe.apply_fe(CFG.train_path)\ntest_data, _ = fe.apply_fe(CFG.test_path)\nclass EDA:\n    \n    def __init__(self, color, data):\n        self._color = color  \n        self.data = data\n\n    def _template(self, fig, title):\n        \n        fig.update_layout(\n            title=title,\n            title_x=0.5, \n            plot_bgcolor='rgba(247, 230, 202, 1)',  \n            paper_bgcolor='rgba(247, 230, 202, 1)', \n            font=dict(color=self._color),\n            margin=dict(l=72, r=72, t=72, b=72), \n            height=720\n        )\n        \n        return fig\n\n    def distribution_plot(self, col, title):\n        \n        fig = px.histogram(\n            self.data,\n            x=col,\n            nbins=100,\n            color_discrete_sequence=[self._color]\n        )\n        \n        fig.update_layout(\n            xaxis_title='Values',\n            yaxis_title='Count',\n            bargap=0.1,\n            xaxis=dict(gridcolor='grey'),\n            yaxis=dict(gridcolor='grey', zerolinecolor='grey')\n        )\n        \n        fig.update_traces(hovertemplate='Value: %{x:.2f}<br>Count: %{y:,}')\n        \n        fig = self._template(fig, f'{title}')\n        fig.show()\n        \n    def _plot_cv(self, scores, title, metric='Stratified C-Index'):\n        \n        fold_scores = [round(score, 3) for score in scores]\n        mean_score = round(np.mean(scores), 3)\n\n        fig = go.Figure()\n\n        fig.add_trace(go.Scatter(\n            x = list(range(1, len(fold_scores) + 1)),\n            y = fold_scores,\n            mode = 'markers', \n            name = 'Fold Scores',\n            marker = dict(size = 27, color=self._color, symbol='diamond'),\n            text = [f'{score:.3f}' for score in fold_scores],\n            hovertemplate = 'Fold %{x}: %{text}<extra></extra>',\n            hoverlabel = dict(font=dict(size=18))  \n        ))\n\n        fig.add_trace(go.Scatter(\n            x = [1, len(fold_scores)],\n            y = [mean_score, mean_score],\n            mode = 'lines',\n            name = f'Mean: {mean_score:.3f}',\n            line = dict(dash = 'dash', color = '#B22222'),\n            hoverinfo = 'none'\n        ))\n        \n        fig.update_layout(\n            title = f'{title} | Cross-validation Mean {metric} Score: {mean_score}',\n            xaxis_title = 'Fold',\n            yaxis_title = f'{metric} Score',\n            plot_bgcolor = 'rgba(247, 230, 202, 1)',  \n            paper_bgcolor = 'rgba(247, 230, 202, 1)',\n            font = dict(color=self._color), \n            xaxis = dict(\n                gridcolor = 'grey',\n                tickmode = 'linear',\n                tick0 = 1,\n                dtick = 1,\n                range = [0.5, len(fold_scores) + 0.5],\n                zerolinecolor = 'grey'\n            ),\n            yaxis = dict(\n                gridcolor = 'grey',\n                zerolinecolor = 'grey'\n            )\n        )\n        \n        fig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Targets:\n\n    def __init__(self, data, cat_cols, penalizer, n_splits):\n        \n        self.data = data\n        self.cat_cols = cat_cols\n        \n        self._length = len(self.data)\n        self._penalizer = penalizer\n        self._n_splits = n_splits\n\n    def _prepare_cv(self):\n        \n        oof_preds = np.zeros(self._length)\n            \n        cv = KFold(n_splits=self._n_splits, shuffle=True, random_state=42)\n\n        return cv, oof_preds\n\n    def validate_model(self, preds, title):\n            \n        y_true = self.data[['ID', 'efs', 'efs_time', 'race_group']].copy()\n        y_pred = self.data[['ID']].copy()\n        \n        y_pred['prediction'] = preds\n            \n        c_index_score = score(y_true.copy(), y_pred.copy(), 'ID')\n        print(f'Overall Stratified C-Index Score for {title}: {c_index_score:.4f}')\n\n    def create_target1(self):  \n\n        '''\n        Inside the CV loop, constant columns are dropped if they exist in a fold. Otherwise, the code produces error:\n\n        delta contains nan value(s). Convergence halted. Please see the following tips in the lifelines documentation: \n        https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n        '''\n\n        cv, oof_preds = self._prepare_cv()\n\n        # Apply one hot encoding to categorical columns\n        data = pd.get_dummies(self.data, columns=self.cat_cols, drop_first=True).drop('ID', axis=1) \n\n        for train_index, valid_index in cv.split(data):\n\n            train_data = data.iloc[train_index]\n            valid_data = data.iloc[valid_index]\n\n            # Drop constant columns if they exist\n            train_data = train_data.loc[:, train_data.nunique() > 1]\n            valid_data = valid_data[train_data.columns]\n\n            cph = CoxPHFitter(penalizer=self._penalizer)\n            cph.fit(train_data, duration_col='efs_time', event_col='efs')\n            \n            oof_preds[valid_index] = cph.predict_partial_hazard(valid_data)              \n\n        self.data['target1'] = oof_preds \n        self.validate_model(oof_preds, 'Cox') \n\n        return self.data\n\n    def create_target2(self):        \n\n        cv, oof_preds = self._prepare_cv()\n\n        for train_index, valid_index in cv.split(self.data):\n\n            train_data = self.data.iloc[train_index]\n            valid_data = self.data.iloc[valid_index]\n\n            kmf = KaplanMeierFitter()\n            kmf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n            \n            oof_preds[valid_index] = kmf.survival_function_at_times(valid_data['efs_time']).values\n\n        self.data['target2'] = oof_preds  \n        self.validate_model(oof_preds, 'Kaplan-Meier')\n\n        return self.data\n\n    def create_target3(self):        \n\n        cv, oof_preds = self._prepare_cv()\n\n        for train_index, valid_index in cv.split(self.data):\n\n            train_data = self.data.iloc[train_index]\n            valid_data = self.data.iloc[valid_index]\n            \n            naf = NelsonAalenFitter()\n            naf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n            \n            oof_preds[valid_index] = -naf.cumulative_hazard_at_times(valid_data['efs_time']).values\n\n        self.data['target3'] = oof_preds  \n        self.validate_model(oof_preds, 'Nelson-Aalen')\n\n        return self.data\n\n    def create_target4(self):\n\n        self.data['target4'] = self.data.efs_time.copy()\n        self.data.loc[self.data.efs == 0, 'target4'] *= -1\n\n        return self.data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MD:\n    \n    def __init__(self, color, data, cat_cols, early_stop, penalizer, n_splits):\n        \n        self.eda = EDA(color, data)\n        self.targets = Targets(data, cat_cols, penalizer, n_splits)\n        \n        self.data = data\n        self.cat_cols = cat_cols\n        self._early_stop = early_stop\n\n    def create_targets(self):\n\n        self.data = self.targets.create_target1()\n        self.data = self.targets.create_target2()\n        self.data = self.targets.create_target3()\n        self.data = self.targets.create_target4()\n\n        return self.data\n        \n    def train_model(self, params, target, title):\n        \n        for col in self.cat_cols:\n            self.data[col] = self.data[col].astype('category')\n            \n        X = self.data.drop(['ID', 'efs', 'efs_time', 'target1', 'target2', 'target3', 'target4'], axis=1)\n        y = self.data[target]\n        \n        models, fold_scores = [], []\n            \n        cv, oof_preds = self.targets._prepare_cv()\n    \n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n                \n            X_train = X.iloc[train_index]\n            X_valid = X.iloc[valid_index]\n                \n            y_train = y.iloc[train_index]\n            y_valid = y.iloc[valid_index]\n    \n            if title.startswith('LightGBM'):\n                        \n                model = lgb.LGBMRegressor(**params)\n                        \n                model.fit(\n                    X_train, \n                    y_train,  \n                    eval_set=[(X_valid, y_valid)],\n                    eval_metric='rmse',\n                    callbacks=[lgb.early_stopping(self._early_stop, verbose=0), lgb.log_evaluation(0)]\n                )\n                        \n            elif title.startswith('CatBoost'):\n                        \n                model = CatBoostRegressor(**params, verbose=0, cat_features=self.cat_cols)\n                        \n                model.fit(\n                    X_train,\n                    y_train,\n                    eval_set=(X_valid, y_valid),\n                    early_stopping_rounds=self._early_stop, \n                    verbose=0\n                )               \n                    \n            models.append(model)\n                \n            oof_preds[valid_index] = model.predict(X_valid)\n\n            y_true_fold = self.data.iloc[valid_index][['ID', 'efs', 'efs_time', 'race_group']].copy()\n            y_pred_fold = self.data.iloc[valid_index][['ID']].copy()\n            \n            y_pred_fold['prediction'] = oof_preds[valid_index]\n    \n            fold_score = score(y_true_fold, y_pred_fold, 'ID')\n            fold_scores.append(fold_score)\n    \n        self.eda._plot_cv(fold_scores, title)\n        self.targets.validate_model(oof_preds, title)\n        \n        return models, oof_preds\n\n    def infer_model(self, data, models):\n        \n        data = data.drop(['ID'], axis=1)\n\n        for col in self.cat_cols:\n            data[col] = data[col].astype('category')\n\n        return np.mean([model.predict(data) for model in models], axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"md = MD(CFG.color, train_data, cat_cols, CFG.early_stop, CFG.penalizer, CFG.n_splits)\ntrain_data = md.create_targets()\nmd.eda.distribution_plot('target1', 'Cox Target')\nmd.eda.distribution_plot('target2', 'Kaplan-Meier Target')\nmd.eda.distribution_plot('target3', 'Nelson-Aalen Target')\nmd.eda.distribution_plot('target4', 'Target for Cox-Loss Models')\nfe.info(train_data)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ctb1_models, ctb1_oof_preds = md.train_model(CFG.ctb_params, target='target1', title='CatBoost')\nlgb1_models, lgb1_oof_preds = md.train_model(CFG.lgb_params, target='target1', title='LightGBM')\nctb1_preds = md.infer_model(test_data, ctb1_models)\nlgb1_preds = md.infer_model(test_data, lgb1_models)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ctb2_models, ctb2_oof_preds = md.train_model(CFG.ctb_params, target='target2', title='CatBoost')\nlgb2_models, lgb2_oof_preds = md.train_model(CFG.lgb_params, target='target2', title='LightGBM')\nctb2_preds = md.infer_model(test_data, ctb2_models)\nlgb2_preds = md.infer_model(test_data, lgb2_models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ctb3_models, ctb3_oof_preds = md.train_model(CFG.ctb_params, target='target3', title='CatBoost')\nlgb3_models, lgb3_oof_preds = md.train_model(CFG.lgb_params, target='target3', title='LightGBM')\nctb3_preds = md.infer_model(test_data, ctb3_models)\nlgb3_preds = md.infer_model(test_data, lgb3_models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cox1_models, cox1_oof_preds = md.train_model(CFG.cox1_params, target='target4', title='CatBoost')\ncox2_models, cox2_oof_preds = md.train_model(CFG.cox2_params, target='target4', title='CatBoost')\ncox1_preds = md.infer_model(test_data, cox1_models)\ncox2_preds = md.infer_model(test_data, cox2_models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oof_preds = [\n    ctb1_oof_preds, \n    lgb1_oof_preds, \n    ctb2_oof_preds, \n    lgb2_oof_preds, \n    ctb3_oof_preds, \n    lgb3_oof_preds, \n    cox1_oof_preds,\n    cox2_oof_preds\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ranked_oof_preds = np.array([rankdata(p) for p in oof_preds])\nensemble_oof_preds = np.dot(CFG.weights, ranked_oof_preds)\nmd.targets.validate_model(ensemble_oof_preds, 'Ensemble Model')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = [\n    ctb1_preds, \n    lgb1_preds, \n    ctb2_preds, \n    lgb2_preds, \n    ctb3_preds, \n    lgb3_preds,\n    cox1_preds,\n    cox2_preds\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ranked_preds = np.array([rankdata(p) for p in preds])\nensemble_preds = np.dot(CFG.weights, ranked_preds)\nsubm_data = pd.read_csv(CFG.subm_path)\nsubm_data['prediction'] = ensemble_preds\nsubm_data.to_csv('submission1.csv', index=False)\ndisplay(subm_data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !cp -r /kaggle/input/refactoring-nn-pairwise-ranking-loss11/submission.csv .\n# !cp -r /kaggle/input/refactoring-nn-pairwise-ranking-loss11/submission1.csv .\n!mv submission.csv submission0.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:25:06.557387Z","iopub.execute_input":"2025-02-10T08:25:06.557659Z","iopub.status.idle":"2025-02-10T08:25:06.899518Z","shell.execute_reply.started":"2025-02-10T08:25:06.557636Z","shell.execute_reply":"2025-02-10T08:25:06.898613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.stats import rankdata\n\n# #  CSV \n# sub1 = pd.read_csv('/kaggle/input/refactoring-nn-pairwise-ranking-loss11/submission.csv')\n# sub2 = pd.read_csv('/kaggle/input/refactoring-nn-pairwise-ranking-loss11/submission1.csv')\n \n#  CSV \nsub1 = pd.read_csv('submission0.csv')\nsub2 = pd.read_csv('submission1.csv') \nsub1 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:25:06.901008Z","iopub.execute_input":"2025-02-10T08:25:06.901334Z","iopub.status.idle":"2025-02-10T08:25:07.558823Z","shell.execute_reply.started":"2025-02-10T08:25:06.901306Z","shell.execute_reply":"2025-02-10T08:25:07.557946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:25:07.560526Z","iopub.execute_input":"2025-02-10T08:25:07.56078Z","iopub.status.idle":"2025-02-10T08:25:07.567617Z","shell.execute_reply.started":"2025-02-10T08:25:07.56076Z","shell.execute_reply":"2025-02-10T08:25:07.566839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n#  id  id \nmerged = pd.merge(sub1, sub2, on='ID', suffixes=('_sub1', '_sub2'))\n\n#  rankdata  prediction \nmerged['prediction'] = (rankdata(merged['prediction_sub1']) + rankdata(merged['prediction_sub2']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:25:07.568844Z","iopub.execute_input":"2025-02-10T08:25:07.569099Z","iopub.status.idle":"2025-02-10T08:25:07.587109Z","shell.execute_reply.started":"2025-02-10T08:25:07.569069Z","shell.execute_reply":"2025-02-10T08:25:07.586336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rankdata(merged['prediction_sub1'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:25:07.588017Z","iopub.execute_input":"2025-02-10T08:25:07.588231Z","iopub.status.idle":"2025-02-10T08:25:07.611429Z","shell.execute_reply.started":"2025-02-10T08:25:07.588213Z","shell.execute_reply":"2025-02-10T08:25:07.610676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rankdata(merged['prediction_sub2'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:25:07.61203Z","iopub.execute_input":"2025-02-10T08:25:07.612229Z","iopub.status.idle":"2025-02-10T08:25:07.626324Z","shell.execute_reply.started":"2025-02-10T08:25:07.612211Z","shell.execute_reply":"2025-02-10T08:25:07.625624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  id  prediction \nmerged = merged[['ID', 'prediction']]\n\n#  CSV \nmerged.to_csv('submission.csv', index=False)\nmerged","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:25:07.627043Z","iopub.execute_input":"2025-02-10T08:25:07.627307Z","iopub.status.idle":"2025-02-10T08:25:07.646031Z","shell.execute_reply.started":"2025-02-10T08:25:07.627277Z","shell.execute_reply":"2025-02-10T08:25:07.645346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mv submission.csv submission3.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.stats import rankdata\n\n# #  CSV \n# sub1 = pd.read_csv('/kaggle/input/refactoring-nn-pairwise-ranking-loss11/submission.csv')\n# sub2 = pd.read_csv('/kaggle/input/refactoring-nn-pairwise-ranking-loss11/submission1.csv')\n \n#  CSV \nsub1 = pd.read_csv('submission2.csv')\nsub2 = pd.read_csv('submission3.csv') \n\n\n#  id  id \nmerged = pd.merge(sub1, sub2, on='ID', suffixes=('_sub1', '_sub2'))\n\n#  rankdata  prediction \nmerged['prediction'] = (rankdata(merged['prediction_sub1']) + rankdata(merged['prediction_sub2'])) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rankdata(merged['prediction_sub1'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rankdata(merged['prediction_sub2'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  id  prediction \nmerged = merged[['ID', 'prediction']]\n\n#  CSV \nmerged.to_csv('submission.csv', index=False)\nmerged","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}